# Analysis of Variance

## Setup: Packages Used Here

```{r, message = FALSE}
knitr::opts_chunk$set(comment = NA)

library(ggridges)
library(janitor)
library(mosaic)
library(tidyverse)

theme_set(theme_bw())
```

## National Youth Fitness Survey

Recall the National Youth Fitness Survey, which we explored a small piece of in [Chapter -@sec-nyfs]. We'll look at a different part of the same survey here - specifically the 280 children whose data are captured in the `nyfs2` file.

```{r}
nyfs2 <- read_csv("data/nyfs2.csv", show_col_types = FALSE) |>
  clean_names()

nyfs2
```

## Comparing Gross Motor Quotient Scores by Income Level (3 Categories)

```{r}
nyfs2a <- nyfs2 |>
    select(subject_id, income_cat3, gmq) |>
    arrange(subject_id)
```

In this first analysis, we'll compare the population mean on the Gross Motor Quotient evaluation of these kids across three groups defined by income level.  Higher values of this GMQ measure indicate improved levels of gross motor development, both in terms of locomotor and object control. See https://wwwn.cdc.gov/Nchs/Nnyfs/Y_GMX.htm for more details.

```{r}
nyfs2a |>
    group_by(income_cat3) |>
    summarise(n = n(), mean(gmq), median(gmq))
```

Uh, oh. We should rearrange those income categories to match a natural order from low to high.

```{r}
nyfs2a <- nyfs2a |> 
  mutate(income_cat3 = fct_relevel(income_cat3, 
         "Low (below 25K)", "Middle (25 - 64K)", "High (65K or more)"))
```

When working with three independent samples, I use graphs analogous to those we built for two independent samples.

```{r}
ggplot(nyfs2a, aes(x = income_cat3, y = gmq, fill = income_cat3)) +
  geom_violin(aes(col = income_cat3), alpha = 0.5) +
  geom_boxplot(notch = TRUE, alpha = 0.75, width = 0.3) +
  stat_summary(fun = "mean", geom = "point", 
               shape = 23, size = 3, fill = "white") +
  coord_flip() +
  guides(fill = "none", col = "none") +
  labs(title = "GMQ Scores for 280 Children in NNYFS",
       y = "GMQ Score, in points", x = "Income Category")
```

In addition to this comparison boxplot, we might consider faceted histograms.

```{r}
ggplot(nyfs2a, aes(x = gmq, fill = income_cat3)) +
  geom_histogram(bins = 15, col = "white") +
  guides(fill = "none") +
  facet_wrap(~ income_cat3)
```

Or, if we want to ignore the (modest) sample size differences, we might consider density functions, perhaps through a ridgeline plot.

```{r}
ggplot(nyfs2a, aes(x = gmq, y = income_cat3, fill = income_cat3)) +
    geom_density_ridges(scale = 0.9) +
    guides(fill = "none") + 
    labs(title = "GMQ Score (in points) by Income Group",
         x = "GMQ Score", y = "") +
    theme_ridges()
```

```{r}
by(nyfs2a$gmq, nyfs2a$income_cat3, favstats)
```

## Alternative Procedures for Comparing More Than Two Means

Now, if we only had two independent samples, we'd be choosing between a pooled t test, a Welch t test, and a non-parametric procedure like the Wilcoxon-Mann-Whitney rank sum test, or even perhaps a bootstrap alternative.

In the case of more than two independent samples, we have methods analogous to the Welch test, and the rank sum test, and even the bootstrap, but we're going to be far more likely to select the **analysis of variance** (ANOVA) or an equivalent regression-based approach. These are the extensions of the pooled t test. Unless the sample outcome data are very clearly not Normally distributed, and no transformation is available which makes them appear approximately Normal in all of the groups we are comparing, we will stick with ANOVA.

### Extending the Welch Test to > 2 Independent Samples

It is possible to extend the Welch two-sample t test (not assuming equal population variances) into an analogous one-factor analysis for comparing population means based on independent samples from more than two groups. 

If we want to compare the population mean GMQ levels across those three income groups without assuming equal population variances, `oneway.test` is up to the task. The hypotheses being tested here are:

- H0: All three means are the same vs.
- HA: At least one of the population means is different than the others.

```{r}
oneway.test(gmq ~ income_cat3, data = nyfs2a)
```

We get a p value, but this isn't much help, though, because we don't have any measure of effect size, nor do we have any confidence intervals. Like the analogous Welch t test, this approach allows us to forego the assumption of equal population variances in each of the three income groups, but it still requires us to assume that the populations are Normally distributed. 

That said, most of the time when we have more than two levels of the factor of interest, we won't bother worrying about the equal population variance assumption, and will just use the one-factor ANOVA approach (with pooled variances) described below, to make the comparisons of interest.

### Extending the Rank Sum Test to > 2 Independent Samples

It is also possible to extend the Wilcoxon-Mann-Whitney two-sample test into an analogous one-factor analysis called the **Kruskal-Wallis test** for comparing population measures of location based on independent samples from more than two groups. 

If we want to compare the centers of the distributions of population GMQ score across our three income groups without assuming Normality, we can use `kruskal.test`.

The hypotheses being tested here are still as before, but for a measure of location other than the population mean

```{r}
kruskal.test(gmq ~ income_cat3, data = nyfs2a)
```

Again, note that this isn't much help, though, because we don't have any measure of effect size, nor do we have any confidence intervals.

That said, most of the time when we have more than two levels of the factor of interest, we won't bother worrying about potential violations of the Normality assumption unless they are glaring, and will just use the usual one-factor ANOVA approach (with pooled variances) described below, to make the comparisons of interest.

### Can we use the bootstrap to compare more than two means?

Sure. There are both ANOVA and ANCOVA analogues using the bootstrap, and in fact, there are power calculations based on the bootstrap, too. If you want to see some example code, look at https://sammancuso.com/2017/11/01/model-based-bootstrapped-anova-and-ancova/ 

## The Analysis of Variance

Extending the two-sample t test (assuming equal population variances) into a comparison of more than two samples uses the **analysis of variance** or ANOVA. 

This is an analysis of a continuous outcome variable on the basis of a single categorical factor, in fact, it's often called one-factor ANOVA or one-way ANOVA to indicate that the outcome is being split up into the groups defined by a single factor. 

The null hypothesis is that the population means are all the same, and the alternative is that this is not the case. When there are just two groups, then this boils down to an F test that is equivalent to the Pooled t test.

### The `oneway.test` approach

R will produce some elements of a one-factor ANOVA using the `oneway.test` command:

```{r}
oneway.test(gmq ~ income_cat3, data = nyfs2a, var.equal=TRUE)
```

This isn't the full analysis, though, which would require a more complete ANOVA table. There are two equivalent approaches to obtaining the full ANOVA table when comparing a series of 2 or more population means based on independent samples.

### Using the `aov` approach and the `summary` function

Here's one possible ANOVA table, which doesn't require directly fitting a linear model.

```{r}
summary(aov(gmq ~ income_cat3, data = nyfs2a))
```

### Using the `anova` function after fitting a linear model

An equivalent way to get identical results in a slightly different format runs the linear model behind the ANOVA approach directly.

```{r}
anova(lm(gmq ~ income_cat3, data = nyfs2a))
```

## Interpreting the ANOVA Table

### What are we Testing?

The null hypothesis for the ANOVA table is that the population means of the outcome across the various levels of the factor of interest are all the same, against a two-sided alternative hypothesis that the level-specific population means are not all the same.

Specifically, if we have a grouping factor with *k* levels, then we are testing:

- H0: All k population means are the same.
- HA: At least one of the population means is different from the others.

### Elements of the ANOVA Table

The ANOVA table breaks down the variation in the outcome explained by the k levels of the factor of interest, and the variation in the outcome which remains (the Residual, or Error).

Specifically, the elements of the ANOVA table are:

1. the degrees of freedom (labeled Df) for the factor of interest and for the Residuals
2. the sums of squares (labeled Sum Sq) for the factor of interest and for the Residuals
3. the mean square (labeled Mean Sq) for the factor of interest and for the Residuals
4. the ANOVA F test statistic (labeled F value), which is used to generate
5. the *p* value for the comparison assessed by the ANOVA model, labeled Pr(>F)

### The Degrees of Freedom

```{r}
anova(lm(gmq ~ income_cat3, data = nyfs2a))
```

- The **degrees of freedom** attributable to the factor of interest (here, Income category) is the number of levels of the factor minus 1. Here, we have three Income categories (levels), so df(income_cat3) = 2.
- The total degrees of freedom are the number of observations (across all levels of the factor) minus 1. We have 280 GMQ scores in the `nyfs2a` data, so the df(Total) must be 279, although the Total row isn't shown by R in its output.
- The Residual degrees of freedom are the Total df - Factor df. So, here, that's 279 - 2 = 277.

### The Sums of Squares

```{r}
anova(lm(gmq ~ income_cat3, data = nyfs2a))
```

- The sum of squares (often abbreviated SS or Sum Sq) represents variation explained. 
- The factor SS is the sum across all levels of the factor of the sample size for the level multiplied by the squared difference between the level mean and the overall mean across all levels. Here, SS(`income_cat3`) = 146
- The total SS is the sum across all observations of the square of the difference between the individual values and the overall mean. Here, that is 146 + 58174 = 58320 
- Residual SS = Total SS - Factor SS.
- Also of interest is a calculation called $\eta^2$, ("eta-squared"), which is equivalent to $R^2$ in a linear model.
    - SS(Factor) / SS(Total) = the proportion of variation in our outcome (here, GMQ) explained by the variation between groups (here, income groups)
    - In our case, $\eta^2$ = 146 / (146 + 58174) = 146 / 58320 = 0.0025
    - So, Income Category alone accounts for about 0.25% of the variation in GMQ levels observed in these data.

### The Mean Square

```{r}
anova(lm(gmq ~ income_cat3, data = nyfs2a))
```

- The Mean Square is the Sum of Squares divided by the degrees of freedom, so MS(Factor) = SS(Factor)/df(Factor). 
- In our case, MS(`income_cat3`) = SS(`income_cat3`)/df(`income_cat3`) = 146 / 2 = 72.848 (notice that R maintains more decimal places than it shows for these calculations) and 
- MS(Residuals) = SS(Residuals) / df(Residuals) = 58174 / 277 = 210.014.
    - MS(Residuals) or MS(Error) is an estimate of the residual variance which corresponds to $\sigma^2$ in the underlying linear model for the outcome of interest, here `GMQ`.

### The F Test Statistic and *p* Value

```{r}
anova(lm(gmq ~ income_cat3, data = nyfs2a))
```

- The ANOVA F test is obtained by calculating MS(Factor) / MS(Residuals). So in our case, F = 72.848 / 210.014 = 0.3469
- The F test statistic is then compared to a specific F distribution to obtain a *p* value, which is shown here to be 0.7072
- Specifically, the observed F test statistic is compared to an F distribution with numerator df = Factor df, and denominator df = Residual df to obtain the *p* value.
    + Here, we have SS(Factor) = 146 (approximately), and df(Factor) = 2, leaving MS(Factor) = 72.848
    + We have SS(Residual) = 58174, and df(Residual) = 277, leaving MS(Residual) = 210.014
    + MS(Factor) / MS(Residual) = F value = 0.3469, which, when compared to an F distribution with 2 and 277 degrees of freedom, yields a *p* value of 0.7072

## The Residual Standard Error

The residual standard error is simply the square root of the variance estimate MS(Residual). Here, MS(Residual) = 210.014, so the Residual standard error = 14.49 points. 

## The Proportion of Variance Explained by the Factor

We will often summarize the proportion of the variation explained by the factor. The summary statistic is called eta-squared ($\eta^2$), and is equivalent to the $R^2$ value we have seen previously in linear regression models.

Again, $\eta^2$ = SS(Factor) / SS(Total) 

Here, we have 
    - SS(`income_cat3`) = 146 and SS(Residuals) = 58174, so SS(Total) = 58320
    - Thus, $\eta^2$ = SS(Factor)/SS(Total) = 146/58320 = 0.0025

The income category accounts for 0.25% of the variation in GMQ levels: only a tiny fraction.

## The Regression Approach to Compare Population Means based on Independent Samples

This approach is equivalent to the ANOVA approach, and thus also (when there are just two samples to compare) to the pooled-variance t test. We run a linear regression model to predict the outcome (here, `GMQ`) on the basis of the categorical factor with three levels (here, `income_cat3`)

```{r}
summary(lm(gmq ~ income_cat3, data=nyfs2a))
```

### Interpreting the Regression Output

This output tells us many things, but for now, we'll focus just on the coefficients output, which tells us that:

- the point estimate for the population mean GMQ score across "Low" income subjects is 97.03
- the point estimate (sample mean difference) for the difference in population mean GMQ level between the "Middle" and "Low" income subjects is -1.66 (in words, the Middle income kids have lower GMQ scores than the Low income kids by 1.66 points on average.)
- the point estimate (sample mean difference) for the difference in population mean GMQ level between the "High" and "Low" income subjects is -1.30 (in words, the High income kids have lower GMQ scores than the Low income kids by 1.30 points on average.)

Of course, we knew all of this already from a summary of the sample means.

```{r}
nyfs2a |>
    group_by(income_cat3) |>
    summarise(n = n(), mean(gmq))
```

The model for predicting GMQ is based on two binary (1/0) indicator variables, specifically, we have:

- Estimated GMQ = 97.03 - 1.66 x [1 if Middle income or 0 if not] - 1.30 x [1 if High income or 0 if not] 

The coefficients section also provides a standard error and t statistic and two-sided *p* value for each coefficient.

### The Full ANOVA Table

To see the full ANOVA table corresponding to any linear regression model, we run...

```{r}
anova(lm(gmq ~ income_cat3, data=nyfs2a))
```

### ANOVA Assumptions

The assumptions behind analysis of variance are the same as those behind a linear model. Of specific interest are:

- The samples obtained from each group are independent.
- Ideally, the samples from each group are a random sample from the population described by that group.
- In the population, the variance of the outcome in each group is equal. (This is less of an issue if our study involves a balanced design.)
- In the population, we have Normal distributions of the outcome in each group.

Happily, the F test is fairly robust to violations of the Normality assumption.

## Equivalent approach to get ANOVA Results

```{r}
summary(aov(gmq ~ income_cat3, data = nyfs2a))
```

So which of the pairs of means are driving the differences we see?

## The Problem of Multiple Comparisons

1. Suppose we compare High to Low, using a test with $\alpha$ = 0.05
2. Then we compare Middle to Low on the same outcome, also using  $\alpha$ = 0.05
3. Then we compare High to Middle, also with $\alpha$ = 0.05

What is our overall $\alpha$ level across these three comparisons?

- It could be as bad as 0.05 + 0.05 + 0.05, or 0.15.
- Rather than our nominal 95% confidence, we have something as low as 85% confidence across this set of simultaneous comparisons.

### The Bonferroni solution

1. Suppose we compare High to Low, using a test with $\alpha$ = 0.05/3
2. Then we compare Middle to Low on the same outcome, also using  $\alpha$ = 0.05/3
3. Then we compare High to Middle, also with $\alpha$ = 0.05/3

Then across these three comparisons, our overall $\alpha$ can be (at worst) 

- 0.05/3 + 0.05/3 + 0.05/3 = 0.05
- So by changing our nominal confidence level from 95% to 98.333% in each comparison, we wind up with at least 95% confidence across this set of simultaneous comparisons.
- This is a conservative (worst case) approach.

Goal: Simultaneous comparisons of White vs AA, AA vs Other and White vs Other

```{r}
pairwise.t.test(nyfs2a$gmq, nyfs2a$income_cat3, p.adjust="bonferroni")
```

These *p* values are very large.

### Pairwise Comparisons using Tukey's HSD Method

Goal: Simultaneous (less conservative) confidence intervals and *p* values for our three pairwise comparisons (High vs. Low, High vs. Middle, Middle vs. Low)

```{r}
TukeyHSD(aov(gmq ~ income_cat3, data = nyfs2a))
```

### Plotting the Tukey HSD results

```{r}
plot(TukeyHSD(aov(gmq ~ income_cat3, data = nyfs2a)))
```

Note that the default positioning of the y axis in the plot of Tukey HSD results can be problematic. If we have longer names, in particular, for the levels of our factor, R will leave out some of the labels. We can alleviate that problem either by using the `fct_recode` function in the `forcats` package to rename the factor levels, or we can use the following code to reconfigure the margins of the plot.

```{r}
mar.default <- c(5,6,4,2) + 0.1 # save default plotting margins

par(mar = mar.default + c(0, 12, 0, 0)) 
plot(TukeyHSD(aov(gmq ~ income_cat3, data = nyfs2a)), las = 2)

par(mar = mar.default) # return to normal plotting margins
```

## What if we consider another outcome, BMI?

We'll look at the full data set in `nyfs2` now, so we can look at BMI as a function of income.

```{r, message = FALSE}
nyfs2$income_cat3 <- 
    fct_relevel(nyfs2$income_cat3,
                "Low (below 25K)", "Middle (25 - 64K)", "High (65K or more)")

ggplot(nyfs2, aes(x = bmi, y = income_cat3, fill = income_cat3)) +
    geom_density_ridges(scale = 0.9) +
    guides(fill = "none") + 
    labs(title = "Body-Mass Index by Income Group",
         x = "Body-Mass Index", y = "") +
    theme_ridges()
```

```{r}
ggplot(nyfs2, aes(x = income_cat3, y = bmi, fill = income_cat3)) +
  geom_violin(aes(col = income_cat3), alpha = 0.5) +
  geom_boxplot(width = 0.3, notch = TRUE, alpha = 0.75) +
  stat_summary(fun = "mean", geom = "point", 
               shape = 23, size = 3, fill = "white") +
  coord_flip() +
  guides(fill = "none", col = "none") +
  labs(title = "BMI for 280 Children in NNYFS",
       y = "Body-Mass Index", x = "Income Category")
```

Here are the descriptive numerical summaries:

```{r}
mosaic::favstats(bmi ~ income_cat3, data = nyfs2)
```

Here is the ANOVA table.

```{r}
anova(lm(bmi ~ income_cat3, data = nyfs2))
```

Let's consider the Tukey HSD results. First, we'll create a factor with shorter labels.

```{r}
nyfs2$inc.new <- 
    fct_recode(nyfs2$income_cat3, 
               "Low" = "Low (below 25K)", "Middle" = "Middle (25 - 64K)",
               "High" = "High (65K or more)")

plot(TukeyHSD(aov(bmi ~ inc.new, data = nyfs2),
                  conf.level = 0.90))
```

It appears that there is a detectable difference between the `bmi` means of the "Low" group and both the "High" and "Middle" group at the 90% confidence level, but no detectable difference between "Middle" and "High." Details of those confidence intervals for those pairwise comparisons follow.

```{r}
TukeyHSD(aov(bmi ~ inc.new, data = nyfs2),
                  conf.level = 0.90)
```


