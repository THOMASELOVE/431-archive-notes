[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Archived Course Notes for 431, version 2023",
    "section": "",
    "text": "This is the archived version of Dr. Love’s Course Notes\nThis document is no longer in active development. Dr. Love is building new materials, but this remains as an archive of where the original notes were as of late August 2023.",
    "crumbs": [
      "This is the archived version of Dr. Love's Course Notes"
    ]
  },
  {
    "objectID": "index.html#what-youll-find-here",
    "href": "index.html#what-youll-find-here",
    "title": "Archived Course Notes for 431, version 2023",
    "section": "What You’ll Find Here",
    "text": "What You’ll Find Here\nThese Notes provided a series of examples using R to work through issues that were likely to come up in PQHS/CRSP/MPHP 431 through 2023. What you will mostly find are brief explanations of a key idea or summary, accompanied (most of the time) by R code and a demonstration of the results of applying that code.\nWhile these Notes share some of the features of a textbook, they are neither comprehensive nor completely original. The main purpose is to give 431 students a set of common materials on which to draw during the course. In class, we will sometimes:\n\nreiterate points made in this document,\namplify what is here,\nsimplify the presentation of things done here,\nuse new examples to show some of the same techniques,\nrefer to issues not mentioned in this document,\n\nbut what we don’t do is follow these notes very precisely. We assume instead that you will read the materials and try to learn from them, just as you will attend classes and try to learn from them. We welcome feedback of all kinds on this document or anything else.\nAll of the code and text in these Notes is available online, and it is also possible to download a PDF version of the document from the down arrow next to the title (Notes for 431) at the top left of this screen.",
    "crumbs": [
      "This is the archived version of Dr. Love's Course Notes"
    ]
  },
  {
    "objectID": "int_r_setup.html",
    "href": "int_r_setup.html",
    "title": "Setting Up R",
    "section": "",
    "text": "Quarto\nThese notes were written using Quarto, which we’ll learn to use in 431. Quarto, like R and RStudio, is free and open source.\nQuarto is described as an scientific and technical publishing system for data science, which lets you (among many other things):\nThe Quarto Get Started page which provides an overview and quick tour of what’s possible with Quarto.\nAnother excellent resource to learn more about Quarto is the Communicate section (especially the [Quarto chapter]https://r4ds.hadley.nz/quarto.html)) of Hadley Wickham and Grolemund (2023).",
    "crumbs": [
      "Setting Up R"
    ]
  },
  {
    "objectID": "int_r_setup.html#quarto",
    "href": "int_r_setup.html#quarto",
    "title": "Setting Up R",
    "section": "",
    "text": "save and execute R code\ngenerate high-quality reports that can be shared with an audience",
    "crumbs": [
      "Setting Up R"
    ]
  },
  {
    "objectID": "int_r_setup.html#r-packages",
    "href": "int_r_setup.html#r-packages",
    "title": "Setting Up R",
    "section": "R Packages",
    "text": "R Packages\nAt the start of each chapter that involves R code, I’ll present a series of commands I run to set up R to use several packages (libraries) of functions that expand its capabilities, make a specific change to how I want R output to be displayed (that’s the comment = NA piece) and sets the theme for most graphs to theme_bw(). A chunk of code like this will occur near the top of any Quarto work.\nFor example, this is the setup for one of our early chapters that loads four packages.\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(palmerpenguins)\nlibrary(janitor)\nlibrary(knitr)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\n\nYou only need to install a package once, but you need to reload it (using the library() function) every time you start a new session. I always load the package called tidyverse last, since doing so avoids some annoying problems.",
    "crumbs": [
      "Setting Up R"
    ]
  },
  {
    "objectID": "int_r_setup.html#the-love-boost.r-script",
    "href": "int_r_setup.html#the-love-boost.r-script",
    "title": "Setting Up R",
    "section": "The Love-boost.R script",
    "text": "The Love-boost.R script\nIn October, when we start Part B of the course, we’ll use some special R functions I’ve gathered for you in a script called Love-boost. I’ll tell R about that code using the following command…\n\nsource(\"data/Love-boost.R\")\n\nThe Love-boost.R script includes four functions:\n\nbootdif\nsaifs.ci\ntwobytwo\nretrodesign",
    "crumbs": [
      "Setting Up R"
    ]
  },
  {
    "objectID": "int_r_setup.html#packages-used-in-these-notes",
    "href": "int_r_setup.html#packages-used-in-these-notes",
    "title": "Setting Up R",
    "section": "Packages Used in these Notes",
    "text": "Packages Used in these Notes\nA list of all R packages we want you to install this semester (which includes some packages not included in these Notes) is maintained at our course web site.\n\n\nPackage\nParts\nKey functions in the Package\n\n\n\narm\nC\n–\n\n\nboot\nB\n–\n\n\nbroom\nA, B, C\n\ntidy, glance, augment (part of tidymodels)\n\n\ncar\nA, C\n\nboxCox, powerTransform\n\n\n\nEpi\nB\ntwoby2\n\n\nfivethirtyeight\nAppendix\nsource of data\n\n\nGGally\nA, C\nggpairs\n\n\nggrepel\nC\n–\n\n\nggridges\nA, B\n–\n\n\nggstance\nA\n–\n\n\ngt\nA\nfor presenting tables\n\n\ngtsummary\nA\ntbl_summary\n\n\nHmisc\nA, B, C\n\ndescribe and others\n\n\njanitor\nA, B, C\n\ntabyl and others\n\n\nkableExtra\nA\n\nkbl, kable_stylings\n\n\n\nknitr\nA, B, C\nkable\n\n\nlvplot\nA\ngeom_lv\n\n\nmice\nC\n–\n\n\nmodelsummary\nA, C\nmodelsummary\n\n\nmosaic\nA, B, C\n\nfavstats, inspect\n\n\n\nnaniar\nA\n\nn_miss, miss_case_table, gg_miss_var\n\n\n\nNHANES\nA\nsource of data\n\n\npalmerpenguins\nA\nsource of data\n\n\npatchwork\nA, B, C\nfor combining/annotating plots\n\n\npsych\nA, B\ndescribe\n\n\npwr\nB\n–\n\n\nrms\nC\n–\n\n\nsessioninfo\nAppendix\n–\n\n\nsimputation\nA\nvarious imputation functions\n\n\nsummarytools\nA\n\ndescr, dfSummary\n\n\n\ntidyverse\nA, B, C, Appendix\ndozens of functions\n\n\nvcd\nB\n–\n\n\nvisdat\nA\n\nvis_dat, vis_miss\n\n\n\n\nThe tidyverse\n\nThe tidyverse package is actually a meta-package which includes the following core packages:\n\n\nggplot2 for creating graphics\n\ndplyr for data manipulation\n\ntidyr for creating tidy data\n\nreadr for reading in rectangular data\n\npurrr for working with functions and vectors\n\ntibble for creating tibbles - lazy, surly data frames\n\nstringr for working with data strings\n\nforcats for solving problems with factors\n\nLoading the tidyverse with library(tidyverse) loads those eight packages.\nInstalling the tidyverse also installs several other useful packages on your machine, like glue and lubridate, for example. Read more about the tidyverse at https://www.tidyverse.org/\n\n\n\n\nHadley Wickham, Mine Çetinyaka-Rundel, and Garrett Grolemund. 2023. R for Data Science. Second. O’Reilly. https://r4ds.hadley.nz/.\n\n\nIsmay, Chester, and Albert Y. Kim. 2022. ModernDive: Statistical Inference via Data Science. http://moderndive.com/.",
    "crumbs": [
      "Setting Up R"
    ]
  },
  {
    "objectID": "01-datascience.html",
    "href": "01-datascience.html",
    "title": "\n1  Data Science and 431\n",
    "section": "",
    "text": "1.1 Data Science Project Cycle\nA typical data science project can be modeled as follows, which comes from the introduction to the amazing book R for Data Science, by Garrett Grolemund and Hadley Wickham, which is a key text for this course (Hadley Wickham and Grolemund 2023).\nSource: R for Data Science: Introduction\nThis diagram is sometimes referred to as the Krebs Cycle of Data Science. For more on the steps of a data science project, we encourage you to read the Introduction of Hadley Wickham and Grolemund (2023).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Science and 431</span>"
    ]
  },
  {
    "objectID": "01-datascience.html#data-science-and-the-431-course",
    "href": "01-datascience.html#data-science-and-the-431-course",
    "title": "\n1  Data Science and 431\n",
    "section": "\n1.2 Data Science and the 431 Course",
    "text": "1.2 Data Science and the 431 Course\nWe’ll discuss each of these elements in the 431 course, focusing at the start on understanding our data through transformation, modeling and (especially in the early stages) visualization. In 431, we learn how to get things done.\n\nWe get people working with R and R Studio and Quarto, even if they are completely new to coding. A gentle introduction is provided at Ismay and Kim (2022)\n\nWe learn how to use the tidyverse (http://www.tidyverse.org/), an array of tools in R (mostly developed by Hadley Wickham and his colleagues at R Studio) which share an underlying philosophy to make data science faster, easier, more reproducible and more fun. A critical text for understanding the tidyverse is Hadley Wickham and Grolemund (2023). Tidyverse tools facilitate:\n\n\nimporting data into R, which can be the source of intense pain for some things, but is really quite easy 95% of the time with the right tool.\n\ntidying data, that is, storing it in a format that includes one row per observation and one column per variable. This is harder, and more important, than you might think.\n\ntransforming data, perhaps by identifying specific subgroups of interest, creating new variables based on existing ones, or calculating summaries.\n\nvisualizing data to generate actual knowledge and identify questions about the data - this is an area where R really shines, and we’ll start with it in class.\n\nmodeling data, taking the approach that modeling is complementary to visualization, and allows us to answer questions that visualization helps us identify.\nand last, but definitely not least, communicating results, models and visualizations to others, in a way that is reproducible and effective.\n\n\nSome programming/coding is an inevitable requirement to accomplish all of these aims. If you are leery of coding, you’ll need to get past that, with the help of this course and our stellar teaching assistants. Getting started is always the most challenging part, but our experience is that most of the pain of developing these new skills evaporates by early October.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Science and 431</span>"
    ]
  },
  {
    "objectID": "01-datascience.html#what-the-course-is-and-isnt",
    "href": "01-datascience.html#what-the-course-is-and-isnt",
    "title": "\n1  Data Science and 431\n",
    "section": "\n1.3 What The Course Is and Isn’t",
    "text": "1.3 What The Course Is and Isn’t\nThe 431 course is about getting things done. In developing this course, we adopt a modern approach that places data at the center of our work. Our goal is to teach you how to do truly reproducible research with modern tools. We want you to be able to collect and use data effectively to address questions of interest.\nThe curriculum includes more on several topics than you might expect from a standard graduate introduction to biostatistics.\n\ndata gathering\ndata wrangling\nexploratory data analysis and visualization\nmultivariate modeling\ncommunication\n\nIt also nearly completely avoids formalism and is extremely applied - this is absolutely not a course in theoretical or mathematical statistics, and these Notes reflect that approach.\nThere’s very little of the mathematical underpinnings here:\n\\[\nf(x) = \\frac{e^{-(x - \\mu)^{2}/(2\\sigma^{2})}}{\\sigma{\\sqrt{2 \\pi }}}\n\\]\nInstead, these notes (and the course) focus on how we get R to do the things we want to do, and how we interpret the results of our work. Our next Chapter provides a first example.\n\n\n\n\nGelman, Andrew, and Deborah Nolan. 2017. Teaching Statistics: A Bag of Tricks. Second Edition. Oxford, UK: Oxford University Press.\n\n\nHadley Wickham, Mine Çetinyaka-Rundel, and Garrett Grolemund. 2023. R for Data Science. Second. O’Reilly. https://r4ds.hadley.nz/.\n\n\nIsmay, Chester, and Albert Y. Kim. 2022. ModernDive: Statistical Inference via Data Science. http://moderndive.com/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Science and 431</span>"
    ]
  },
  {
    "objectID": "02-penguins1.html",
    "href": "02-penguins1.html",
    "title": "\n2  The Palmer Penguins\n",
    "section": "",
    "text": "2.1 Setup: Packages Used Here\nWe will use the palmerpenguins package to supply us with data for this chapter. The janitor packages includes several useful functions, including tabyl. The knitr package includes the kable() function we’ll use. Finally, the tidyverse package will provide the bulk of the functions we’ll use in our work throughout the semester.\nI always load the tidyverse last, because it solves some problems to do so.\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(palmerpenguins) \nlibrary(janitor) \nlibrary(knitr) \nlibrary(kableExtra)\nlibrary(gt)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Palmer Penguins</span>"
    ]
  },
  {
    "objectID": "02-penguins1.html#viewing-a-data-set",
    "href": "02-penguins1.html#viewing-a-data-set",
    "title": "\n2  The Palmer Penguins\n",
    "section": "\n2.2 Viewing a Data Set",
    "text": "2.2 Viewing a Data Set\nThe penguins data from the palmerpenguins package contains 344 rows and 8 columns. Each row contains data for a different penguin, and each column describes a variable contained in the data set.\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nFor instance, the first penguin in the data is of the species Adelie, and was observed on the island called Torgeson. The remaining data for that penguin include measures of its bill length and depth, its flipper length and body mass, its sex and the year in which it was observed.\nNote that though there are 344 rows in the tibble of data called penguins, only the first ten rows (penguins) are shown in the table above. Note also that the symbol NA or &lt;NA&gt; is used to indicate a missing (not available) value.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Palmer Penguins</span>"
    ]
  },
  {
    "objectID": "02-penguins1.html#create-new_penguins-eliminating-missing-data",
    "href": "02-penguins1.html#create-new_penguins-eliminating-missing-data",
    "title": "\n2  The Palmer Penguins\n",
    "section": "\n2.3 Create new_penguins: Eliminating Missing Data",
    "text": "2.3 Create new_penguins: Eliminating Missing Data\nNext, let’s take the penguins data from the palmerpenguins package, and identify those observations which have complete data (so, no missing values) in four variables of interest. We’ll store that result in a new tibble (data set) called new_penguins and then take a look at that result using the following code.\nNote that the code below:\n\nuses the “pipe” |&gt; to send the penguins tibble to the filter() function\nuses &lt;- to assign the result of our work to the new_penguins tibble\nuses the complete.cases() function to remove cases within penguins that have missing data on any of the four variables (flipper_length_mm, body_mass_g, species or sex) that we identify\n\n\nnew_penguins &lt;- penguins |&gt;\n    filter(complete.cases(flipper_length_mm, body_mass_g, species, sex))\n\nnew_penguins\n\n# A tibble: 333 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           39.2          19.6               195        4675\n 8 Adelie  Torgersen           41.1          17.6               182        3200\n 9 Adelie  Torgersen           38.6          21.2               191        3800\n10 Adelie  Torgersen           34.6          21.1               198        4400\n# ℹ 323 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Palmer Penguins</span>"
    ]
  },
  {
    "objectID": "02-penguins1.html#counting-things-and-making-tables",
    "href": "02-penguins1.html#counting-things-and-making-tables",
    "title": "\n2  The Palmer Penguins\n",
    "section": "\n2.4 Counting Things and Making Tables",
    "text": "2.4 Counting Things and Making Tables\nSo, how many penguins are in our new_penguins data? When we printed out the result, we got an answer, but (as with many things in R) there are many ways to get the same result.\n\nnrow(new_penguins)\n\n[1] 333\n\n\nHow do our new_penguins data break down by sex and species? We’ll use the tabyl() function from the janitor package to look at this.\n\nnew_penguins |&gt; \n    tabyl(sex, species) \n\n    sex Adelie Chinstrap Gentoo\n female     73        34     58\n   male     73        34     61\n\n\nThe output is reasonably clear (there are 73 female and 73 male Adelie penguins in the newpenguins tibble, for example) but could we make that table a little prettier, and while we’re at it, can we add the row and column totals?\n\nnew_penguins |&gt; \n    tabyl(sex, species) |&gt;\n    adorn_totals(where = c(\"row\", \"col\")) |&gt; # add row, column totals\n    kable()  # one convenient way to make the table prettier\n\n\n\nsex\nAdelie\nChinstrap\nGentoo\nTotal\n\n\n\nfemale\n73\n34\n58\n165\n\n\nmale\n73\n34\n61\n168\n\n\nTotal\n146\n68\n119\n333\n\n\n\n\n\nThe kable() function comes from the knitr package we loaded earlier. Notice that we added some comments to the code here with the prefix #. These comments are ignored by R in processing the data.\nAnother approach we could have used here is aided by the kableExtra package’s function called kbl(), which lets us set up the alignment of our columns. We’ll also add the species name using adorn_title() from the janitor package.\n\nnew_penguins |&gt; \n    tabyl(sex, species) |&gt;\n    adorn_totals(where = c(\"row\", \"col\")) |&gt; \n    adorn_title(placement = \"combined\") |&gt;\n    kbl(align = c('lcccr')) |&gt;\n    kable_styling(full_width = FALSE)\n\n\n\nsex/species\nAdelie\nChinstrap\nGentoo\nTotal\n\n\n\nfemale\n73\n34\n58\n165\n\n\nmale\n73\n34\n61\n168\n\n\nTotal\n146\n68\n119\n333\n\n\n\n\n\nWe can switch the rows and columns, and add some additional features, using the code below, which makes use of the gt() and tab_header() functions from the gt package, which is designed to help build complex tables. More on the incredibly versatile gt() package is available at https://gt.rstudio.com/.\n\nnew_penguins |&gt; \n    tabyl(species, sex) |&gt;\n    adorn_totals(where = c(\"row\", \"col\")) |&gt; \n    gt() |&gt;\n    tab_header(\n      title = md(\"Palmer Penguins in **newpenguins**\"),\n      subtitle = \"Comparing sexes by species\"\n    )\n\n\n\n\n\n\nPalmer Penguins in newpenguins\n\n\n\nComparing sexes by species\n\n\nspecies\nfemale\nmale\nTotal\n\n\n\n\nAdelie\n73\n73\n146\n\n\nChinstrap\n34\n34\n68\n\n\nGentoo\n58\n61\n119\n\n\nTotal\n165\n168\n333",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Palmer Penguins</span>"
    ]
  },
  {
    "objectID": "02-penguins1.html#creating-a-scatterplot",
    "href": "02-penguins1.html#creating-a-scatterplot",
    "title": "\n2  The Palmer Penguins\n",
    "section": "\n2.5 Creating a Scatterplot",
    "text": "2.5 Creating a Scatterplot\nNow, let’s look at the other two variables of interest. Let’s create a graph showing the association of body mass with flipper length across the complete set of 333 penguins.\n\nggplot(new_penguins, aes(x = body_mass_g, y = flipper_length_mm)) +\n    geom_point() \n\n\n\n\n\n\n\nSome of you may want to include a straight-line model (fit by a classical linear regression) to this plot. One way to do that in R involves the addition of a single line of code, like this:\n\nggplot(new_penguins, aes(x = body_mass_g, y = flipper_length_mm)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x,\n                col = \"red\", se = FALSE)\n\n\n\n\n\n\n\nWhenever we build a graph for ourselves, these default choices may be sufficient. But I’d like to see a prettier version if I was going to show it to someone else. So, I might use a different color for each species, and I might add a title, like this.\n\nggplot(new_penguins, aes(x = body_mass_g, y = flipper_length_mm, col = species)) +\n    geom_point() + \n    labs(title = \"Flipper Length and Body Mass for 333 of the Palmer Penguins\")",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Palmer Penguins</span>"
    ]
  },
  {
    "objectID": "02-penguins1.html#six-ways-to-improve-this-graph",
    "href": "02-penguins1.html#six-ways-to-improve-this-graph",
    "title": "\n2  The Palmer Penguins\n",
    "section": "\n2.6 Six Ways To “Improve” This Graph",
    "text": "2.6 Six Ways To “Improve” This Graph\nNow, let’s build a new graph to incorporate some additional information and improve the appearance. Here, I want to:\n\nplot the relationship between body mass and flipper length in light of both Sex and Species\nincrease the size of the points and add a little transparency so we can see if points overlap,\nadd some smooth curves to summarize the relationships between the two quantities (body mass and flipper length) within each combination of species and sex,\nsplit the graph into two “facets” (one for each sex),\nimprove the axis labels,\nimprove the titles by adding a subtitle, and also adding in some code to count the penguins (rather than hard-coding in the total number.)\n\n\nggplot(new_penguins, aes(x = body_mass_g, y = flipper_length_mm, \n                         col = species)) +\n    geom_point(size = 2, alpha = 0.5) + \n    geom_smooth(method = \"loess\", formula = y ~ x, \n                se = FALSE, size = 1.5) +\n    facet_grid(~ sex) +\n    labs(title = \"Flipper Length and Body Mass, by Sex & Species\",\n         subtitle = str_glue(nrow(new_penguins), \" of the Palmer Penguins\"),\n         x = \"Body Mass (g)\", \n         y = \"Flipper Length (mm)\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Palmer Penguins</span>"
    ]
  },
  {
    "objectID": "02-penguins1.html#a-little-reflection",
    "href": "02-penguins1.html#a-little-reflection",
    "title": "\n2  The Palmer Penguins\n",
    "section": "\n2.7 A Little Reflection",
    "text": "2.7 A Little Reflection\nWhat can we learn from these plots and their construction? In particular,\n\nWhat do these plots suggest about the center of the distribution of each quantity (body mass and flipper length) overall, and within each combination of Sex and Species?\nWhat does the final plot suggest about the spread of the distribution of each of those quantities in each combination of Sex and Species?\nWhat do the plots suggest about the association of body mass and flipper length across the complete set of penguins?\nHow does the shape and nature of this body mass - flipper length relationship change based on Sex and Species?\nDo you think it would be helpful to plot a straight-line relationship (rather than a smooth curve) within each combination of Sex and Species in the final plot? Why or why not? (Also, what would we have to do to the code to accomplish this?)\nHow was the R code for the plot revised to accomplish each of the six “wants” specified above?",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Palmer Penguins</span>"
    ]
  },
  {
    "objectID": "02-penguins1.html#coming-up",
    "href": "02-penguins1.html#coming-up",
    "title": "\n2  The Palmer Penguins\n",
    "section": "\n2.8 Coming Up",
    "text": "2.8 Coming Up\nNext, we’ll introduce and demonstrate some more of the many available tools for creating summaries (both graphical and numerical) in R, again working with the Palmer Penguins data.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Palmer Penguins</span>"
    ]
  },
  {
    "objectID": "02-penguins1.html#footnotes",
    "href": "02-penguins1.html#footnotes",
    "title": "\n2  The Palmer Penguins\n",
    "section": "",
    "text": "Two fun facts: (1) Male Gentoo and Adelie penguins “propose” to females by giving them a pebble. (2) The Adelie penguin was named for his wife by Jules Dumont d’Urville, who also rediscovered the Venus de Milo.↩︎",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Palmer Penguins</span>"
    ]
  },
  {
    "objectID": "03-penguins2.html",
    "href": "03-penguins2.html",
    "title": "3  Summarizing Penguins",
    "section": "",
    "text": "3.1 Setup: Packages Used Here\nHere, we’ll add several new packages to allow us to display some additional summaries, and present our tables and plots in different ways.\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(palmerpenguins) \nlibrary(kableExtra)\nlibrary(gtsummary)\nlibrary(summarytools)\nlibrary(visdat)\nlibrary(lvplot)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())\nWe will also use functions from the mosaic and Hmisc packages here, though I won’t load them into our session at this time.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summarizing Penguins</span>"
    ]
  },
  {
    "objectID": "03-penguins2.html#our-data-set",
    "href": "03-penguins2.html#our-data-set",
    "title": "3  Summarizing Penguins",
    "section": "\n3.2 Our Data Set",
    "text": "3.2 Our Data Set\nLet’s look again at the penguins data contained in the palmerpenguins package.\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summarizing Penguins</span>"
    ]
  },
  {
    "objectID": "03-penguins2.html#numerical-summaries-for-a-tibble",
    "href": "03-penguins2.html#numerical-summaries-for-a-tibble",
    "title": "3  Summarizing Penguins",
    "section": "\n3.3 Numerical Summaries for a Tibble",
    "text": "3.3 Numerical Summaries for a Tibble\nNote that in this work, I sometimes don’t explain all of the numerical summaries provided. Some of that discussion is postponed to Chapter 7.\n\n3.3.1 Using summary()\nWe have several ways to obtain useful summaries of all variables in the penguins data.\n\npenguins |&gt;\n  summary()\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\n\n3.3.2 Using inspect() from mosaic\n\nSome people like the inspect() function from the mosaic package.\n\npenguins |&gt; \n  mosaic::inspect()\n\n\ncategorical variables:  \n     name  class levels   n missing\n1 species factor      3 344       0\n2  island factor      3 344       0\n3     sex factor      2 333      11\n                                   distribution\n1 Adelie (44.2%), Gentoo (36%) ...             \n2 Biscoe (48.8%), Dream (36%) ...              \n3 male (50.5%), female (49.5%)                 \n\nquantitative variables:  \n               name   class    min       Q1  median     Q3    max       mean\n1    bill_length_mm numeric   32.1   39.225   44.45   48.5   59.6   43.92193\n2     bill_depth_mm numeric   13.1   15.600   17.30   18.7   21.5   17.15117\n3 flipper_length_mm integer  172.0  190.000  197.00  213.0  231.0  200.91520\n4       body_mass_g integer 2700.0 3550.000 4050.00 4750.0 6300.0 4201.75439\n5              year integer 2007.0 2007.000 2008.00 2009.0 2009.0 2008.02907\n           sd   n missing\n1   5.4595837 342       2\n2   1.9747932 342       2\n3  14.0617137 342       2\n4 801.9545357 342       2\n5   0.8183559 344       0\n\n\nDaniel Kaplan’s Statistical Modeling, 2nd edition provides an entire course which coordinates nicely with the tools available in the mosaic package. In our course, we’ll most often use this inspect() tool, and a related tool called favstats.\n\n3.3.3 Using favstats() from mosaic.\nThe favstats function lets us look at some common summaries for a single variable, or for one variable divided into groups by another. We’ll also return to this approach in Chapter 7.\n\nmosaic::favstats(~ bill_length_mm, data = penguins) |&gt;\n  kbl() |&gt;\n  kable_styling()\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n32.1\n39.225\n44.45\n48.5\n59.6\n43.92193\n5.459584\n342\n2\n\n\n\n\n\nmosaic::favstats(bill_length_mm ~ species, data = penguins) |&gt;\n  kbl() |&gt;\n  kable_styling()\n\n\n\nspecies\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\nAdelie\n32.1\n36.75\n38.80\n40.750\n46.0\n38.79139\n2.663405\n151\n1\n\n\nChinstrap\n40.9\n46.35\n49.55\n51.075\n58.0\n48.83382\n3.339256\n68\n0\n\n\nGentoo\n40.9\n45.30\n47.30\n49.550\n59.6\n47.50488\n3.081857\n123\n1\n\n\n\n\n\n\n3.3.4 Using describe() from psych\n\nWe can use the describe() function from the psych package to get some additional summaries, if we’re interested, and here we also demonstrate the use of the kbl() and kable_styling() functions from the kableExtra package to make the table look appealing in HTML. More on the use of the kableExtra package is available here. We’ll also return to this approach in Chapter 7.\n\npenguins |&gt;\n  psych::describe() |&gt;\n  kbl() |&gt;\n  kable_styling()\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\nspecies*\n1\n344\n1.918605\n0.8933198\n2.00\n1.898551\n1.48260\n1.0\n3.0\n2.0\n0.1591315\n-1.7318773\n0.0481646\n\n\nisland*\n2\n344\n1.662791\n0.7261940\n2.00\n1.579710\n1.48260\n1.0\n3.0\n2.0\n0.6086049\n-0.9064333\n0.0391538\n\n\nbill_length_mm\n3\n342\n43.921930\n5.4595837\n44.45\n43.906934\n7.04235\n32.1\n59.6\n27.5\n0.0526530\n-0.8931397\n0.2952205\n\n\nbill_depth_mm\n4\n342\n17.151170\n1.9747932\n17.30\n17.172628\n2.22390\n13.1\n21.5\n8.4\n-0.1422086\n-0.9233523\n0.1067846\n\n\nflipper_length_mm\n5\n342\n200.915205\n14.0617137\n197.00\n200.335766\n16.30860\n172.0\n231.0\n59.0\n0.3426554\n-0.9991866\n0.7603704\n\n\nbody_mass_g\n6\n342\n4201.754386\n801.9545357\n4050.00\n4154.014598\n889.56000\n2700.0\n6300.0\n3600.0\n0.4662117\n-0.7395200\n43.3647348\n\n\nsex*\n7\n333\n1.504504\n0.5007321\n2.00\n1.505618\n0.00000\n1.0\n2.0\n1.0\n-0.0179376\n-2.0056743\n0.0274400\n\n\nyear\n8\n344\n2008.029070\n0.8183559\n2008.00\n2008.036232\n1.48260\n2007.0\n2009.0\n2.0\n-0.0532601\n-1.5092478\n0.0441228\n\n\n\n\n\n\n3.3.5 Using describe() from Hmisc\n\nOne approach Frank Harrell has developed that I find helpful is the describe() function within his Hmisc package, which produces these results. We’ll also return to this approach in Chapter 7.\n\npenguins |&gt; \n  Hmisc::describe() \n\npenguins \n\n 8  Variables      344  Observations\n--------------------------------------------------------------------------------\nspecies \n       n  missing distinct \n     344        0        3 \n                                        \nValue         Adelie Chinstrap    Gentoo\nFrequency        152        68       124\nProportion     0.442     0.198     0.360\n--------------------------------------------------------------------------------\nisland \n       n  missing distinct \n     344        0        3 \n                                        \nValue         Biscoe     Dream Torgersen\nFrequency        168       124        52\nProportion     0.488     0.360     0.151\n--------------------------------------------------------------------------------\nbill_length_mm \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     342        2      164        1    43.92    6.274    35.70    36.60 \n     .25      .50      .75      .90      .95 \n   39.23    44.45    48.50    50.80    51.99 \n\nlowest : 32.1 33.1 33.5 34   34.1, highest: 55.1 55.8 55.9 58   59.6\n--------------------------------------------------------------------------------\nbill_depth_mm \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     342        2       80        1    17.15    2.267     13.9     14.3 \n     .25      .50      .75      .90      .95 \n    15.6     17.3     18.7     19.5     20.0 \n\nlowest : 13.1 13.2 13.3 13.4 13.5, highest: 20.7 20.8 21.1 21.2 21.5\n--------------------------------------------------------------------------------\nflipper_length_mm \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     342        2       55    0.999    200.9    16.03    181.0    185.0 \n     .25      .50      .75      .90      .95 \n   190.0    197.0    213.0    220.9    225.0 \n\nlowest : 172 174 176 178 179, highest: 226 228 229 230 231\n--------------------------------------------------------------------------------\nbody_mass_g \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     342        2       94        1     4202    911.8     3150     3300 \n     .25      .50      .75      .90      .95 \n    3550     4050     4750     5400     5650 \n\nlowest : 2700 2850 2900 2925 2975, highest: 5850 5950 6000 6050 6300\n--------------------------------------------------------------------------------\nsex \n       n  missing distinct \n     333       11        2 \n                        \nValue      female   male\nFrequency     165    168\nProportion  0.495  0.505\n--------------------------------------------------------------------------------\nyear \n       n  missing distinct     Info     Mean      Gmd \n     344        0        3    0.888     2008   0.8919 \n                            \nValue       2007  2008  2009\nFrequency    110   114   120\nProportion 0.320 0.331 0.349\n\nFor the frequency table, variable is rounded to the nearest 0\n--------------------------------------------------------------------------------\n\n\n\n3.3.6 Using tbl_summary() from gtsummary\n\nIf you want to produce results which look like you might expect to see in a published paper, the tbl_summary() function from the gtsummary package has many nice features.\n\npenguins |&gt; \n  tbl_summary()\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nN = 3441\n\n\n\n\nspecies\n\n\n\n    Adelie\n152 (44%)\n\n\n    Chinstrap\n68 (20%)\n\n\n    Gentoo\n124 (36%)\n\n\nisland\n\n\n\n    Biscoe\n168 (49%)\n\n\n    Dream\n124 (36%)\n\n\n    Torgersen\n52 (15%)\n\n\nbill_length_mm\n44.5 (39.2, 48.5)\n\n\n    Unknown\n2\n\n\nbill_depth_mm\n17.30 (15.60, 18.70)\n\n\n    Unknown\n2\n\n\nflipper_length_mm\n197 (190, 213)\n\n\n    Unknown\n2\n\n\nbody_mass_g\n4,050 (3,550, 4,750)\n\n\n    Unknown\n2\n\n\nsex\n\n\n\n    female\n165 (50%)\n\n\n    male\n168 (50%)\n\n\n    Unknown\n11\n\n\nyear\n\n\n\n    2007\n110 (32%)\n\n\n    2008\n114 (33%)\n\n\n    2009\n120 (35%)\n\n\n\n\n1 n (%); Median (IQR)\n\n\n\n\n\nA vignette explaining the use of the gtsummary package is available here. We’ll also return to this approach in Chapter 7.\n\n3.3.6.1 Using descr from summarytools\n\nThe descr() function from the summarytools package can also be used to provide numerical descriptions of all of the numerical variables contained within a tibble.\n\npenguins |&gt; \n  descr(stats = \"common\" )\n\nNon-numerical variable(s) ignored: species, island, sex\n\n\nDescriptive Statistics  \npenguins  \nN: 344  \n\n                  bill_depth_mm   bill_length_mm   body_mass_g   flipper_length_mm      year\n--------------- --------------- ---------------- ------------- ------------------- ---------\n           Mean           17.15            43.92       4201.75              200.92   2008.03\n        Std.Dev            1.97             5.46        801.95               14.06      0.82\n            Min           13.10            32.10       2700.00              172.00   2007.00\n         Median           17.30            44.45       4050.00              197.00   2008.00\n            Max           21.50            59.60       6300.00              231.00   2009.00\n        N.Valid          342.00           342.00        342.00              342.00    344.00\n      Pct.Valid           99.42            99.42         99.42               99.42    100.00\n\n\nAn introduction to the summarytools package is available here, and illustrates some other ways to modify this output to suit your needs. We’ll also return to this approach in Chapter 7.\n\n3.3.7 dfSummary() from summarytools\n\nThe dfSummary() function from the summarytools package can be used to provide some additional descriptions of all variables within a tibble. You’ll find more information about these numerical descriptions in Chapter 7.\n\ndfSummary(penguins, \n          plain.ascii  = FALSE, \n          style        = \"grid\", \n          graph.magnif = 0.75, \n          valid.col    = FALSE)\n\n### Data Frame Summary  \n#### penguins  \n**Dimensions:** 344 x 8  \n**Duplicates:** 0  \n\n+----+--------------------+---------------------------+---------------------+------------------------+---------+\n| No | Variable           | Stats / Values            | Freqs (% of Valid)  | Graph                  | Missing |\n+====+====================+===========================+=====================+========================+=========+\n| 1  | species\\           | 1\\. Adelie\\               | 152 (44.2%)\\        | IIIIIIII \\             | 0\\      |\n|    | [factor]           | 2\\. Chinstrap\\            | 68 (19.8%)\\         | III \\                  | (0.0%)  |\n|    |                    | 3\\. Gentoo                | 124 (36.0%)         | IIIIIII                |         |\n+----+--------------------+---------------------------+---------------------+------------------------+---------+\n| 2  | island\\            | 1\\. Biscoe\\               | 168 (48.8%)\\        | IIIIIIIII \\            | 0\\      |\n|    | [factor]           | 2\\. Dream\\                | 124 (36.0%)\\        | IIIIIII \\              | (0.0%)  |\n|    |                    | 3\\. Torgersen             | 52 (15.1%)          | III                    |         |\n+----+--------------------+---------------------------+---------------------+------------------------+---------+\n| 3  | bill_length_mm\\    | Mean (sd) : 43.9 (5.5)\\   | 164 distinct values | \\ \\ \\ \\ . \\ \\ \\ \\ . :\\ | 2\\      |\n|    | [numeric]          | min &lt; med &lt; max:\\         |                     | \\ \\ . : : : : :\\       | (0.6%)  |\n|    |                    | 32.1 &lt; 44.5 &lt; 59.6\\       |                     | \\ \\ : : : : : :\\       |         |\n|    |                    | IQR (CV) : 9.3 (0.1)      |                     | \\ \\ : : : : : : .\\     |         |\n|    |                    |                           |                     | : : : : : : : : .      |         |\n+----+--------------------+---------------------------+---------------------+------------------------+---------+\n| 4  | bill_depth_mm\\     | Mean (sd) : 17.2 (2)\\     | 80 distinct values  | \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ :\\ | 2\\      |\n|    | [numeric]          | min &lt; med &lt; max:\\         |                     | \\ \\ \\ \\ \\ \\ \\ \\ : :\\   | (0.6%)  |\n|    |                    | 13.1 &lt; 17.3 &lt; 21.5\\       |                     | \\ \\ : . : : : .\\       |         |\n|    |                    | IQR (CV) : 3.1 (0.1)      |                     | . : : : : : :\\         |         |\n|    |                    |                           |                     | : : : : : : : . .      |         |\n+----+--------------------+---------------------------+---------------------+------------------------+---------+\n| 5  | flipper_length_mm\\ | Mean (sd) : 200.9 (14.1)\\ | 55 distinct values  | \\ \\ \\ \\ \\ \\ :\\         | 2\\      |\n|    | [integer]          | min &lt; med &lt; max:\\         |                     | \\ \\ \\ \\ . :\\           | (0.6%)  |\n|    |                    | 172 &lt; 197 &lt; 231\\          |                     | \\ \\ \\ \\ : : : \\ \\ . .\\ |         |\n|    |                    | IQR (CV) : 23 (0.1)       |                     | \\ \\ . : : : \\ \\ : : :\\ |         |\n|    |                    |                           |                     | \\ \\ : : : : : : : : :  |         |\n+----+--------------------+---------------------------+---------------------+------------------------+---------+\n| 6  | body_mass_g\\       | Mean (sd) : 4201.8 (802)\\ | 94 distinct values  | \\ \\ \\ \\ :\\             | 2\\      |\n|    | [integer]          | min &lt; med &lt; max:\\         |                     | \\ \\ . :\\               | (0.6%)  |\n|    |                    | 2700 &lt; 4050 &lt; 6300\\       |                     | \\ \\ : : : :\\           |         |\n|    |                    | IQR (CV) : 1200 (0.2)     |                     | \\ \\ : : : : : .\\       |         |\n|    |                    |                           |                     | . : : : : : :          |         |\n+----+--------------------+---------------------------+---------------------+------------------------+---------+\n| 7  | sex\\               | 1\\. female\\               | 165 (49.5%)\\        | IIIIIIIII \\            | 11\\     |\n|    | [factor]           | 2\\. male                  | 168 (50.5%)         | IIIIIIIIII             | (3.2%)  |\n+----+--------------------+---------------------------+---------------------+------------------------+---------+\n| 8  | year\\              | Mean (sd) : 2008 (0.8)\\   | 2007 : 110 (32.0%)\\ | IIIIII \\               | 0\\      |\n|    | [integer]          | min &lt; med &lt; max:\\         | 2008 : 114 (33.1%)\\ | IIIIII \\               | (0.0%)  |\n|    |                    | 2007 &lt; 2008 &lt; 2009\\       | 2009 : 120 (34.9%)  | IIIIII                 |         |\n|    |                    | IQR (CV) : 2 (0)          |                     |                        |         |\n+----+--------------------+---------------------------+---------------------+------------------------+---------+\n\n\n\n3.3.8 Visualizing with visdat functions\nThe vis_dat() function from the visdat package shows something about the types of variables, providing visual clues about what’s inside. The picture below identifies variables types, and missing values.\n\nvis_dat(penguins)\n\n\n\n\n\n\n\nWe can explore the missing data further using the vis_miss function.\n\nvis_miss(penguins)\n\n\n\n\n\n\n\nA vignette explaining the use of the visdat package is available here.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summarizing Penguins</span>"
    ]
  },
  {
    "objectID": "03-penguins2.html#histograms-for-a-variable",
    "href": "03-penguins2.html#histograms-for-a-variable",
    "title": "3  Summarizing Penguins",
    "section": "\n3.4 Histograms for a Variable",
    "text": "3.4 Histograms for a Variable\nThe most common tool we use in producing a graphical summary of a variable, like the penguin’s flipper length, is a histogram. Here’s one option.\n\nggplot(data = penguins, aes(x = flipper_length_mm)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\nThis approach produces two messages that alert us to potential concerns, and a fairly unattractive plot.\nThis time, we’ll first exclude the two penguins without a measured flipper length, and then set the binwidth to be 10. How well does that work?\n\npenguins2 &lt;- \n  penguins |&gt;\n  filter(complete.cases(flipper_length_mm))\n\nggplot(data = penguins2, aes(x = flipper_length_mm)) +\n  geom_histogram(binwidth = 10)\n\n\n\n\n\n\n\nNow we’ve eliminated the messages, but it would be nice to have some more granularity in the bars (so we’d like a smaller binwidth) and I’d also like to make the bars more clearly separated with colors. I’d also like to add a title. Like this:\n\nggplot(data = penguins2, aes(x = flipper_length_mm)) +\n  geom_histogram(binwidth = 5, fill = \"orange\", col = \"navy\") +\n  labs(title = \"Distribution of Flipper Length in 342 Palmer Penguins\")\n\n\n\n\n\n\n\nThere are some other options for creating a graphical summary of a variable’s distribution. For example, we might consider a density plot, as well as a rug plot along the horizontal (X) axis:\n\nggplot(data = penguins2, aes(x = flipper_length_mm)) +\n  geom_density(col = \"navy\") +\n  geom_rug(col = \"red\") +\n  labs(title = \"Density and Rug Plot of Flipper Length in 342 Palmer Penguins\")\n\n\n\n\n\n\n\nOr perhaps a dotplot would provide a useful look…\n\nggplot(data = penguins2, aes(x = flipper_length_mm)) +\n  geom_dotplot(binwidth = 1, fill = \"orange\", col = \"navy\") +\n  labs(title = \"Dot Plot of Flipper Length in 342 Palmer Penguins\")\n\n\n\n\n\n\n\nWe’ll learn about several other approaches to summarizing the distribution of a variable graphically later in the course.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summarizing Penguins</span>"
    ]
  },
  {
    "objectID": "03-penguins2.html#comparing-penguins-by-species-numerically",
    "href": "03-penguins2.html#comparing-penguins-by-species-numerically",
    "title": "3  Summarizing Penguins",
    "section": "\n3.5 Comparing Penguins by Species Numerically",
    "text": "3.5 Comparing Penguins by Species Numerically\nWe have data from three different species of penguin. Can we compare their flipper lengths numerically, perhaps by calculating the mean flipper length within each species?\n\npenguins |&gt;\n  group_by(species) |&gt;\n  summarise(mean(flipper_length_mm))\n\n# A tibble: 3 × 2\n  species   `mean(flipper_length_mm)`\n  &lt;fct&gt;                         &lt;dbl&gt;\n1 Adelie                          NA \n2 Chinstrap                      196.\n3 Gentoo                          NA \n\n\nWell, that’s a problem. Looks like we have some missing values. Can we fix that, and also provide some additional summaries, like the sample size (n) and the median and standard deviation within each species? While we’re at it, can we make it prettier, with kbl() and kable_styling()?\n\npenguins |&gt;\n  filter(complete.cases(species, flipper_length_mm)) |&gt;\n  group_by(species) |&gt;\n  summarise(n = n(), \n            mean = mean(flipper_length_mm), \n            sd = sd(flipper_length_mm), \n            median = median(flipper_length_mm)) |&gt;\n  kbl() |&gt;\n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE)\n\n\n\nspecies\nn\nmean\nsd\nmedian\n\n\n\nAdelie\n151\n189.9536\n6.539457\n190\n\n\nChinstrap\n68\n195.8235\n7.131894\n196\n\n\nGentoo\n123\n217.1870\n6.484976\n216",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summarizing Penguins</span>"
    ]
  },
  {
    "objectID": "03-penguins2.html#using-favstats-from-the-mosaic-package",
    "href": "03-penguins2.html#using-favstats-from-the-mosaic-package",
    "title": "3  Summarizing Penguins",
    "section": "\n3.6 Using favstats() from the mosaic package",
    "text": "3.6 Using favstats() from the mosaic package\nAs we noted previously, we can also use favstats() from the mosaic package to help us look at the results for a single variable, split into groups by another, like this:\n\nmosaic::favstats(bill_length_mm ~ species, data = penguins) |&gt;\n  kbl() |&gt;\n  kable_styling()\n\n\n\nspecies\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\nAdelie\n32.1\n36.75\n38.80\n40.750\n46.0\n38.79139\n2.663405\n151\n1\n\n\nChinstrap\n40.9\n46.35\n49.55\n51.075\n58.0\n48.83382\n3.339256\n68\n0\n\n\nGentoo\n40.9\n45.30\n47.30\n49.550\n59.6\n47.50488\n3.081857\n123\n1\n\n\n\n\n\nOne advantage of this approach is that (as you’ll note) it handles the missing data in the way we’d probably expect, by restricting the summaries to the complete cases.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summarizing Penguins</span>"
    ]
  },
  {
    "objectID": "03-penguins2.html#using-tbl_summary-to-summarize-the-tibble",
    "href": "03-penguins2.html#using-tbl_summary-to-summarize-the-tibble",
    "title": "3  Summarizing Penguins",
    "section": "\n3.7 Using tbl_summary() to summarize the tibble",
    "text": "3.7 Using tbl_summary() to summarize the tibble\nThe tbl_summary() function from the gtsummary package can also do the job of summarizing all of the other variables in the tibble, broken down by species, very nicely.\n\npenguins |&gt; \n  tbl_summary(by = species)\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nAdelie, N = 1521\n\n\nChinstrap, N = 681\n\n\nGentoo, N = 1241\n\n\n\n\nisland\n\n\n\n\n\n    Biscoe\n44 (29%)\n0 (0%)\n124 (100%)\n\n\n    Dream\n56 (37%)\n68 (100%)\n0 (0%)\n\n\n    Torgersen\n52 (34%)\n0 (0%)\n0 (0%)\n\n\nbill_length_mm\n38.8 (36.8, 40.8)\n49.6 (46.4, 51.1)\n47.3 (45.3, 49.6)\n\n\n    Unknown\n1\n0\n1\n\n\nbill_depth_mm\n18.40 (17.50, 19.00)\n18.45 (17.50, 19.40)\n15.00 (14.20, 15.70)\n\n\n    Unknown\n1\n0\n1\n\n\nflipper_length_mm\n190 (186, 195)\n196 (191, 201)\n216 (212, 221)\n\n\n    Unknown\n1\n0\n1\n\n\nbody_mass_g\n3,700 (3,350, 4,000)\n3,700 (3,488, 3,950)\n5,000 (4,700, 5,500)\n\n\n    Unknown\n1\n0\n1\n\n\nsex\n\n\n\n\n\n    female\n73 (50%)\n34 (50%)\n58 (49%)\n\n\n    male\n73 (50%)\n34 (50%)\n61 (51%)\n\n\n    Unknown\n6\n0\n5\n\n\nyear\n\n\n\n\n\n    2007\n50 (33%)\n26 (38%)\n34 (27%)\n\n\n    2008\n50 (33%)\n18 (26%)\n46 (37%)\n\n\n    2009\n52 (34%)\n24 (35%)\n44 (35%)\n\n\n\n\n1 n (%); Median (IQR)",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summarizing Penguins</span>"
    ]
  },
  {
    "objectID": "03-penguins2.html#comparing-penguins-by-species-graphically",
    "href": "03-penguins2.html#comparing-penguins-by-species-graphically",
    "title": "3  Summarizing Penguins",
    "section": "\n3.8 Comparing Penguins by Species Graphically",
    "text": "3.8 Comparing Penguins by Species Graphically\n\n3.8.1 Faceting Histograms with facet_wrap()\nWe could compare the distributions of the flipper lengths across the three species, by creating a set of faceted histograms, like so…\n\npenguins3 &lt;- \n  penguins |&gt;\n  filter(complete.cases(flipper_length_mm, species))\n\nggplot(data = penguins3, aes(x = flipper_length_mm, fill = species)) +\n  geom_histogram(binwidth = 5, col = \"white\") +\n  facet_wrap(~ species) +\n  labs(title = \"Distribution of Flipper Length in Palmer Penguins, by Species\")\n\n\n\n\n\n\n\nWe might add in the command\n  guides(fill = \"none\") +\nto eliminate the redundant legend on the right-hand side of the plot.\n\n3.8.2 Using facet_grid() instead\nThe facet_wrap() approach has created three histograms, spread horizontally. Alternatively, we could plot the species vertically using facet_grid(), which clearly shows which species produces the penguins with the larger flipper lengths, especially if we reduce the width of the bins a bit.\n\nggplot(data = penguins3, aes(x = flipper_length_mm, fill = species)) +\n  geom_histogram(binwidth = 2, col = \"white\") +\n  facet_grid(species ~ .) +\n  guides(fill = \"none\") +\n  labs(title = \"Distribution of Flipper Length in Palmer Penguins, by Species\")\n\n\n\n\n\n\n\nWe’ll use facets like this all the time in what follows.\n\n3.8.3 Boxplots\nAnother very common tool we’ll use for looking simultaneously at the distributions of a variable across two or more categories is a boxplot. More on this later, but here’s one example of what this might look like.\n\nggplot(data = penguins3, aes(x = flipper_length_mm, y = species, \n                             fill = species)) +\n  geom_boxplot() +\n  guides(fill = \"none\") +\n  labs(title = \"Distribution of Flipper Length in Palmer Penguins, by Species\")\n\n\n\n\n\n\n\n\n3.8.4 Adding Violins\nAnd here’s a somewhat fancier version, including a violin plot, and with the coordinates flipped so the plots are shown vertically rather than horizontally.\n\nggplot(data = penguins3, aes(x = flipper_length_mm, y = species)) +\n  geom_violin(aes(col = species)) +\n  geom_boxplot(aes(fill = species), width = 0.3) +\n  guides(col = \"none\", fill = \"none\") +\n  coord_flip() +\n  labs(title = \"Augmented Boxplot of Flipper Length in Penguins by Species\")\n\n\n\n\n\n\n\n\n3.8.5 Letter-Value Plots (Boxplots for Large Data)\nWe might also consider a letter-value plot, using the geom_lv() function from the lvplot package in R, although I rarely use such a plot unless I have at least 1000 observations to work with.\n\nggplot(data = penguins3, aes(x = species, y = flipper_length_mm)) +\n  geom_lv(aes(fill=..LV..)) + scale_fill_brewer() +\n  labs(title = \"Letter-Value Plot of Flipper Length in Penguins by Species\")\n\nWarning: The dot-dot notation (`..LV..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(LV)` instead.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summarizing Penguins</span>"
    ]
  },
  {
    "objectID": "03-penguins2.html#coming-up",
    "href": "03-penguins2.html#coming-up",
    "title": "3  Summarizing Penguins",
    "section": "\n3.9 Coming Up",
    "text": "3.9 Coming Up\nYou’re probably tiring of the penguins now. Next, we’ll look at some data on people, taken from the National Health and Nutrition Examination Survey, or NHANES.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summarizing Penguins</span>"
    ]
  },
  {
    "objectID": "04-nhanes.html",
    "href": "04-nhanes.html",
    "title": "4  NHANES Data",
    "section": "",
    "text": "4.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(NHANES)\nlibrary(naniar)\nlibrary(kableExtra)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>NHANES Data</span>"
    ]
  },
  {
    "objectID": "04-nhanes.html#the-nhanes-data-a-first-sample",
    "href": "04-nhanes.html#the-nhanes-data-a-first-sample",
    "title": "4  NHANES Data",
    "section": "\n4.2 The NHANES data: A First Sample",
    "text": "4.2 The NHANES data: A First Sample\nThe NHANES package provides a sample of 10,000 NHANES responses from the 2009-10 and 2011-12 administrations, in a tibble also called NHANES. We can obtain the dimensions of this tibble with the dim() function.\n\nNHANES\n\n# A tibble: 10,000 × 76\n      ID SurveyYr Gender   Age AgeDecade AgeMonths Race1 Race3 Education   \n   &lt;int&gt; &lt;fct&gt;    &lt;fct&gt;  &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;       \n 1 51624 2009_10  male      34 \" 30-39\"        409 White &lt;NA&gt;  High School \n 2 51624 2009_10  male      34 \" 30-39\"        409 White &lt;NA&gt;  High School \n 3 51624 2009_10  male      34 \" 30-39\"        409 White &lt;NA&gt;  High School \n 4 51625 2009_10  male       4 \" 0-9\"           49 Other &lt;NA&gt;  &lt;NA&gt;        \n 5 51630 2009_10  female    49 \" 40-49\"        596 White &lt;NA&gt;  Some College\n 6 51638 2009_10  male       9 \" 0-9\"          115 White &lt;NA&gt;  &lt;NA&gt;        \n 7 51646 2009_10  male       8 \" 0-9\"          101 White &lt;NA&gt;  &lt;NA&gt;        \n 8 51647 2009_10  female    45 \" 40-49\"        541 White &lt;NA&gt;  College Grad\n 9 51647 2009_10  female    45 \" 40-49\"        541 White &lt;NA&gt;  College Grad\n10 51647 2009_10  female    45 \" 40-49\"        541 White &lt;NA&gt;  College Grad\n# ℹ 9,990 more rows\n# ℹ 67 more variables: MaritalStatus &lt;fct&gt;, HHIncome &lt;fct&gt;, HHIncomeMid &lt;int&gt;,\n#   Poverty &lt;dbl&gt;, HomeRooms &lt;int&gt;, HomeOwn &lt;fct&gt;, Work &lt;fct&gt;, Weight &lt;dbl&gt;,\n#   Length &lt;dbl&gt;, HeadCirc &lt;dbl&gt;, Height &lt;dbl&gt;, BMI &lt;dbl&gt;,\n#   BMICatUnder20yrs &lt;fct&gt;, BMI_WHO &lt;fct&gt;, Pulse &lt;int&gt;, BPSysAve &lt;int&gt;,\n#   BPDiaAve &lt;int&gt;, BPSys1 &lt;int&gt;, BPDia1 &lt;int&gt;, BPSys2 &lt;int&gt;, BPDia2 &lt;int&gt;,\n#   BPSys3 &lt;int&gt;, BPDia3 &lt;int&gt;, Testosterone &lt;dbl&gt;, DirectChol &lt;dbl&gt;, …\n\n\nWe see that we have 10000 rows and 76 columns in the NHANES tibble. For more on what makes a particular data frame into a tibble, and why we’d want such a thing, you might be interested in the Tibbles section of Hadley Wickham and Grolemund (2023). Essentially, tibbles are data frames that are easier and more predictable to work with.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>NHANES Data</span>"
    ]
  },
  {
    "objectID": "04-nhanes.html#newNHANES",
    "href": "04-nhanes.html#newNHANES",
    "title": "4  NHANES Data",
    "section": "\n4.3 Sampling NHANES Adults",
    "text": "4.3 Sampling NHANES Adults\nSuppose we want to take this NHANES tibble, and use it to generate a sample describing 750 unique (distinct) adult subjects who completed the 2011-12 version of the survey when they were between the ages of 21 and 64.\n\n4.3.1 Creating a Temporary, Cleaner Tibble\nI’ll start by describing the plan we will use to create a new tibble called nh_temp from which we will eventually build our final sample. In particular, let me lay out the steps I will use to create the nh_temp frame from the original NHANES tibble available in the R package called NHANES.\n\nWe’ll filter the original NHANES tibble to include only the responses from the 2011-12 administration of the survey. This will cut the sample in half, from 10,000 rows to 5,000.\nWe’ll then filter again to restrict the sample to adults whose age is at least 21 and also less than 65. I’ll do this because I want to avoid problems with including both children and adults in my sample, and because I also want to focus on the population of people in the US who are usually covered by private insurance from their job, or by Medicaid insurance from the government, rather than those covered by Medicare.\nWhat is listed in the NHANES tibble as Gender should be more correctly referred to as Sex. Sex is a biological feature of an individual, while Gender is a social construct. This is an important distinction, so I’ll change the name of the variable.\nWe’ll also rename three other variables, specifically we’ll use Race to describe the Race3 variable in the original NHANES tibble, as well as SBP to refer to the average systolic blood pressure, which is specified as BPSysAve, and DBP to refer to the average diastolic blood pressure, which is specified as BPDiaAve.\nHaving accomplished the previous four steps, we’ll then select the variables we want to keep in the sample. (We use select for choosing variables or columns in the tibble, and filter for selecting subjects or rows.) The sixteen variables we will select are: ID, Sex, Age, Height, Weight, Race, Education, BMI, SBP, DBP, Pulse, PhysActive, Smoke100, SleepTrouble, MaritalStatus and HealthGen.\nThe original NHANES tibble includes some subjects (rows) multiple times in an effort to incorporate some of the sampling weights used in most NHANES analyses. For our purposes, though, we’d like to only include each subject one time. We use the distinct() function to limit the tibble to completely unique subjects (so that, for example, we don’t wind up with two or more rows that have the same ID number.)\n\nHere is the code I used to complete the six steps listed above and create the nh_temp tibble.\n\nnh_temp &lt;- NHANES |&gt;\n    filter(SurveyYr == \"2011_12\") |&gt;\n    filter(Age &gt;= 21 & Age &lt; 65) |&gt;\n    rename(Sex = Gender, Race = Race3, SBP = BPSysAve, DBP = BPDiaAve) |&gt;\n    select(ID, Sex, Age, Height, Weight, Race, Education, BMI, SBP, DBP, \n           Pulse, PhysActive, Smoke100, SleepTrouble, \n           MaritalStatus, HealthGen) |&gt;\n   distinct()\n\nThe resulting nh_temp tibble has 1700 rows and 16 columns.\n\nnh_temp\n\n# A tibble: 1,700 × 16\n      ID Sex      Age Height Weight Race     Education     BMI   SBP   DBP Pulse\n   &lt;int&gt; &lt;fct&gt;  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 62172 female    43   172    98.6 Black    High School  33.3   103    72    80\n 2 62176 female    34   172.   68.7 White    College Gr…  23.3   107    69    92\n 3 62180 male      35   179.   89   White    College Gr…  27.9   107    66    66\n 4 62199 male      57   186    96.9 White    College Gr…  28     110    65    84\n 5 62205 male      28   171.   84.8 White    College Gr…  28.9   122    87    70\n 6 62206 female    35   167.   81.5 White    Some Colle…  29.1   106    50    58\n 7 62208 male      38   169.   63.2 Hispanic Some Colle…  22.2   105    59    52\n 8 62209 female    62   143.   53.5 Mexican  8th Grade    26     108    57    72\n 9 62220 female    31   167.  113.  Black    College Gr…  40.4   120    71    62\n10 62222 male      32   179    80.1 White    College Gr…  25     104    73    78\n# ℹ 1,690 more rows\n# ℹ 5 more variables: PhysActive &lt;fct&gt;, Smoke100 &lt;fct&gt;, SleepTrouble &lt;fct&gt;,\n#   MaritalStatus &lt;fct&gt;, HealthGen &lt;fct&gt;\n\n\n\n4.3.2 Sampling nh_temp to obtain our nh_adult750 sample\nHaving established the nh_temp tibble, we now select a random sample of 750 adults from the 1700 available responses.\n\nWe will use the set.seed() function in R to set a random numerical seed to ensure that if you redo this work, you will obtain the same sample.\n\nSetting a seed is an important part of being able to replicate the work later when sampling is involved. If you and I use the same seed, we should get the same sample.\n\n\nThen we will use the slice_sample() function to actually draw the random sample, without replacement.\n\n“Without replacement” means that once we’ve selected a particular subject, we won’t select them again.\n\n\n\n\nset.seed(431002) \n# use set.seed to ensure that we all get the same random sample \n\nnh_adult750 &lt;- slice_sample(nh_temp, n = 750, replace = F) \n\nnh_adult750\n\n# A tibble: 750 × 16\n      ID Sex      Age Height Weight Race     Education     BMI   SBP   DBP Pulse\n   &lt;int&gt; &lt;fct&gt;  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 68648 female    30   181.   67.1 White    College Gr…  20.4   103    59    78\n 2 67200 male      30   180.   86.6 White    College Gr…  26.7   113    68    70\n 3 66404 female    35   160.   71.1 White    College Gr…  27.8   116    80    68\n 4 70535 male      40   177.   82   White    College Gr…  26.3   130    79    68\n 5 65308 female    54   151.   60.6 Mexican  8th Grade    26.6   130    64    48\n 6 67392 male      41   171.   90.7 Hispanic College Gr…  31.2   124    82    68\n 7 63218 male      35   163.   81   Mexican  8th Grade    30.3   128    96    82\n 8 65879 female    32   160.   66.4 Mexican  College Gr…  25.9   104    70    78\n 9 63617 male      29   189.   83.3 White    College Gr…  23.2   105    72    76\n10 64720 male      29   174.   62.3 Black    College Gr…  20.6   127    60    84\n# ℹ 740 more rows\n# ℹ 5 more variables: PhysActive &lt;fct&gt;, Smoke100 &lt;fct&gt;, SleepTrouble &lt;fct&gt;,\n#   MaritalStatus &lt;fct&gt;, HealthGen &lt;fct&gt;\n\n\nThe nh_adult750 tibble now includes 750 rows (observations) on 16 variables (columns). Essentially, we have 16 pieces of information on each of 750 adult NHANES subjects who were included in the 2011-12 panel.\n\n4.3.3 Summarizing the Data’s Structure\nWe can identify the number of rows and columns in a data frame or tibble with the dim function.\n\ndim(nh_adult750)\n\n[1] 750  16\n\n\nThe str function provides a lot of information about the structure of a data frame or tibble.\n\nstr(nh_adult750)\n\ntibble [750 × 16] (S3: tbl_df/tbl/data.frame)\n $ ID           : int [1:750] 68648 67200 66404 70535 65308 67392 63218 65879 63617 64720 ...\n $ Sex          : Factor w/ 2 levels \"female\",\"male\": 1 2 1 2 1 2 2 1 2 2 ...\n $ Age          : int [1:750] 30 30 35 40 54 41 35 32 29 29 ...\n $ Height       : num [1:750] 181 180 160 177 151 ...\n $ Weight       : num [1:750] 67.1 86.6 71.1 82 60.6 90.7 81 66.4 83.3 62.3 ...\n $ Race         : Factor w/ 6 levels \"Asian\",\"Black\",..: 5 5 5 5 4 3 4 4 5 2 ...\n $ Education    : Factor w/ 5 levels \"8th Grade\",\"9 - 11th Grade\",..: 5 5 5 5 1 5 1 5 5 5 ...\n $ BMI          : num [1:750] 20.4 26.7 27.8 26.3 26.6 31.2 30.3 25.9 23.2 20.6 ...\n $ SBP          : int [1:750] 103 113 116 130 130 124 128 104 105 127 ...\n $ DBP          : int [1:750] 59 68 80 79 64 82 96 70 72 60 ...\n $ Pulse        : int [1:750] 78 70 68 68 48 68 82 78 76 84 ...\n $ PhysActive   : Factor w/ 2 levels \"No\",\"Yes\": 1 2 2 1 1 2 1 2 2 2 ...\n $ Smoke100     : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 2 2 1 2 1 2 2 ...\n $ SleepTrouble : Factor w/ 2 levels \"No\",\"Yes\": 2 1 1 1 1 1 1 1 2 1 ...\n $ MaritalStatus: Factor w/ 6 levels \"Divorced\",\"LivePartner\",..: 3 4 3 3 2 3 3 3 3 2 ...\n $ HealthGen    : Factor w/ 5 levels \"Excellent\",\"Vgood\",..: 1 1 1 2 4 3 NA 1 2 4 ...\n\n\nTo see the first few observations, use head, and to see the last few, try tail…\n\ntail(nh_adult750, 5) # shows the last five observations in the data set\n\n# A tibble: 5 × 16\n     ID Sex      Age Height Weight Race  Education      BMI   SBP   DBP Pulse\n  &lt;int&gt; &lt;fct&gt;  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 63924 female    29   165.  113.  Black High School   41.9    98    56    74\n2 69825 female    43   164.   63.3 White College Grad  23.7   122    83    88\n3 68109 male      45   170.   78.7 Black High School   27.1   140    79   102\n4 64598 female    60   158    74.5 White Some College  29.8   137    80    78\n5 64048 female    54   161.   67.5 White Some College  26.2   121    87    72\n# ℹ 5 more variables: PhysActive &lt;fct&gt;, Smoke100 &lt;fct&gt;, SleepTrouble &lt;fct&gt;,\n#   MaritalStatus &lt;fct&gt;, HealthGen &lt;fct&gt;\n\n\n\n4.3.4 What are the variables?\nWe can use the glimpse function to get a short preview of the data.\n\nglimpse(nh_adult750)\n\nRows: 750\nColumns: 16\n$ ID            &lt;int&gt; 68648, 67200, 66404, 70535, 65308, 67392, 63218, 65879, …\n$ Sex           &lt;fct&gt; female, male, female, male, female, male, male, female, …\n$ Age           &lt;int&gt; 30, 30, 35, 40, 54, 41, 35, 32, 29, 29, 64, 28, 31, 59, …\n$ Height        &lt;dbl&gt; 181.3, 180.2, 159.8, 176.6, 150.9, 170.6, 163.4, 160.2, …\n$ Weight        &lt;dbl&gt; 67.1, 86.6, 71.1, 82.0, 60.6, 90.7, 81.0, 66.4, 83.3, 62…\n$ Race          &lt;fct&gt; White, White, White, White, Mexican, Hispanic, Mexican, …\n$ Education     &lt;fct&gt; College Grad, College Grad, College Grad, College Grad, …\n$ BMI           &lt;dbl&gt; 20.4, 26.7, 27.8, 26.3, 26.6, 31.2, 30.3, 25.9, 23.2, 20…\n$ SBP           &lt;int&gt; 103, 113, 116, 130, 130, 124, 128, 104, 105, 127, 128, 1…\n$ DBP           &lt;int&gt; 59, 68, 80, 79, 64, 82, 96, 70, 72, 60, 74, 76, 82, 66, …\n$ Pulse         &lt;int&gt; 78, 70, 68, 68, 48, 68, 82, 78, 76, 84, 62, 56, 78, 66, …\n$ PhysActive    &lt;fct&gt; No, Yes, Yes, No, No, Yes, No, Yes, Yes, Yes, Yes, No, N…\n$ Smoke100      &lt;fct&gt; No, Yes, No, Yes, Yes, No, Yes, No, Yes, Yes, No, No, Ye…\n$ SleepTrouble  &lt;fct&gt; Yes, No, No, No, No, No, No, No, Yes, No, No, Yes, No, Y…\n$ MaritalStatus &lt;fct&gt; Married, NeverMarried, Married, Married, LivePartner, Ma…\n$ HealthGen     &lt;fct&gt; Excellent, Excellent, Excellent, Vgood, Fair, Good, NA, …\n\n\nThe variables we have collected are described in the brief table below1.\n\n\n\n\n\n\n\nVariable\nDescription\nSample Values\n\n\n\nID\na numerical code identifying the subject\n68648, 67200\n\n\nSex\nsex of subject (2 levels)\nfemale, male\n\n\nAge\nage (years) at screening of subject\n30, 35\n\n\nHeight\nheight (in cm) at screening of subject\n181.3, 180.2\n\n\nWeight\nweight (in kg) at screening of subject\n67.1, 86.6\n\n\nRace\nreported race of subject (6 levels)\nWhite, Black\n\n\nEducation\neducational level of subject (5 levels)\nCollege Grad, High School\n\n\nBMI\nbody-mass index, in kg/m2\n\n20.4, 26.7\n\n\nSBP\nsystolic blood pressure in mm Hg\n103, 113\n\n\nDBP\ndiastolic blood pressure in mm Hg\n59, 68\n\n\nPulse\n60 second pulse rate in beats per minute\n78, 70\n\n\nPhysActive\nModerate or vigorous-intensity sports?\nYes, No\n\n\nSmoke100\nSmoked at least 100 cigarettes lifetime?\nYes, No\n\n\nSleepTrouble\nTold a doctor they have trouble sleeping?\nYes, No\n\n\nMaritalStatus\nMarital Status\nMarried, Divorced\n\n\nHealthGen\nSelf-report general health rating (5 levels)\nVgood, Fair\n\n\n\nThe levels for the multi-categorical variables are:\n\n\nRace: Mexican, Hispanic, White, Black, Asian, or Other.\n\nEducation: 8th Grade, 9 - 11th Grade, High School, Some College, or College Grad.\n\nMaritalStatus: Married, Widowed, Divorced, Separated, NeverMarried or LivePartner (living with partner).\n\nHealthGen: Excellent, Vgood, Good, Fair or Poor.\n\nSome details can be obtained using the summary function, or any of the other approaches we saw used with the penguins data earlier.\n\nsummary(nh_adult750)\n\n       ID            Sex           Age            Height          Weight      \n Min.   :62206   female:388   Min.   :21.00   Min.   :142.4   Min.   : 39.30  \n 1st Qu.:64277   male  :362   1st Qu.:30.00   1st Qu.:161.8   1st Qu.: 67.40  \n Median :66925                Median :40.00   Median :168.9   Median : 80.00  \n Mean   :66936                Mean   :40.82   Mean   :168.9   Mean   : 83.16  \n 3rd Qu.:69414                3rd Qu.:51.00   3rd Qu.:175.7   3rd Qu.: 95.30  \n Max.   :71911                Max.   :64.00   Max.   :200.4   Max.   :198.70  \n                                              NA's   :5       NA's   :5       \n       Race              Education        BMI             SBP       \n Asian   : 70   8th Grade     : 50   Min.   :16.70   Min.   : 83.0  \n Black   :128   9 - 11th Grade: 76   1st Qu.:24.20   1st Qu.:108.0  \n Hispanic: 63   High School   :143   Median :27.90   Median :118.0  \n Mexican : 80   Some College  :241   Mean   :29.08   Mean   :118.8  \n White   :393   College Grad  :240   3rd Qu.:32.10   3rd Qu.:127.0  \n Other   : 16                        Max.   :80.60   Max.   :209.0  \n                                     NA's   :5       NA's   :33     \n      DBP             Pulse        PhysActive Smoke100  SleepTrouble\n Min.   :  0.00   Min.   : 40.00   No :326    No :453   No :555     \n 1st Qu.: 66.00   1st Qu.: 66.00   Yes:424    Yes:297   Yes:195     \n Median : 73.00   Median : 72.00                                    \n Mean   : 72.69   Mean   : 73.53                                    \n 3rd Qu.: 80.00   3rd Qu.: 80.00                                    \n Max.   :108.00   Max.   :124.00                                    \n NA's   :33       NA's   :32                                        \n      MaritalStatus     HealthGen  \n Divorced    : 78   Excellent: 84  \n LivePartner : 70   Vgood    :197  \n Married     :388   Good     :252  \n NeverMarried:179   Fair     :104  \n Separated   : 19   Poor     : 14  \n Widowed     : 16   NA's     : 99  \n                                   \n\n\nNote the appearance of NA's (indicating missing values) in some columns, and that some variables are summarized by a list of their (categorical) values (with counts) and some (quantitative/numeric) variables are summarized with a minimum, quartiles and means.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>NHANES Data</span>"
    ]
  },
  {
    "objectID": "04-nhanes.html#counting-missing-values",
    "href": "04-nhanes.html#counting-missing-values",
    "title": "4  NHANES Data",
    "section": "\n4.4 Counting Missing Values",
    "text": "4.4 Counting Missing Values\nThe summary() command counts the number of missing observations in each variable, but sometimes you want considerably more information.\nWe can use some functions from the naniar package to learn useful things about the missing data in our nh_adult750 sample. (Recall that we could also use the vis_miss() function from the visdat package, as we saw earlier with the penguins to get some of this information, but the naniar approach provides more exploratory tools.)\nThe miss_var_table command provides a table of the number of variables with 0, 1, 2, up to n, missing values and the percentage of the total number of variables those variables make up.\n\nmiss_var_table(nh_adult750)\n\n# A tibble: 5 × 3\n  n_miss_in_var n_vars pct_vars\n          &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;\n1             0      9    56.2 \n2             5      3    18.8 \n3            32      1     6.25\n4            33      2    12.5 \n5            99      1     6.25\n\n\nSo, for instance, we have 9 variables with no missing data, and that constitutes 56.25% of the 16 variables in our nh_adult750 data.\nThe miss_var_summary() function tabulates the number, percent missing, and cumulative sum of missing of each variable in our tibble, in order of most to least missing values.\n\nmiss_var_summary(nh_adult750) |&gt;\n  kbl() |&gt;\n  kable_styling(full_width = FALSE, position = \"center\")\n\n\n\nvariable\nn_miss\npct_miss\n\n\n\nHealthGen\n99\n13.2\n\n\nSBP\n33\n4.4\n\n\nDBP\n33\n4.4\n\n\nPulse\n32\n4.27\n\n\nHeight\n5\n0.667\n\n\nWeight\n5\n0.667\n\n\nBMI\n5\n0.667\n\n\nID\n0\n0\n\n\nSex\n0\n0\n\n\nAge\n0\n0\n\n\nRace\n0\n0\n\n\nEducation\n0\n0\n\n\nPhysActive\n0\n0\n\n\nSmoke100\n0\n0\n\n\nSleepTrouble\n0\n0\n\n\nMaritalStatus\n0\n0\n\n\n\n\n\nSo, for example, the HealthGen variable is the one missing more of our data than anything else within the nh_adult750 tibble.\nA graph of this information is available, as well.\n\ngg_miss_var(nh_adult750)\n\n\n\n\n\n\n\nI’ll note that there are also functions to count the number of missing observations by case (observation) rather than variable. For example, we can use miss_case_table.\n\nmiss_case_table(nh_adult750)\n\n# A tibble: 6 × 3\n  n_miss_in_case n_cases pct_cases\n           &lt;int&gt;   &lt;int&gt;     &lt;dbl&gt;\n1              0     636    84.8  \n2              1      78    10.4  \n3              3      15     2    \n4              4      19     2.53 \n5              6       1     0.133\n6              7       1     0.133\n\n\nNow we see that 636 observations, or 84.8% of all cases have no missing data.\nWe can use miss_case_summary() to identify cases with missing data, as well.\n\nmiss_case_summary(nh_adult750)\n\n# A tibble: 750 × 3\n    case n_miss pct_miss\n   &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;\n 1   342      7     43.8\n 2   606      6     37.5\n 3   157      4     25  \n 4   169      4     25  \n 5   204      4     25  \n 6   234      4     25  \n 7   323      4     25  \n 8   415      4     25  \n 9   478      4     25  \n10   483      4     25  \n# ℹ 740 more rows",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>NHANES Data</span>"
    ]
  },
  {
    "objectID": "04-nhanes.html#sec-nh_cc",
    "href": "04-nhanes.html#sec-nh_cc",
    "title": "4  NHANES Data",
    "section": "\n4.5 Sampling 500 Complete Cases",
    "text": "4.5 Sampling 500 Complete Cases\nIf we wanted a sample of exactly 750 subjects with complete data, we would have needed to add a step in the development of our nh_temp tibble to filter for complete cases.\n\nnh_temp2 &lt;- NHANES |&gt;\n    filter(SurveyYr == \"2011_12\") |&gt;\n    filter(Age &gt;= 21 & Age &lt; 65) |&gt;\n    rename(Sex = Gender, Race = Race3, SBP = BPSysAve, DBP = BPDiaAve) |&gt;\n    select(ID, Sex, Age, Height, Weight, Race, Education, BMI, SBP, DBP, \n           Pulse, PhysActive, Smoke100, SleepTrouble, \n           MaritalStatus, HealthGen) |&gt;\n    distinct() |&gt;\n    na.omit()\n\nLet’s check that this new tibble has no missing data.\n\nmiss_var_table(nh_temp2)\n\n# A tibble: 1 × 3\n  n_miss_in_var n_vars pct_vars\n          &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;\n1             0     16      100\n\n\nOK. Now, let’s create a second sample, called nh_adult500cc, where now, we will select 500 adults with complete data on all of the variables of interest, and using a different random seed. The cc here stands for complete cases.\n\nset.seed(431003) \n# use set.seed to ensure that we all get the same random sample \n\nnh_adult500cc &lt;- slice_sample(nh_temp2, n = 500, replace = F) \n\nnh_adult500cc\n\n# A tibble: 500 × 16\n      ID Sex      Age Height Weight Race     Education     BMI   SBP   DBP Pulse\n   &lt;int&gt; &lt;fct&gt;  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 64079 female    25   159.   86.2 Hispanic Some Colle…  34.2   120    67    84\n 2 64374 female    52   169    65.5 Asian    College Gr…  22.9    92    58    60\n 3 71875 male      42   182.   94.1 Black    College Gr…  28.5   102    63    76\n 4 66396 female    46   161.  107.  Asian    8th Grade    41.2   111    61    70\n 5 64315 female    52   161.   64.5 White    9 - 11th G…  24.9   130    69    68\n 6 64015 male      32   168.   82.3 Mexican  Some Colle…  29     119    79    70\n 7 63590 male      21   181.   98.3 Black    Some Colle…  29.9   121    67    58\n 8 70893 female    30   171.   65.7 White    9 - 11th G…  22.5   104    75    74\n 9 70828 male      26   178.  100.  White    Some Colle…  31.5   119    77    66\n10 67930 male      59   172.   91.7 Mexican  College Gr…  31     127    85    66\n# ℹ 490 more rows\n# ℹ 5 more variables: PhysActive &lt;fct&gt;, Smoke100 &lt;fct&gt;, SleepTrouble &lt;fct&gt;,\n#   MaritalStatus &lt;fct&gt;, HealthGen &lt;fct&gt;",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>NHANES Data</span>"
    ]
  },
  {
    "objectID": "04-nhanes.html#saving-our-samples-in-.rds-files",
    "href": "04-nhanes.html#saving-our-samples-in-.rds-files",
    "title": "4  NHANES Data",
    "section": "\n4.6 Saving our Samples in .Rds files",
    "text": "4.6 Saving our Samples in .Rds files\nWe’ll save the nh_adult750 and nh_adult500cc samples to use in later parts of the notes. To do this, we’ll save them as .Rds files, which are files we can read directly into R with the read_rds command, and which will have some advantages for us later on.\n\nwrite_rds(nh_adult750, file = \"data/nh_adult750.Rds\")\nwrite_rds(nh_adult500cc, file = \"data/nh_adult500cc.Rds\")\n\nYou will also find these .Rds files as part of the 431-data repository for the course.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>NHANES Data</span>"
    ]
  },
  {
    "objectID": "04-nhanes.html#coming-up",
    "href": "04-nhanes.html#coming-up",
    "title": "4  NHANES Data",
    "section": "\n4.7 Coming Up",
    "text": "4.7 Coming Up\nNext, we’ll introduce some new ways of thinking about data and variables as we load, explore and learn about some of the variables in our two NHANES samples.\n\n\n\n\nHadley Wickham, Mine Çetinyaka-Rundel, and Garrett Grolemund. 2023. R for Data Science. Second. O’Reilly. https://r4ds.hadley.nz/.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>NHANES Data</span>"
    ]
  },
  {
    "objectID": "04-nhanes.html#footnotes",
    "href": "04-nhanes.html#footnotes",
    "title": "4  NHANES Data",
    "section": "",
    "text": "Descriptions are adapted from the ?NHANES help file. Remember that what NHANES lists as Gender is captured here as Sex, and similarly Race3, BPSysAve and BPDiaAve from NHANES are here listed as Race, SBP and DBP.↩︎",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>NHANES Data</span>"
    ]
  },
  {
    "objectID": "05-data_types.html",
    "href": "05-data_types.html",
    "title": "\n5  Types of Data\n",
    "section": "",
    "text": "5.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor)\nlibrary(gtsummary)\nlibrary(kableExtra)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\nWe’ll also use the describe() function from the psych package in what follows, but I won’t load the whole psych package here.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Types of Data</span>"
    ]
  },
  {
    "objectID": "05-data_types.html#data-require-structure-and-context",
    "href": "05-data_types.html#data-require-structure-and-context",
    "title": "\n5  Types of Data\n",
    "section": "\n5.2 Data require structure and context",
    "text": "5.2 Data require structure and context\nDescriptive statistics are concerned with the presentation, organization and summary of data, as suggested in Norman and Streiner (2014). This includes various methods of organizing and graphing data to get an idea of what those data can tell us.\nAs Vittinghoff et al. (2012) suggest, the nature of the measurement determines how best to describe it statistically, and the main distinction is between numerical and categorical variables. Even this is a little tricky - plenty of data can have values that look like numerical values, but are just numerals serving as labels.\nAs Bock, Velleman, and De Veaux (2004) point out, the truly critical notion, of course, is that data values, no matter what kind, are useless without their contexts. The Five W’s (Who, What [and in what units], When, Where, Why, and often How) are just as useful for establishing the context of data as they are in journalism. If you can’t answer Who and What, in particular, you don’t have any useful information.\nIn general, each row of a data frame corresponds to an individual (respondent, experimental unit, record, or observation) about whom some characteristics are gathered in columns (and these characteristics may be called variables, factors or data elements.) Every column / variable should have a name that indicates what it is measuring, and every row / observation should have a name that indicates who is being measured.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Types of Data</span>"
    ]
  },
  {
    "objectID": "05-data_types.html#reading-in-the-complete-cases-sample",
    "href": "05-data_types.html#reading-in-the-complete-cases-sample",
    "title": "\n5  Types of Data\n",
    "section": "\n5.3 Reading in the “Complete Cases” Sample",
    "text": "5.3 Reading in the “Complete Cases” Sample\nLet’s begin by loading into the nh_500cc tibble the information from the nh_adult500cc.Rds file we created in Section 4.5. Notice that I am simplifying the name of the tibble, to save me some typing.\n\nnh_500cc &lt;- read_rds(\"data/nh_adult500cc.Rds\")\n\nOne obvious hurdle we’ll avoid for the moment is what to do about missing data, since the nh_500cc data are specifically drawn from complete responses. Working with complete cases only can introduce bias to our estimates and visualizations, so it will be necessary in time to address what we should do when a complete-case analysis isn’t a good choice. We’ll return to this issue later.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Types of Data</span>"
    ]
  },
  {
    "objectID": "05-data_types.html#quantitative-variables",
    "href": "05-data_types.html#quantitative-variables",
    "title": "\n5  Types of Data\n",
    "section": "\n5.4 Quantitative Variables",
    "text": "5.4 Quantitative Variables\nVariables recorded in numbers that we use as numbers are called quantitative. Familiar examples include incomes, heights, weights, ages, distances, times, and counts. All quantitative variables have measurement units, which tell you how the quantitative variable was measured. Without units (like miles per hour, angstroms, yen or degrees Celsius) the values of a quantitative variable have no meaning.\n\nIt does little good to be told the price of something if you don’t know the currency being used.\nYou might be surprised to see someone whose age is 72 listed in a database on childhood diseases until you find out that age is measured in months.\nOften just seeking the units can reveal a variable whose definition is challenging - just how do we measure “friendliness”, or “success,” for example.\nQuantitative variables may also be classified by whether they are continuous or can only take on a discrete set of values. Continuous data may take on any value, within a defined range. Suppose we are measuring height. While height is really continuous, our measuring stick usually only lets us measure with a certain degree of precision. If our measurements are only trustworthy to the nearest centimeter with the ruler we have, we might describe them as discrete measures. But we could always get a more precise ruler. The measurement divisions we make in moving from a continuous concept to a discrete measurement are usually fairly arbitrary. Another way to think of this, if you enjoy music, is that, as suggested in Norman and Streiner (2014), a piano is a discrete instrument, but a violin is a continuous one, enabling finer distinctions between notes than the piano is capable of making. Sometimes the distinction between continuous and discrete is important, but usually, it’s not.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Types of Data</span>"
    ]
  },
  {
    "objectID": "05-data_types.html#quantitative-variables-in-nh_500cc",
    "href": "05-data_types.html#quantitative-variables-in-nh_500cc",
    "title": "\n5  Types of Data\n",
    "section": "\n5.5 Quantitative Variables in nh_500cc\n",
    "text": "5.5 Quantitative Variables in nh_500cc\n\nHere’s a list of the variables contained in our nh_500cc tibble.\n\nnames(nh_500cc)\n\n [1] \"ID\"            \"Sex\"           \"Age\"           \"Height\"       \n [5] \"Weight\"        \"Race\"          \"Education\"     \"BMI\"          \n [9] \"SBP\"           \"DBP\"           \"Pulse\"         \"PhysActive\"   \n[13] \"Smoke100\"      \"SleepTrouble\"  \"MaritalStatus\" \"HealthGen\"    \n\n\nThe nh_500cc data includes seven quantitative variables, including Age, Height, Weight, BMI, SBP, DBP and Pulse.\n\nWe know these are quantitative variables because they have units:\n\n\nAge in years, Height in centimeters, Weight in kilograms,\n\nBMI in kg/m2, the BP measurements in mm Hg, and Pulse in beats per minute.\n\n\n\nLet’s summarize them with the describe() function from the psych package.\n\nnh_500cc |&gt;\n  select(Age, Height, Weight, BMI, SBP, DBP, Pulse) |&gt;\n  psych::describe() |&gt;\n  kbl() |&gt;\n  kable_styling()\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\nAge\n1\n500\n41.6060\n12.803853\n42.00\n41.48000\n16.30860\n21.0\n64.0\n43.0\n0.0469923\n-1.2377866\n0.5726057\n\n\nHeight\n2\n500\n169.3726\n9.935372\n169.05\n169.19025\n10.52646\n144.8\n200.4\n55.6\n0.1618323\n-0.2539869\n0.4443233\n\n\nWeight\n3\n500\n82.7094\n20.884652\n80.00\n81.05650\n19.94097\n41.9\n184.5\n142.6\n0.9835819\n1.7024261\n0.9339900\n\n\nBMI\n4\n500\n28.7574\n6.567995\n27.70\n28.11625\n6.07866\n17.0\n63.3\n46.3\n1.1522475\n2.2838638\n0.2937297\n\n\nSBP\n5\n500\n119.3320\n15.049344\n119.00\n118.41250\n13.34340\n84.0\n221.0\n137.0\n1.1872427\n4.7556753\n0.6730271\n\n\nDBP\n6\n500\n72.1580\n11.245709\n72.00\n72.24500\n8.89560\n0.0\n110.0\n110.0\n-0.5339975\n4.5794674\n0.5029234\n\n\nPulse\n7\n500\n74.1640\n11.500505\n74.00\n73.76500\n11.86080\n48.0\n114.0\n66.0\n0.3414617\n-0.0027802\n0.5143182\n\n\n\n\n\nAs an alternative, we could use tbl_summary() from the gtsummary package, as well. The approach below works nicely for producing a mean, standard deviation, and five-number summary for each of the variables we’ve identified as quantitative.\n\nQuantitative variables lend themselves to many of the summaries we will discuss, like means, quantiles, and our various measures of spread, like the standard deviation or inter-quartile range. They also have at least a chance to follow the Normal distribution.\n\n\nnh_500cc |&gt;\n  select(Age, Height, Weight, BMI, SBP, DBP, Pulse) |&gt;\n  tbl_summary( statistic = list(all_continuous() ~ \"{mean} ({sd}): \n                     [ {min}, {p25}, {median}, {p75}, {max} ]\")) \n\n\n\n\n\nCharacteristic\n\nN = 5001\n\n\n\n\nAge\n42 (13): [ 21, 30, 42, 52, 64 ]\n\n\nHeight\n169 (10): [ 145, 162, 169, 176, 200 ]\n\n\nWeight\n83 (21): [ 42, 68, 80, 94, 185 ]\n\n\nBMI\n29 (7): [ 17, 24, 28, 32, 63 ]\n\n\nSBP\n119 (15): [ 84, 110, 119, 127, 221 ]\n\n\nDBP\n72 (11): [ 0, 66, 72, 79, 110 ]\n\n\nPulse\n74 (12): [ 48, 66, 74, 82, 114 ]\n\n\n\n\n1 Mean (SD): [ Minimum, 25%, Median, 75%, Maximum ]\n\n\n\n\n\n\n5.5.1 A look at BMI (Body-Mass Index)\nThe definition of BMI (body-mass index) for adult subjects (which is expressed in units of kg/m2) is:\n\\[\n\\mbox{Body Mass Index} = \\frac{\\mbox{weight in kg}}{(\\mbox{height in meters})^2} = 703 \\times \\frac{\\mbox{weight in pounds}}{(\\mbox{height in inches})^2}\n\\]\n\n[BMI is essentially] … a measure of a person’s thinness or thickness… BMI was designed for use as a simple means of classifying average sedentary (physically inactive) populations, with an average body composition. For these individuals, the current value recommendations are as follow: a BMI from 18.5 up to 25 may indicate optimal weight, a BMI lower than 18.5 suggests the person is underweight, a number from 25 up to 30 may indicate the person is overweight, and a number from 30 upwards suggests the person is obese.\nWikipedia, https://en.wikipedia.org/wiki/Body_mass_index\n\n\n5.5.2 Types of Quantitative Variables\nDepending on the context, we would likely treat most of these quantitative variables as discrete given that are measurements are fairly crude (this is certainly true for Age, measured in years) although BMI is probably continuous in most settings, even though it is a function of two other measures (Height and Weight) which are rounded off to integer numbers of centimeters and kilograms, respectively.\nIt is also possible to separate out quantitative variables into ratio variables or interval variables.\n\nAn interval variable has equal distances between values, but the zero point is arbitrary.\nA ratio variable has equal intervals between values, and a meaningful zero point.\n\nFor example, weight is an example of a ratio variable, while IQ is an example of an interval variable. We all know what zero weight is. An intelligence score like IQ is a different matter. We say that the average IQ is 100, but that’s only by convention. We could just as easily have decided to add 400 to every IQ value and make the average 500 instead. Because IQ’s intervals are equal, the difference between and IQ of 70 and an IQ of 80 is the same as the difference between 120 and 130. However, an IQ of 100 is not twice as high as an IQ of 50. The point is that if the zero point is artificial and movable, then the differences between numbers are meaningful but the ratios between them are not.\nOn the other hand, most lab test values are ratio variables, as are physical characteristics like height and weight. Each of the quantitative variables in our nh_500cc data can be thought of as a ratio variable.A person who weighs 100 kg is twice as heavy as one who weighs 50 kg; even when we convert kg to pounds, this is still true. For the most part, we can treat and analyze interval or ratio variables the same way.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Types of Data</span>"
    ]
  },
  {
    "objectID": "05-data_types.html#qualitative-categorical-variables",
    "href": "05-data_types.html#qualitative-categorical-variables",
    "title": "\n5  Types of Data\n",
    "section": "\n5.6 Qualitative (Categorical) Variables",
    "text": "5.6 Qualitative (Categorical) Variables\nQualitative or categorical variables consist of names of categories. These names may be numerical, but the numbers (or names) are simply codes to identify the groups or categories into which the individuals are divided. Categorical variables with two categories, like yes or no, up or down, or, more generally, 1 and 0, are called binary variables. Those with more than two-categories are sometimes called multi-categorical variables.\nIn the nh_500cc data, we have eight categorical variables, four binary and four with multiple categories.\n\nnh_500cc |&gt;\n  select(Sex, PhysActive, Smoke100, SleepTrouble,\n         Race, Education, MaritalStatus, HealthGen) |&gt;\n  summary()\n\n     Sex      PhysActive Smoke100  SleepTrouble       Race    \n female:236   No :216    No :291   No :380      Asian   : 51  \n male  :264   Yes:284    Yes:209   Yes:120      Black   : 81  \n                                                Hispanic: 37  \n                                                Mexican : 48  \n                                                White   :262  \n                                                Other   : 21  \n          Education        MaritalStatus     HealthGen  \n 8th Grade     : 26   Divorced    : 47   Excellent: 52  \n 9 - 11th Grade: 59   LivePartner : 46   Vgood    :167  \n High School   : 89   Married     :256   Good     :204  \n Some College  :153   NeverMarried:125   Fair     : 65  \n College Grad  :173   Separated   : 17   Poor     : 12  \n                      Widowed     :  9                  \n\n\n\n5.6.1 Nominal vs. Ordinal Categories\n\nWhen the categories included in a variable are merely names, and come in no particular order, we sometimes call them nominal variables. The most important summary of such a variable is usually a table of frequencies, and the mode becomes an important single summary, while the mean and median are essentially useless.\n\nIn the nh_500cc data, Race is a nominal variable with multiple unordered categories. So is MaritalStatus.\n\nThe alternative categorical variable (where order matters) is called ordinal, and includes variables that are sometimes thought of as falling right in between quantitative and qualitative variables.\n\nExamples of ordinal multi-categorical variables in the nh_500cc data include the Education and HealthGen variables.\n\nAnswers to questions like “How is your overall physical health?” with available responses Excellent, Very Good, Good, Fair or Poor, which are often coded as 1-5, certainly provide a perceived order, but a group of people with average health status 4 (Very Good) is not necessarily twice as healthy as a group with average health status of 2 (Fair).\nSometimes we treat the values from ordinal variables as sufficiently scaled to permit us to use quantitative approaches like means, quantiles, and standard deviations to summarize and model the results, and at other times, we’ll treat ordinal variables as if they were nominal, with tables and percentages our primary tools.\nNote that all binary variables may be treated as either ordinal, or nominal.\n\nBinary variables in the nh_500cc data include Sex, PhysActive, Smoke100, SleepTrouble. Each can be thought of as either ordinal or nominal.\nLots of variables may be treated as either quantitative or qualitative, depending on how we use them. For instance, we usually think of age as a quantitative variable, but if we simply use age to make the distinction between “child” and “adult” then we are using it to describe categorical information. Just because your variable’s values are numbers, don’t assume that the information provided is quantitative.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Types of Data</span>"
    ]
  },
  {
    "objectID": "05-data_types.html#tabulating-binary-variables",
    "href": "05-data_types.html#tabulating-binary-variables",
    "title": "\n5  Types of Data\n",
    "section": "\n5.7 Tabulating Binary Variables",
    "text": "5.7 Tabulating Binary Variables\nNote how the tbl_summary() approach works with binary variables, and in particular with variables coded Yes and No, like PhysActive, Smoke100 and SleepTrouble.\n\nnh_500cc |&gt;\n  select(Sex, PhysActive, Smoke100, SleepTrouble) |&gt;\n  tbl_summary()\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nN = 5001\n\n\n\n\nSex\n\n\n\n    female\n236 (47%)\n\n\n    male\n264 (53%)\n\n\nPhysActive\n284 (57%)\n\n\nSmoke100\n209 (42%)\n\n\nSleepTrouble\n120 (24%)\n\n\n\n\n1 n (%)\n\n\n\n\n\nWe can also summarize any particular variable with the tabyl() function from the janitor package.\n\nnh_500cc |&gt;\n  tabyl(Sex)\n\n    Sex   n percent\n female 236   0.472\n   male 264   0.528\n\n\nOr, we can make a basic cross-tabulation of two binary variables, like this:\n\nnh_500cc |&gt;\n  tabyl(PhysActive, Smoke100) |&gt;\n  adorn_title()\n\n            Smoke100    \n PhysActive       No Yes\n         No      111 105\n        Yes      180 104",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Types of Data</span>"
    ]
  },
  {
    "objectID": "05-data_types.html#tabulating-multi-categorical-variables",
    "href": "05-data_types.html#tabulating-multi-categorical-variables",
    "title": "\n5  Types of Data\n",
    "section": "\n5.8 Tabulating Multi-Categorical Variables",
    "text": "5.8 Tabulating Multi-Categorical Variables\n\nnh_500cc |&gt;\n  select(Race, Education, MaritalStatus, HealthGen) |&gt;\n  tbl_summary()\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nN = 5001\n\n\n\n\nRace\n\n\n\n    Asian\n51 (10%)\n\n\n    Black\n81 (16%)\n\n\n    Hispanic\n37 (7.4%)\n\n\n    Mexican\n48 (9.6%)\n\n\n    White\n262 (52%)\n\n\n    Other\n21 (4.2%)\n\n\nEducation\n\n\n\n    8th Grade\n26 (5.2%)\n\n\n    9 - 11th Grade\n59 (12%)\n\n\n    High School\n89 (18%)\n\n\n    Some College\n153 (31%)\n\n\n    College Grad\n173 (35%)\n\n\nMaritalStatus\n\n\n\n    Divorced\n47 (9.4%)\n\n\n    LivePartner\n46 (9.2%)\n\n\n    Married\n256 (51%)\n\n\n    NeverMarried\n125 (25%)\n\n\n    Separated\n17 (3.4%)\n\n\n    Widowed\n9 (1.8%)\n\n\nHealthGen\n\n\n\n    Excellent\n52 (10%)\n\n\n    Vgood\n167 (33%)\n\n\n    Good\n204 (41%)\n\n\n    Fair\n65 (13%)\n\n\n    Poor\n12 (2.4%)\n\n\n\n\n1 n (%)\n\n\n\n\n\nWe can also use tabyl() to look at combinations of multi-categorical variables, whether they are ordinal or nominal.\n\nnh_500cc |&gt;\n  tabyl(Education, HealthGen) |&gt;\n  adorn_totals(where = c(\"row\", \"col\")) |&gt;\n  adorn_title() |&gt;\n  kbl(align = 'lrrrrrc') |&gt;\n  kable_styling(full_width = FALSE)\n\n\n\n\nHealthGen\n\n\n\n\n\n\n\n\nEducation\nExcellent\nVgood\nGood\nFair\nPoor\nTotal\n\n\n8th Grade\n2\n3\n9\n9\n3\n26\n\n\n9 - 11th Grade\n4\n12\n28\n12\n3\n59\n\n\nHigh School\n9\n26\n36\n18\n0\n89\n\n\nSome College\n11\n46\n75\n17\n4\n153\n\n\nCollege Grad\n26\n80\n56\n9\n2\n173\n\n\nTotal\n52\n167\n204\n65\n12\n500",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Types of Data</span>"
    ]
  },
  {
    "objectID": "05-data_types.html#coming-up",
    "href": "05-data_types.html#coming-up",
    "title": "\n5  Types of Data\n",
    "section": "\n5.9 Coming Up",
    "text": "5.9 Coming Up\nNext, we’ll look at several additional approaches to building tabular and graphical summaries for these data, beyond the ideas provided here.\n\n\n\n\nBock, David E., Paul F. Velleman, and Richard D. De Veaux. 2004. Stats: Modelling the World. Boston MA: Pearson Addison-Wesley.\n\n\nNorman, Geoffrey R., and David L. Streiner. 2014. Biostatistics: The Bare Essentials. Fourth. People’s Medical Publishing House.\n\n\nVittinghoff, Eric, David V. Glidden, Stephen C. Shiboski, and Charles E. McCulloch. 2012. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. Second. Springer-Verlag, Inc. http://www.biostat.ucsf.edu/vgsm/.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Types of Data</span>"
    ]
  },
  {
    "objectID": "06-nhanes_cc.html",
    "href": "06-nhanes_cc.html",
    "title": "\n6  More NHANES Summaries\n",
    "section": "",
    "text": "6.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor)\nlibrary(kableExtra)\nlibrary(patchwork)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More NHANES Summaries</span>"
    ]
  },
  {
    "objectID": "06-nhanes_cc.html#re-loading-the-complete-cases-sample",
    "href": "06-nhanes_cc.html#re-loading-the-complete-cases-sample",
    "title": "\n6  More NHANES Summaries\n",
    "section": "\n6.2 Re-Loading the “Complete Cases” Sample",
    "text": "6.2 Re-Loading the “Complete Cases” Sample\nIn this chapter, we’ll build on the summaries we’ve illustrated previously. Let’s begin by again loading into the nh_500cc tibble the information from the nh_adult500cc.Rds file we created in Section 4.5.\n\nnh_500cc &lt;- read_rds(\"data/nh_adult500cc.Rds\")",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More NHANES Summaries</span>"
    ]
  },
  {
    "objectID": "06-nhanes_cc.html#distribution-of-heights",
    "href": "06-nhanes_cc.html#distribution-of-heights",
    "title": "\n6  More NHANES Summaries\n",
    "section": "\n6.3 Distribution of Heights",
    "text": "6.3 Distribution of Heights\nWhat is the distribution of height in this new sample?\n\nggplot(data = nh_500cc, aes(x = Height)) + \n    geom_histogram() \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nWe can do several things to clean this up.\n\nWe’ll change the color of the lines for each bar of the histogram.\nWe’ll change the fill inside each bar to make them stand out a bit more.\nWe’ll add a title and relabel the horizontal (x) axis to include the units of measurement.\nWe’ll avoid the warning by selecting a number of bins (we’ll use 25 here) into which we’ll group the heights before drawing the histogram.\n\n\nggplot(data = nh_500cc, aes(x = Height)) + \n    geom_histogram(bins = 25, col = \"yellow\", fill = \"blue\") + \n    labs(title = \"Height of 500 NHANES Adults\",\n         x = \"Height in cm.\")\n\n\n\n\n\n\n\n\n6.3.1 Changing a Histogram’s Fill and Color\nThe CWRU color guide (https://case.edu/umc/our-brand/visual-guidelines/) lists the HTML color schemes for CWRU blue and CWRU gray. Let’s match that color scheme. We will also change the bins for the histogram, to gather observations into groups of 2 cm. each, by specifying the width of the bins, rather than the number of bins.\n\ncwru.blue &lt;- '#0a304e'\ncwru.gray &lt;- '#626262'\n\nggplot(data = nh_500cc, aes(x = Height)) + \n    geom_histogram(binwidth = 2, \n                   col = cwru.gray, fill = cwru.blue) + \n    labs(title = \"Height of 500 NHANES Adults\",\n         x = \"Height in cm.\") \n\n\n\n\n\n\n\n\n6.3.2 Using a frequency polygon\nA frequency polygon essentially smooths out the top of the histogram, and can also be used to show the distribution of Height.\n\nggplot(data = nh_500cc, aes(x = Height)) +\n    geom_freqpoly(bins = 20) +\n    labs(title = \"Height of 500 NHANES Adults\",\n         x = \"Height in cm.\")\n\n\n\n\n\n\n\n\n6.3.3 Using a dotplot\nA dotplot can also be used to show the distribution of a variable like Height, and produces a somewhat more granular histogram, depending on the settings for binwidth and dotsize.\n\nggplot(data = nh_500cc, aes(x = Height)) +\n    geom_dotplot(dotsize = 0.75, binwidth = 1) +\n    scale_y_continuous(NULL, breaks = NULL) + # hide y axis\n    labs(title = \"Height of 500 NHANES Adults\",\n         x = \"Height in cm.\")",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More NHANES Summaries</span>"
    ]
  },
  {
    "objectID": "06-nhanes_cc.html#height-and-sex",
    "href": "06-nhanes_cc.html#height-and-sex",
    "title": "\n6  More NHANES Summaries\n",
    "section": "\n6.4 Height and Sex",
    "text": "6.4 Height and Sex\nLet’s look again at the impact of a respondent’s sex on their height, but now within our sample of adults.\n\nggplot(data = nh_500cc, \n       aes(x = Sex, y = Height, color = Sex)) + \n    geom_point() + \n    labs(title = \"Height by Sex for 500 NHANES Adults\",\n         y = \"Height in cm.\")\n\n\n\n\n\n\n\nThis plot isn’t so useful. We can improve things a little by jittering the points horizontally, so that the overlap is reduced.\n\nggplot(data = nh_500cc, aes(x = Sex, y = Height, color = Sex)) + \n    geom_jitter(width = 0.2) + \n    labs(title = \"Height by Sex (jittered) for 500 NHANES Adults\",\n         y = \"Height in cm.\")\n\n\n\n\n\n\n\nPerhaps it might be better to summarize the distribution in a different way. We might consider a boxplot of the data.\n\n6.4.1 A Boxplot of Height by Sex\n\nggplot(data = nh_500cc, aes(x = Sex, y = Height, fill = Sex)) + \n    geom_boxplot() + \n    labs(title = \"Boxplot of Height by Sex for 500 NHANES Adults\",\n         y = \"Height in cm.\")\n\n\n\n\n\n\n\nThe boxplot shows some summary statistics based on percentiles. The boxes in the middle of the line show the data values that include the middle half of the data once its been sorted. The 25th percentile (value that exceeds 1/4 of the data) is indicated by the bottom of the box, while the top of the box is located at the 75th percentile. The solid line inside the box indicates the median (also called the 50th percentile) of the Heights for that Sex.\n\n6.4.2 Adding a violin plot\nA boxplot is often supplemented with a violin plot to better show the shape of the distribution.\n\nggplot(data = nh_500cc, aes(x = Sex, y = Height, fill = Sex)) +\n    geom_violin() +\n    geom_boxplot(width = 0.3) +\n    labs(title = \"Boxplot of Height by Sex for 500 NHANES Adults\",\n         y = \"Height in cm.\")\n\n\n\n\n\n\n\nThis usually works better if the boxes are given a different fill than the violins, as shown in the following figure.\n\nggplot(data = nh_500cc, aes(x = Sex, y = Height)) +\n    geom_violin(aes(fill = Sex)) +\n    geom_boxplot(width = 0.3) +\n    labs(title = \"Boxplot of Height by Sex for 500 NHANES Adults\",\n         y = \"Height in cm.\")\n\n\n\n\n\n\n\nWe can also flip the boxplots on their side, using coord_flip().\n\nggplot(data = nh_500cc, aes(x = Sex, y = Height)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = Sex), width = 0.3) +\n    labs(title = \"Boxplot of Height by Sex for 500 NHANES Adults\",\n         y = \"Height in cm.\") +\n    coord_flip()\n\n\n\n\n\n\n\n\n6.4.3 Histograms of Height by Sex\nOr perhaps we’d like to see a pair of faceted histograms?\n\nggplot(data = nh_500cc, aes(x = Height, fill = Sex)) + \n    geom_histogram(color = \"white\", bins = 20) + \n    labs(title = \"Histogram of Height by Sex for 500 NHANES Adults\",\n         x = \"Height in cm.\") + \n    facet_wrap(~ Sex)\n\n\n\n\n\n\n\nCan we redraw these histograms so that they are a little more comparable, and to get rid of the unnecessary legend?\n\nggplot(data = nh_500cc, aes(x = Height, fill = Sex)) + \n    geom_histogram(color = \"white\", bins = 20) + \n    labs(title = \"Histogram of Height by Sex for 500 NHANES Adults\",\n         x = \"Height in cm.\") + \n    guides(fill = \"none\") +\n    facet_grid(Sex ~ .)",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More NHANES Summaries</span>"
    ]
  },
  {
    "objectID": "06-nhanes_cc.html#visualizing-age-and-heights-relationship-by-sex",
    "href": "06-nhanes_cc.html#visualizing-age-and-heights-relationship-by-sex",
    "title": "\n6  More NHANES Summaries\n",
    "section": "\n6.5 Visualizing Age and Height’s Relationship, by Sex",
    "text": "6.5 Visualizing Age and Height’s Relationship, by Sex\nWe can start with a simple scatterplot of the Height (y-axis) and Age (x-axis) relationship across the subjects in our nh_500cc tibble.\n\nggplot(data = nh_500cc, aes(x = Age, y = Height)) +\n  geom_point() +\n  labs(title = \"Height-Age Relationship in 500 NHANES adults\")\n\n\n\n\n\n\n\nIs there a meaningful difference in what this relationship looks like, depending on Sex?\n\n6.5.1 Adding Color to the plot\nLet’s add Sex to the plot using color, and also adjust the y axis label to incorporate the units of measurement.\n\nggplot(data = nh_500cc, aes(x = Age, y = Height, color = Sex)) +\n    geom_point() +\n    labs(title = \"Height-Age Relationship in 500 NHANES adults, by Sex\", \n         y = \"Height in cm.\")\n\n\n\n\n\n\n\n\n6.5.2 Can we show the Female and Male relationships in separate panels?\nSure. We can facet the scatterplot into a panel for each Sex, as follows.\n\nggplot(data = nh_500cc, aes(x = Age, y = Height, color = Sex)) +\n    geom_point() + \n    labs(title = \"Height-Age Relationship in 500 NHANES adults\", \n         y = \"Height in cm.\") +\n    facet_wrap(~ Sex)\n\n\n\n\n\n\n\n\n6.5.3 Can we add a smooth curve to show the relationship in each plot?\nYes, by adding a call to the geom_smooth() function. Is there any indication of a strong relationship between Age and Height in this sample?\n\nggplot(data = nh_500cc, aes(x = Age, y = Height, color = Sex)) +\n    geom_point() + \n    geom_smooth(method = \"loess\", formula = y ~ x) +\n    labs(title = \"Height-Age Relationship in NHANES sample\", \n         y = \"Height in cm.\") +\n    facet_wrap(~ Sex)\n\n\n\n\n\n\n\n\n6.5.4 What if we want to assume straight line relationships?\nWe could look at a linear model in each part of the plot instead, and this time, we’ll also get rid of the redundant legend, using the guides() command. Does assuming a straight line make much of a difference here?\n\nggplot(data = nh_500cc, aes(x = Age, y = Height, color = Sex)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", formula = y ~ x) +\n    guides(color = \"none\") +\n    labs(title = \"Height-Age Relationship in NHANES sample\", \n         y = \"Height in cm.\") +\n    facet_wrap(~ Sex)",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More NHANES Summaries</span>"
    ]
  },
  {
    "objectID": "06-nhanes_cc.html#combining-plots-with-patchwork",
    "href": "06-nhanes_cc.html#combining-plots-with-patchwork",
    "title": "\n6  More NHANES Summaries\n",
    "section": "\n6.6 Combining Plots with patchwork\n",
    "text": "6.6 Combining Plots with patchwork\n\nThe patchwork package in R allows us to use some simple commands to put two plots together.\nSuppose we create two separate plots, which we’ll name p1 and p2, as follows.\n\np1 &lt;- ggplot(data = nh_500cc, aes(x = Age, y = Height)) +\n    geom_point() + \n    labs(title = \"Height and Age\")\n\np2 &lt;- ggplot(data = nh_500cc, aes(x = Sex, y = Height)) +\n    geom_boxplot() +\n    labs(title = \"Height, by Sex\")\n\nNow, suppose we want to put them together in a single figure. Thanks to patchwork, we can simply type in the following.\n\np1 / p2\n\n\n\n\n\n\n\nor we can place the images next to each other, and add an annotation, like this:\n\np1 + p2 +\n    plot_annotation(title = \"Our Combined Plots\")\n\n\n\n\n\n\n\nThe patchwork package website provides lots of great examples and guides to make it very easy to combine separate ggplots into the same graphic. While there are other packages (gridExtra and cowplot are very nice, for instance) to do this task, I think patchwork is the most user-friendly, so that’s the focus of these notes.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More NHANES Summaries</span>"
    ]
  },
  {
    "objectID": "06-nhanes_cc.html#looking-at-pulse-rate",
    "href": "06-nhanes_cc.html#looking-at-pulse-rate",
    "title": "\n6  More NHANES Summaries\n",
    "section": "\n6.7 Looking at Pulse Rate",
    "text": "6.7 Looking at Pulse Rate\nLet’s look at a different outcome, the pulse rate for our subjects.\nHere’s a histogram, again with CWRU colors, for the pulse rates in our sample.\n\nggplot(data = nh_500cc, aes(x = Pulse)) + \n    geom_histogram(binwidth = 1, \n                   fill = cwru.blue, col = cwru.gray) + \n    labs(title = \"Histogram of Pulse Rate: NHANES Adults\",\n         x = \"Pulse Rate (beats per minute)\")\n\n\n\n\n\n\n\nSuppose we instead bin up groups of 5 beats per minute together as we plot the Pulse rates.\n\nggplot(data = nh_500cc, aes(x = Pulse)) + \n    geom_histogram(binwidth = 5, \n                   fill = cwru.blue, col = cwru.gray) + \n    labs(title = \"Histogram of Pulse Rate: NHANES Adults\",\n         x = \"Pulse Rate (beats per minute)\")\n\n\n\n\n\n\n\nWhich is the more useful representation will depend a lot on what questions you’re trying to answer.\n\n6.7.1 Pulse Rate and Physical Activity\nWe can also split up our data into groups based on whether the subjects are physically active. Let’s try a boxplot.\n\nggplot(data = nh_500cc, \n       aes(y = Pulse, x = PhysActive, fill = PhysActive)) + \n    geom_boxplot() + \n    guides(fill = \"none\") +\n    labs(title = \"Pulse Rate by Physical Activity Status in NHANES Adults\")\n\n\n\n\n\n\n\nAs an accompanying numerical summary, we might ask how many people fall into each of these PhysActive categories, and what is their “average” Pulse rate.\n\nnh_500cc |&gt;\n    group_by(PhysActive) |&gt;\n    summarise(count = n(), mean(Pulse), median(Pulse)) |&gt;\n    kbl(digits = 2) |&gt;\n    kable_styling(full_width = FALSE)\n\n\n\nPhysActive\ncount\nmean(Pulse)\nmedian(Pulse)\n\n\n\nNo\n216\n74.44\n74\n\n\nYes\n284\n73.96\n74\n\n\n\n\n\nThe end of this chunk of code tells Quarto to generate a table with some attractive formatting, and rounding any decimals to two figures.\n\n6.7.2 Pulse by Sleeping Trouble\n\nggplot(data = nh_500cc, aes(x = Pulse, fill = SleepTrouble)) + \n    geom_histogram(color = \"white\", bins = 20) + \n    labs(title = \"Histogram of Pulse Rate by Sleep Trouble for NHANES Adults\",\n         x = \"Pulse Rate (beats per minute)\") + \n    guides(fill = \"none\") +\n    facet_grid(SleepTrouble ~ ., labeller = \"label_both\")\n\n\n\n\n\n\n\nHow many people fall into each of these SleepTrouble categories, and what is their “average” Pulse rate?\n\nnh_500cc |&gt;\n    group_by(SleepTrouble) |&gt;\n    summarise(count = n(), mean(Pulse), median(Pulse)) |&gt;\n    kbl(digits = 2) |&gt;\n    kable_styling(full_width = F)\n\n\n\nSleepTrouble\ncount\nmean(Pulse)\nmedian(Pulse)\n\n\n\nNo\n380\n73.45\n73\n\n\nYes\n120\n76.43\n76\n\n\n\n\n\n\n6.7.3 Pulse and HealthGen\nWe can compare the distribution of Pulse rate across groups by the subject’s self-reported overall health (HealthGen), as well.\n\nggplot(data = nh_500cc, aes(x = HealthGen, y = Pulse, fill = HealthGen)) + \n    geom_boxplot() +\n    labs(title = \"Pulse by Self-Reported Overall Health for NHANES Adults\",\n         x = \"Self-Reported Overall Health\", y = \"Pulse Rate\") + \n    guides(fill = \"none\") \n\n\n\n\n\n\n\nHow many people fall into each of these HealthGen categories, and what is their “average” Pulse rate?\n\nnh_500cc |&gt;\n    group_by(HealthGen) |&gt;\n    summarise(count = n(), mean(Pulse), median(Pulse)) |&gt;\n    kbl(digits = 2) |&gt;\n    kable_styling(full_width = F)\n\n\n\nHealthGen\ncount\nmean(Pulse)\nmedian(Pulse)\n\n\n\nExcellent\n52\n72.08\n72\n\n\nVgood\n167\n71.78\n72\n\n\nGood\n204\n75.22\n74\n\n\nFair\n65\n76.55\n76\n\n\nPoor\n12\n85.50\n82\n\n\n\n\n\n\n6.7.4 Pulse Rate and Systolic Blood Pressure\n\nggplot(data = nh_500cc, aes(x = SBP, y = Pulse)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x) +\n    labs(title = \"Pulse Rate vs. SBP for NHANES Adults\")\n\n\n\n\n\n\n\n\n6.7.5 Sleep Trouble vs. No Sleep Trouble?\nCould we see whether subjects who have described SleepTrouble show different SBP-pulse rate patterns than the subjects who haven’t?\n\nLet’s try doing this by changing the shape and the color of the points based on SleepTrouble.\n\n\nggplot(data = nh_500cc, \n       aes(x = SBP, y = Pulse, \n           color = SleepTrouble, shape = SleepTrouble)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x) +\n    labs(title = \"Pulse Rate vs. SBP for NHANES Adults\")\n\n\n\n\n\n\n\nThis plot might be easier to interpret if we faceted by SleepTrouble, as well.\n\nggplot(data = nh_500cc, \n       aes(x = SBP, y = Pulse, \n           color = SleepTrouble, shape = SleepTrouble)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x) +\n    labs(title = \"Pulse Rate vs. SBP for NHANES Adults\") +\n    facet_wrap(~ SleepTrouble, labeller = \"label_both\")",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More NHANES Summaries</span>"
    ]
  },
  {
    "objectID": "06-nhanes_cc.html#general-health-status",
    "href": "06-nhanes_cc.html#general-health-status",
    "title": "\n6  More NHANES Summaries\n",
    "section": "\n6.8 General Health Status",
    "text": "6.8 General Health Status\nHere’s a Table of the General Health Status results. Again, this is a self-reported rating of each subject’s health on a five point scale (Excellent, Very Good, Good, Fair, Poor.)\n\nnh_500cc |&gt;\n    tabyl(HealthGen) \n\n HealthGen   n percent\n Excellent  52   0.104\n     Vgood 167   0.334\n      Good 204   0.408\n      Fair  65   0.130\n      Poor  12   0.024\n\n\nThe HealthGen data are categorical, which means that summarizing them with averages isn’t as appealing as looking at percentages, proportions and rates. The tabyl function comes from the janitor package in R.\n\nI don’t actually like the title of percent here, as it’s really a proportion, but that can be adjusted, and we can add a total.\n\n\nnh_500cc |&gt;\n    tabyl(HealthGen) |&gt;\n    adorn_totals() |&gt;\n    adorn_pct_formatting()\n\n HealthGen   n percent\n Excellent  52   10.4%\n     Vgood 167   33.4%\n      Good 204   40.8%\n      Fair  65   13.0%\n      Poor  12    2.4%\n     Total 500  100.0%\n\n\nWhen working with an unordered categorical variable, like MaritalStatus, the same approach can work.\n\nnh_500cc |&gt;\n    tabyl(MaritalStatus) |&gt;\n    adorn_totals() |&gt;\n    adorn_pct_formatting()\n\n MaritalStatus   n percent\n      Divorced  47    9.4%\n   LivePartner  46    9.2%\n       Married 256   51.2%\n  NeverMarried 125   25.0%\n     Separated  17    3.4%\n       Widowed   9    1.8%\n         Total 500  100.0%\n\n\n\n6.8.1 Bar Chart for Categorical Data\nUsually, a bar chart is the best choice for graphing a variable made up of categories.\n\nggplot(data = nh_500cc, aes(x = HealthGen)) + \n    geom_bar()\n\n\n\n\n\n\n\nThere are lots of things we can do to make this plot fancier.\n\nggplot(data = nh_500cc, aes(x = HealthGen, fill = HealthGen)) + \n    geom_bar() + \n    guides(fill = \"none\") +\n    labs(x = \"Self-Reported Health Status\",\n         y = \"Number of NHANES subjects\",\n         title = \"Self-Reported Health Status in NHANES Adults\")\n\n\n\n\n\n\n\nOr, we can really go crazy…\n\nnh_500cc |&gt;\n    count(HealthGen) |&gt;\n    mutate(pct = round_half_up(prop.table(n) * 100, 1)) |&gt;\n    ggplot(aes(x = HealthGen, y = pct, fill = HealthGen)) + \n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    geom_text(aes(y = pct + 1,    # nudge above top of bar\n                  label = paste0(pct, '%')),  # prettify\n              position = position_dodge(width = .9), \n              size = 4) +\n    labs(x = \"Self-Reported Health Status\",\n         y = \"Percentage of Subjects\",\n         title = \"Self-Reported Health Status in NHANES Adults\")",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More NHANES Summaries</span>"
    ]
  },
  {
    "objectID": "06-nhanes_cc.html#two-way-tables",
    "href": "06-nhanes_cc.html#two-way-tables",
    "title": "\n6  More NHANES Summaries\n",
    "section": "\n6.9 Two-Way Tables",
    "text": "6.9 Two-Way Tables\nWe can create cross-classifications of two categorical variables (for example HealthGen and Smoke100), adding both row and column marginal totals, and compare subjects by Sex, as follows…\n\nnh_500cc |&gt;\n    tabyl(Smoke100, HealthGen) |&gt;\n    adorn_totals(c(\"row\", \"col\")) |&gt;\n    adorn_title()\n\n          HealthGen                           \n Smoke100 Excellent Vgood Good Fair Poor Total\n       No        44   108  105   29    5   291\n      Yes         8    59   99   36    7   209\n    Total        52   167  204   65   12   500\n\n\nIf we like, we can make this look a little more polished with the kbl() and kable_styling() functions from the kableExtra package.\n\nnh_500cc |&gt;\n    tabyl(Smoke100, HealthGen) |&gt;\n    adorn_totals(c(\"row\", \"col\")) |&gt;\n    adorn_title() |&gt;\n    kbl(align = 'crrrrr') |&gt;\n    kable_styling(position = \"center\", full_width = FALSE)\n\n\n\n\nHealthGen\n\n\n\n\n\n\n\n\nSmoke100\nExcellent\nVgood\nGood\nFair\nPoor\nTotal\n\n\nNo\n44\n108\n105\n29\n5\n291\n\n\nYes\n8\n59\n99\n36\n7\n209\n\n\nTotal\n52\n167\n204\n65\n12\n500\n\n\n\n\n\nOr, we can get a complete cross-tabulation, including (in this case) the percentages of people within each of the two categories of Smoke100 that fall in each HealthGen category (percentages within each row) like this.\n\nnh_500cc |&gt;\n    tabyl(Smoke100, HealthGen) |&gt;\n    adorn_totals(\"row\") |&gt;\n    adorn_percentages(\"row\") |&gt;\n    adorn_pct_formatting() |&gt;\n    adorn_ns() |&gt;\n    adorn_title() |&gt;\n    kbl(align = 'crrrrr') |&gt;\n    kable_styling(position = \"center\", full_width = FALSE)\n\n\n\n\nHealthGen\n\n\n\n\n\n\n\nSmoke100\nExcellent\nVgood\nGood\nFair\nPoor\n\n\nNo\n15.1% (44)\n37.1% (108)\n36.1% (105)\n10.0% (29)\n1.7% (5)\n\n\nYes\n3.8% (8)\n28.2% (59)\n47.4% (99)\n17.2% (36)\n3.3% (7)\n\n\nTotal\n10.4% (52)\n33.4% (167)\n40.8% (204)\n13.0% (65)\n2.4% (12)\n\n\n\n\n\nAnd, if we wanted the column percentages, to determine which sex had the higher rate of each HealthGen status level, we can get that by changing the adorn_percentages to describe results at the column level:\n\nnh_500cc |&gt;\n    tabyl(Sex, HealthGen) |&gt;\n    adorn_totals(\"col\") |&gt;\n    adorn_percentages(\"col\") |&gt;\n    adorn_pct_formatting() |&gt;\n    adorn_ns() |&gt;\n    adorn_title() |&gt;\n    kbl(align = 'crrrrr') |&gt;\n    kable_styling(position = \"center\", full_width = FALSE)\n\n\n\n\nHealthGen\n\n\n\n\n\n\n\n\nSex\nExcellent\nVgood\nGood\nFair\nPoor\nTotal\n\n\nfemale\n63.5% (33)\n44.3% (74)\n43.6% (89)\n47.7% (31)\n75.0% (9)\n47.2% (236)\n\n\nmale\n36.5% (19)\n55.7% (93)\n56.4% (115)\n52.3% (34)\n25.0% (3)\n52.8% (264)",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More NHANES Summaries</span>"
    ]
  },
  {
    "objectID": "06-nhanes_cc.html#sbp-by-general-health-status",
    "href": "06-nhanes_cc.html#sbp-by-general-health-status",
    "title": "\n6  More NHANES Summaries\n",
    "section": "\n6.10 SBP by General Health Status",
    "text": "6.10 SBP by General Health Status\nLet’s consider now the relationship between self-reported overall health and systolic blood pressure.\n\nggplot(data = nh_500cc, aes(x = HealthGen, y = SBP, \n                            fill = HealthGen)) + \n    geom_boxplot() + \n    labs(title = \"SBP by Health Status, Overall Health for NHANES Adults\",\n         y = \"Systolic Blood Pressure\", \n         x = \"Self-Reported Overall Health\") + \n    guides(fill = \"none\") \n\n\n\n\n\n\n\nWe can see that not too many people self-identify with the “Poor” health category.\n\nnh_500cc |&gt;\n    group_by(HealthGen) |&gt;\n    summarise(count = n(), mean(SBP), median(SBP)) |&gt;\n    kbl() |&gt;\n    kable_styling(position = \"center\", full_width = FALSE)\n\n\n\nHealthGen\ncount\nmean(SBP)\nmedian(SBP)\n\n\n\nExcellent\n52\n113.9231\n113\n\n\nVgood\n167\n117.5928\n118\n\n\nGood\n204\n121.5931\n120\n\n\nFair\n65\n120.3846\n118\n\n\nPoor\n12\n122.8333\n124\n\n\n\n\n\n\n6.10.1 SBP by Physical Activity and General Health Status\nWe’ll build a panel of boxplots to try to understand the relationships between Systolic Blood Pressure, General Health Status and Physical Activity. Note the use of coord_flip to rotate the graph 90 degrees, and the use of labeller within facet_wrap to include both the name of the (Physical Activity) variable and its value.\n\nggplot(data = nh_500cc, aes(x = HealthGen, y = SBP, fill = HealthGen)) + \n    geom_boxplot() + \n    labs(title = \"SBP by Health Status, Overall Health for NHANES Adults\",\n         y = \"Systolic BP\", x = \"Self-Reported Overall Health\") + \n    guides(fill = \"none\") +\n    facet_wrap(~ PhysActive, labeller = \"label_both\") + \n    coord_flip()\n\n\n\n\n\n\n\n\n6.10.2 SBP by Sleep Trouble and General Health Status\nHere’s a plot of faceted histograms, which might be used to address similar questions related to the relationship between Overall Health, Systolic Blood Pressure and whether someone has trouble sleeping.\n\nggplot(data = nh_500cc, aes(x = SBP, fill = HealthGen)) + \n    geom_histogram(color = \"white\", bins = 20) + \n    labs(title = \"SBP by Overall Health and Sleep Trouble for NHANES Adults\",\n         x = \"Systolic BP\") + \n    guides(fill = \"none\") +\n    facet_grid(SleepTrouble ~ HealthGen, labeller = \"label_both\")",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More NHANES Summaries</span>"
    ]
  },
  {
    "objectID": "06-nhanes_cc.html#conclusions",
    "href": "06-nhanes_cc.html#conclusions",
    "title": "\n6  More NHANES Summaries\n",
    "section": "\n6.11 Conclusions",
    "text": "6.11 Conclusions\nThis is just a small piece of the toolbox for visualizations that we’ll create in this class. Many additional tools are on the way, but the main idea won’t change. Using the ggplot2 package, we can accomplish several critical tasks in creating a visualization, including:\n\nIdentifying (and labeling) the axes and titles\nIdentifying a type of geom to use, like a point, bar or histogram\nChanging fill, color, shape, size to facilitate comparisons\nBuilding “small multiples” of plots with faceting\n\nGood data visualizations make it easy to see the data, and ggplot2’s tools make it relatively difficult to make a really bad graph.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More NHANES Summaries</span>"
    ]
  },
  {
    "objectID": "07-summarizing_quantities.html",
    "href": "07-summarizing_quantities.html",
    "title": "7  Summarizing Quantities",
    "section": "",
    "text": "7.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(gtsummary)\nlibrary(kableExtra)\nlibrary(patchwork)\nlibrary(summarytools)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\nThis chapter also requires that the Hmisc, mosaic, and psych packages are loaded on your machine, but these packages are not loaded with library() above.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summarizing Quantities</span>"
    ]
  },
  {
    "objectID": "07-summarizing_quantities.html#working-with-the-nh_750-data",
    "href": "07-summarizing_quantities.html#working-with-the-nh_750-data",
    "title": "7  Summarizing Quantities",
    "section": "\n7.2 Working with the nh_750 data",
    "text": "7.2 Working with the nh_750 data\nTo demonstrate key ideas in this Chapter, we will consider our sample of 750 adults ages 21-64 from NHANES 2011-12 which includes some missing values. We’ll load into the nh_750 data frame the information from the nh_adult750.Rds file we created in Section 4.3.2.\n\nnh_750 &lt;- read_rds(\"data/nh_adult750.Rds\")",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summarizing Quantities</span>"
    ]
  },
  {
    "objectID": "07-summarizing_quantities.html#the-summary-function-for-quantitative-data",
    "href": "07-summarizing_quantities.html#the-summary-function-for-quantitative-data",
    "title": "7  Summarizing Quantities",
    "section": "\n7.3 The summary function for Quantitative data",
    "text": "7.3 The summary function for Quantitative data\nR provides a small sampling of numerical summaries with the summary function, for instance.\n\nnh_750 |&gt;\n  select(Age, BMI, SBP, DBP, Pulse) |&gt;\n  summary()\n\n      Age             BMI             SBP             DBP        \n Min.   :21.00   Min.   :16.70   Min.   : 83.0   Min.   :  0.00  \n 1st Qu.:30.00   1st Qu.:24.20   1st Qu.:108.0   1st Qu.: 66.00  \n Median :40.00   Median :27.90   Median :118.0   Median : 73.00  \n Mean   :40.82   Mean   :29.08   Mean   :118.8   Mean   : 72.69  \n 3rd Qu.:51.00   3rd Qu.:32.10   3rd Qu.:127.0   3rd Qu.: 80.00  \n Max.   :64.00   Max.   :80.60   Max.   :209.0   Max.   :108.00  \n                 NA's   :5       NA's   :33      NA's   :33      \n     Pulse       \n Min.   : 40.00  \n 1st Qu.: 66.00  \n Median : 72.00  \n Mean   : 73.53  \n 3rd Qu.: 80.00  \n Max.   :124.00  \n NA's   :32      \n\n\nThis basic summary includes a set of five quantiles1, plus the sample’s mean.\n\n\nMin. = the minimum value for each variable, so, for example, the youngest subject’s Age was 21.\n\n1st Qu. = the first quartile (25th percentile) for each variable - for example, 25% of the subjects were Age 30 or younger.\n\nMedian = the median (50th percentile) - half of the subjects were Age 40 or younger.\n\nMean = the mean, usually what one means by an average - the sum of the Ages divided by 750 is 40.8,\n\n3rd Qu. = the third quartile (75th percentile) - 25% of the subjects were Age 51 or older.\n\nMax. = the maximum value for each variable, so the oldest subject was Age 64.\n\nThe summary also specifies the number of missing values for each variable. Here, we are missing 5 of the BMI values, for example.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summarizing Quantities</span>"
    ]
  },
  {
    "objectID": "07-summarizing_quantities.html#measuring-the-center-of-a-distribution",
    "href": "07-summarizing_quantities.html#measuring-the-center-of-a-distribution",
    "title": "7  Summarizing Quantities",
    "section": "\n7.4 Measuring the Center of a Distribution",
    "text": "7.4 Measuring the Center of a Distribution\n\n7.4.1 The Mean and The Median\nThe mean and median are the most commonly used measures of the center of a distribution for a quantitative variable. The median is the more generally useful value, as it is relevant even if the data have a shape that is not symmetric. We might also collect the sum of the observations, and the count of the number of observations, usually symbolized with n.\nFor variables without missing values, like Age, this is pretty straightforward.\n\nnh_750 |&gt;\n    summarise(n = n(), Mean = mean(Age), Median = median(Age), Sum = sum(Age))\n\n# A tibble: 1 × 4\n      n  Mean Median   Sum\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1   750  40.8     40 30616\n\n\nAnd again, the Mean is just the Sum (30616), divided by the number of non-missing values of Age (750), or 40.8213333.\nThe Median is the middle value when the data are sorted in order. When we have an odd number of values, this is sufficient. When we have an even number, as in this case, we take the mean of the two middle values. We could sort and list all 500 Ages, if we wanted to do so.\n\nnh_750 |&gt; select(Age) |&gt; \n    arrange(Age)\n\n# A tibble: 750 × 1\n     Age\n   &lt;int&gt;\n 1    21\n 2    21\n 3    21\n 4    21\n 5    21\n 6    21\n 7    21\n 8    21\n 9    21\n10    21\n# ℹ 740 more rows\n\n\nBut this data set figures we don’t want to output more than 10 observations to a table like this.\nIf we really want to see all of the data, we can use View(nh_750) to get a spreadsheet-style presentation, or use the sort command…\n\nsort(nh_750$Age)\n\n  [1] 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 22 22 22\n [26] 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 23 23 23 23 23 23\n [51] 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 24 24 24 24 24 24 24 24 24\n [76] 24 24 24 24 24 24 24 24 24 24 24 24 24 24 25 25 25 25 25 25 25 25 25 25 25\n[101] 25 25 25 26 26 26 26 26 26 26 26 26 26 26 26 26 26 27 27 27 27 27 27 27 27\n[126] 27 27 27 27 27 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28\n[151] 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 30 30 30 30 30\n[176] 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 31 31 31 31 31 31\n[201] 31 31 31 31 31 31 31 31 31 31 31 31 31 32 32 32 32 32 32 32 32 32 32 32 32\n[226] 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 32 33 33 33 33 33 33 33 33 33\n[251] 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 34 34 34 34 34 34 34 34 34 34\n[276] 34 34 34 34 35 35 35 35 35 35 35 35 35 35 35 36 36 36 36 36 36 36 36 36 36\n[301] 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 37 37 37 37 37 37 37 37 37\n[326] 37 37 37 37 37 37 37 37 37 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38\n[351] 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 40 40 40 40 40 40\n[376] 40 40 40 40 40 40 40 40 40 40 40 40 41 41 41 41 41 41 41 41 41 41 41 41 41\n[401] 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 43 43 43 43 43 43\n[426] 43 43 43 43 43 43 43 43 43 43 43 43 44 44 44 44 44 44 44 44 44 44 44 44 44\n[451] 44 44 44 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 46 46 46 46 46\n[476] 46 46 46 46 46 46 46 46 46 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47 47\n[501] 47 48 48 48 48 48 48 48 48 48 48 48 49 49 49 49 49 49 49 49 49 49 49 49 49\n[526] 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50\n[551] 50 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 52 52 52 52\n[576] 52 52 52 52 52 52 52 52 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 54 54\n[601] 54 54 54 54 54 54 54 54 54 54 54 54 54 54 55 55 55 55 55 55 55 55 55 55 56\n[626] 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 56 57 57 57 57 57 57 57\n[651] 57 57 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 58 59 59 59 59 59\n[676] 59 59 59 59 59 59 59 59 60 60 60 60 60 60 60 60 60 60 60 60 60 61 61 61 61\n[701] 61 61 61 61 61 61 61 61 61 61 62 62 62 62 62 62 62 62 62 62 62 62 62 63 63\n[726] 63 63 63 63 63 63 63 63 63 63 63 64 64 64 64 64 64 64 64 64 64 64 64 64 64\n\n\nAgain, to find the median, we would take the mean of the middle two observations in this sorted data set. That would be the 250th and 251st largest Ages.\n\nsort(nh_750$Age)[250:251]\n\n[1] 33 33\n\n\n\n7.4.2 Dealing with Missingness\nWhen calculating a mean, you may be tempted to try something like this…\n\nnh_750 |&gt;\n    summarise(mean(Pulse), median(Pulse))\n\n# A tibble: 1 × 2\n  `mean(Pulse)` `median(Pulse)`\n          &lt;dbl&gt;           &lt;int&gt;\n1            NA              NA\n\n\nThis fails because we have some missing values in the Pulse data. We can address this by either omitting the data with missing values before we run the summarise() function, or tell the mean and median summary functions to remove missing values2.\n\nnh_750 |&gt;\n    filter(complete.cases(Pulse)) |&gt;\n    summarise(count = n(), mean(Pulse), median(Pulse))\n\n# A tibble: 1 × 3\n  count `mean(Pulse)` `median(Pulse)`\n  &lt;int&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n1   718          73.5              72\n\n\nOr, we could tell the summary functions themselves to remove NA values.\n\nnh_750 |&gt;\n    summarise(mean(Pulse, na.rm=TRUE), median(Pulse, na.rm=TRUE))\n\n# A tibble: 1 × 2\n  `mean(Pulse, na.rm = TRUE)` `median(Pulse, na.rm = TRUE)`\n                        &lt;dbl&gt;                         &lt;dbl&gt;\n1                        73.5                            72\n\n\nIn Chapter 9, we will discuss various assumptions we can make about missing data, and the importance of imputation when dealing with it in modeling or making inferences. For now, we will limit our descriptive summaries to observed values, in what are called complete case or available case analyses.\n\n7.4.3 The Mode of a Quantitative Variable\nOne other less common measure of the center of a quantitative variable’s distribution is its most frequently observed value, referred to as the mode. This measure is only appropriate for discrete variables, be they quantitative or categorical. To find the mode, we usually tabulate the data, and then sort by the counts of the numbers of observations.\n\nnh_750 |&gt;\n    group_by(Age) |&gt;\n    summarise(count = n()) |&gt;\n    arrange(desc(count)) \n\n# A tibble: 44 × 2\n     Age count\n   &lt;int&gt; &lt;int&gt;\n 1    32    28\n 2    36    26\n 3    50    26\n 4    30    24\n 5    33    24\n 6    24    23\n 7    21    22\n 8    22    22\n 9    23    22\n10    28    20\n# ℹ 34 more rows\n\n\nThe mode is just the most common Age observed in the data.\nNote the use of three different “verbs” in our function there - for more explanation of this strategy, visit Hadley Wickham and Grolemund (2023). The group_by function here is very useful. It converts the nh_750 data frame into a new grouped tibble where operations are performed on the groups. Here, this means that it groups the data by Age before counting observations, and then sorting the groups (the Ages) by their frequencies.\nAs an alternative, the modeest package’s mlv function calculates the sample mode (or most frequent value) 3.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summarizing Quantities</span>"
    ]
  },
  {
    "objectID": "07-summarizing_quantities.html#measuring-the-spread-of-a-distribution",
    "href": "07-summarizing_quantities.html#measuring-the-spread-of-a-distribution",
    "title": "7  Summarizing Quantities",
    "section": "\n7.5 Measuring the Spread of a Distribution",
    "text": "7.5 Measuring the Spread of a Distribution\nStatistics is all about variation, so spread or dispersion is an important fundamental concept in statistics. Measures of spread like the inter-quartile range and range (maximum - minimum) can help us understand and compare data sets. If the values in the data are close to the center, the spread will be small. If many of the values in the data are scattered far away from the center, the spread will be large.\n\n7.5.1 The Range and the Interquartile Range (IQR)\nThe range of a quantitative variable is sometimes interpreted as the difference between the maximum and the minimum, even though R presents the actual minimum and maximum values when you ask for a range…\n\nnh_750 |&gt; \n    select(Age) |&gt; \n    range()\n\n[1] 21 64\n\n\nAnd, for a variable with missing values, we can use…\n\nnh_750 |&gt; \n    select(BMI) |&gt; \n    filter(complete.cases(BMI)) |&gt;\n    range()\n\n[1] 16.7 80.6\n\n\nA more interesting and useful statistic is the inter-quartile range, or IQR, which is the range of the middle half of the distribution, calculated by subtracting the 25th percentile value from the 75th percentile value.\n\nnh_750 |&gt;\n    summarise(IQR(Age), quantile(Age, 0.25), quantile(Age, 0.75))\n\n# A tibble: 1 × 3\n  `IQR(Age)` `quantile(Age, 0.25)` `quantile(Age, 0.75)`\n       &lt;dbl&gt;                 &lt;dbl&gt;                 &lt;dbl&gt;\n1         21                    30                    51\n\n\nWe can calculate the range and IQR nicely from the summary information on quantiles, of course:\n\nnh_750 |&gt;\n    select(Age, BMI, SBP, DBP, Pulse) |&gt;\n    summary()\n\n      Age             BMI             SBP             DBP        \n Min.   :21.00   Min.   :16.70   Min.   : 83.0   Min.   :  0.00  \n 1st Qu.:30.00   1st Qu.:24.20   1st Qu.:108.0   1st Qu.: 66.00  \n Median :40.00   Median :27.90   Median :118.0   Median : 73.00  \n Mean   :40.82   Mean   :29.08   Mean   :118.8   Mean   : 72.69  \n 3rd Qu.:51.00   3rd Qu.:32.10   3rd Qu.:127.0   3rd Qu.: 80.00  \n Max.   :64.00   Max.   :80.60   Max.   :209.0   Max.   :108.00  \n                 NA's   :5       NA's   :33      NA's   :33      \n     Pulse       \n Min.   : 40.00  \n 1st Qu.: 66.00  \n Median : 72.00  \n Mean   : 73.53  \n 3rd Qu.: 80.00  \n Max.   :124.00  \n NA's   :32      \n\n\n\n7.5.2 The Variance and the Standard Deviation\nThe IQR is always a reasonable summary of spread, just as the median is always a reasonable summary of the center of a distribution. Yet, most people are inclined to summarize a batch of data using two numbers: the mean and the standard deviation. This is really only a sensible thing to do if you are willing to assume the data follow a Normal distribution: a bell-shaped, symmetric distribution without substantial outliers.\nBut most data do not (even approximately) follow a Normal distribution. Summarizing by the median and quartiles (25th and 75th percentiles) is much more robust, explaining R’s emphasis on them.\n\n7.5.3 Obtaining the Variance and Standard Deviation in R\nHere are the variances of the quantitative variables in the nh_750 data. Note the need to include na.rm = TRUE to deal with the missing values in some variables.\n\nnh_750 |&gt;\n    select(Age, BMI, SBP, DBP, Pulse) |&gt;\n    summarise_all(var, na.rm = TRUE)\n\n# A tibble: 1 × 5\n    Age   BMI   SBP   DBP Pulse\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  157.  52.4  229.  128.  136.\n\n\nAnd here are the standard deviations of those same variables.\n\nnh_750 |&gt;\n    select(Age, BMI, SBP, DBP, Pulse) |&gt;\n    summarise_all(sd, na.rm = TRUE)\n\n# A tibble: 1 × 5\n    Age   BMI   SBP   DBP Pulse\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  12.5  7.24  15.1  11.3  11.6\n\n\n\n7.5.4 Defining the Variance and Standard Deviation\nBock, Velleman, and De Veaux (2004) have lots of useful thoughts here, which are lightly edited here.\nIn thinking about spread, we might consider how far each data value is from the mean. Such a difference is called a deviation. We could just average the deviations, but the positive and negative differences always cancel out, leaving an average deviation of zero, so that’s not helpful. Instead, we square each deviation to obtain non-negative values, and to emphasize larger differences. When we add up these squared deviations and find their mean (almost), this yields the variance.\n\\[\n\\mbox{Variance} = s^2 = \\frac{\\Sigma (y - \\bar{y})^2}{n-1}\n\\]\nWhy almost? It would be the mean of the squared deviations only if we divided the sum by \\(n\\), but instead we divide by \\(n-1\\) because doing so produces an estimate of the true (population) variance that is unbiased4. If you’re looking for a more intuitive explanation, this Stack Exchange link awaits your attention.\n\nTo return to the original units of measurement, we take the square root of \\(s^2\\), and instead work with \\(s\\), the standard deviation, also abbreviated SD.\n\n\\[\n\\mbox{Standard Deviation} = s = \\sqrt{\\frac{\\Sigma (y - \\bar{y})^2}{n-1}}\n\\]\n\n7.5.5 Interpreting the SD when the data are Normally distributed\nFor a set of measurements that follow a Normal distribution, the interval:\n\nMean \\(\\pm\\) Standard Deviation contains approximately 68% of the measurements;\nMean \\(\\pm\\) 2(Standard Deviation) contains approximately 95% of the measurements;\nMean \\(\\pm\\) 3(Standard Deviation) contains approximately all (99.7%) of the measurements.\n\nWe often refer to the population or process mean of a distribution with \\(\\mu\\) and the standard deviation with \\(\\sigma\\), leading to the Figure below.\n\n\n\n\nThe Normal Distribution and the Empirical Rule\n\n\n\nBut if the data are not from an approximately Normal distribution, then this Empirical Rule is less helpful.\n\n7.5.6 Chebyshev’s Inequality: One Interpretation of the Standard Deviation\nChebyshev’s Inequality tells us that for any distribution, regardless of its relationship to a Normal distribution, no more than 1/k2 of the distribution’s values can lie more than k standard deviations from the mean. This implies, for instance, that for any distribution, at least 75% of the values must lie within two standard deviations of the mean, and at least 89% must lie within three standard deviations of the mean.\nAgain, most data sets do not follow a Normal distribution. We’ll return to this notion soon. But first, let’s try to draw some pictures that let us get a better understanding of the distribution of our data.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summarizing Quantities</span>"
    ]
  },
  {
    "objectID": "07-summarizing_quantities.html#measuring-the-shape-of-a-distribution",
    "href": "07-summarizing_quantities.html#measuring-the-shape-of-a-distribution",
    "title": "7  Summarizing Quantities",
    "section": "\n7.6 Measuring the Shape of a Distribution",
    "text": "7.6 Measuring the Shape of a Distribution\nWhen considering the shape of a distribution, one is often interested in three key points.\n\nThe number of modes in the distribution, which I always assess through plotting the data.\nThe skewness, or symmetry that is present, which I typically assess by looking at a plot of the distribution of the data, but if required to, will summarize with a non-parametric measure of skewness, that we will discuss in Section 10.13.\nThe kurtosis, or heavy-tailedness (outlier-proneness) that is present, usually in comparison to a Normal distribution. Again, this is something I nearly inevitably assess graphically, but there are measures.\n\nA Normal distribution has a single mode, is symmetric and, naturally, is neither heavy-tailed nor light-tailed as compared to a Normal distribution (we call this mesokurtic).\n\n7.6.1 Multimodal vs. Unimodal distributions\nA unimodal distribution, on some level, is straightforward. It is a distribution with a single mode, or “peak” in the distribution. Such a distribution may be skewed or symmetric, light-tailed or heavy-tailed. We usually describe as multimodal distributions like the two on the right below, which have multiple local maxima, even though they have just a single global maximum peak.\n\n\n\n\nUnimodal and Multimodal Sketches\n\n\n\nTruly multimodal distributions are usually described that way in terms of shape. For unimodal distributions, skewness and kurtosis become useful ideas.\n\n7.6.2 Skew\nWhether or not a distribution is approximately symmetric is an important consideration in describing its shape. Graphical assessments are always most useful in this setting, particularly for unimodal data. My favorite measure of skew, or skewness if the data have a single mode, is:\n\\[\nskew_1 = \\frac{\\mbox{mean} - \\mbox{median}}{\\mbox{standard deviation}}\n\\]\n\nSymmetric distributions generally show values of \\(skew_1\\) near zero. If the distribution is actually symmetric, the mean should be equal to the median.\nDistributions with \\(skew_1\\) values above 0.2 in absolute value generally indicate meaningful skew.\nPositive skew (mean &gt; median if the data are unimodal) is also referred to as right skew.\nNegative skew (mean &lt; median if the data are unimodal) is referred to as left skew.\n\n\n\n\n\nNegative (Left) Skew and Positive (Right) Skew\n\n\n\n\n7.6.3 Kurtosis\nWhen we have a unimodal distribution that is symmetric, we will often be interested in the behavior of the tails of the distribution, as compared to a Normal distribution with the same mean and standard deviation. High values of kurtosis measures (and there are several) indicate data which has extreme outliers, or is heavy-tailed.\n\nA mesokurtic distribution has similar tail behavior to what we would expect from a Normal distribution.\nA leptokurtic distribution is a thinner, more slender distribution, with heavier tails than we’d expect from a Normal distribution. One example is the t distribution.\nA platykurtic distribution is a broader, flatter distribution, with thinner tails than we’d expect from a Normal distribution. One example is a uniform distribution.\n\nThe visualization below (shown after the code) displays these three types of distributions.\n\nset.seed(431)\nsims_kurt &lt;- tibble(meso = rnorm(n = 300, mean = 0, sd = 1),\n                    lepto = rt(n = 300, df = 4),\n                    platy = runif(n = 300, min = -2, max = 2))\n\np1 &lt;- ggplot(sims_kurt, aes(x = meso)) +\n  geom_histogram(aes(y = stat(density)), \n                 bins = 25, fill = \"royalblue\", col = \"white\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(sims_kurt$meso), \n                            sd = sd(sims_kurt$meso)),\n                col = \"red\") +\n  labs(title = \"Normal (mesokurtic)\")\n\np1a &lt;- ggplot(sims_kurt, aes(x = meso, y = \"\")) +\n  geom_violin() +\n  geom_boxplot(fill = \"royalblue\", outlier.color = \"royalblue\", width = 0.3) +\n  labs(y = \"\", x = \"Normal (mesokurtic)\")\n\np2 &lt;- ggplot(sims_kurt, aes(x = lepto)) +\n  geom_histogram(aes(y = stat(density)), \n                 bins = 25, fill = \"tomato\", col = \"white\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(sims_kurt$lepto), \n                            sd = sd(sims_kurt$lepto)),\n                col = \"royalblue\") +\n  labs(title = \"t (leptokurtic)\")\n\np2a &lt;- ggplot(sims_kurt, aes(x = lepto, y = \"\")) +\n  geom_violin() +\n  geom_boxplot(fill = \"tomato\", outlier.color = \"tomato\", width = 0.3) +\n  labs(y = \"\", x = \"t (slender with heavy tails)\")\n\np3 &lt;- ggplot(sims_kurt, aes(x = platy)) +\n  geom_histogram(aes(y = stat(density)), \n                 bins = 25, fill = \"yellow\", col = \"black\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(sims_kurt$platy), \n                            sd = sd(sims_kurt$platy)),\n                col = \"royalblue\", lwd = 1.5) +\n  xlim(-3, 3) +\n  labs(title = \"Uniform (platykurtic)\")\n\np3a &lt;- ggplot(sims_kurt, aes(x = platy, y = \"\")) +\n  geom_violin() +\n  geom_boxplot(fill = \"yellow\", width = 0.3) + \n  xlim(-3, 3) +\n  labs(y = \"\", x = \"Uniform (broad with thin tails)\")\n\n\n(p1 + p2 + p3) / (p1a + p2a + p3a) + \n  plot_layout(heights = c(3, 1))\n\n\n\n\n\n\n\nGraphical tools are in most cases the best way to identify issues related to kurtosis, and we usually only focus on kurtosis if we are willing to assume symmetry (or at least lack of meaningful skew) in our data.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summarizing Quantities</span>"
    ]
  },
  {
    "objectID": "07-summarizing_quantities.html#multiple-summaries-at-once",
    "href": "07-summarizing_quantities.html#multiple-summaries-at-once",
    "title": "7  Summarizing Quantities",
    "section": "\n7.7 Multiple Summaries at once",
    "text": "7.7 Multiple Summaries at once\n\n7.7.1 favstats() from the mosaic package\nThe favstats function adds the standard deviation, and counts of overall and missing observations to our usual summary for a continuous variable. Let’s look at systolic blood pressure, because we haven’t yet.\n\nmosaic::favstats(~ SBP, data = nh_750)\n\n min  Q1 median  Q3 max     mean       sd   n missing\n  83 108    118 127 209 118.7908 15.14329 717      33\n\n\nWe could, of course, duplicate these results with several summarise() pieces…\n\nnh_750 |&gt;\n    filter(complete.cases(SBP)) |&gt;\n    summarise(min = min(SBP), Q1 = quantile(SBP, 0.25), \n              median = median(SBP), Q3 = quantile(SBP, 0.75), \n              max = max(SBP),  mean = mean(SBP), \n              sd = sd(SBP), n = n(), miss = sum(is.na(SBP))) |&gt;\n  kbl(digits = 2)\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmiss\n\n\n83\n108\n118\n127\n209\n118.79\n15.14\n717\n0\n\n\n\n\nThe somewhat unusual structure of favstats (complete with an easy to forget ~) is actually helpful. It allows you to look at some interesting grouping approaches, like this:\n\nmosaic::favstats(SBP ~ Education, data = nh_750)\n\n       Education min     Q1 median     Q3 max     mean       sd   n missing\n1      8th Grade  96 110.25  119.5 129.75 167 122.4565 16.34993  46       4\n2 9 - 11th Grade  85 107.75  116.0 127.00 191 118.8026 15.79453  76       0\n3    High School  84 111.50  120.5 129.00 209 121.0882 16.52853 136       7\n4   Some College  85 108.00  117.0 126.00 186 118.6293 14.32736 232       9\n5   College Grad  83 107.00  117.0 125.00 171 116.8326 14.41202 227      13\n\n\nOf course, we could accomplish the same comparison with dplyr commands, too, but the favstats approach has much to offer.\n\nnh_750 |&gt;\n    filter(complete.cases(SBP, Education)) |&gt;\n    group_by(Education) |&gt;\n    summarise(min = min(SBP), Q1 = quantile(SBP, 0.25), \n              median = median(SBP), Q3 = quantile(SBP, 0.75), \n              max = max(SBP), mean = mean(SBP), \n              sd = sd(SBP), n = n(), miss = sum(is.na(SBP))) |&gt;\n    kbl(digits = 2)\n\n\n\nEducation\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmiss\n\n\n\n8th Grade\n96\n110.25\n119.5\n129.75\n167\n122.46\n16.35\n46\n0\n\n\n9 - 11th Grade\n85\n107.75\n116.0\n127.00\n191\n118.80\n15.79\n76\n0\n\n\nHigh School\n84\n111.50\n120.5\n129.00\n209\n121.09\n16.53\n136\n0\n\n\nSome College\n85\n108.00\n117.0\n126.00\n186\n118.63\n14.33\n232\n0\n\n\nCollege Grad\n83\n107.00\n117.0\n125.00\n171\n116.83\n14.41\n227\n0\n\n\n\n\n\n\n7.7.2 Using descr() from summarytools\n\nThe descr() function from the summarytools package produces numerous numerical summaries for quantities.\n\nnh_750 |&gt;\n  select(Age, BMI, SBP, DBP, Pulse) |&gt;\n  descr() |&gt;\n  kbl(digits = 2) |&gt;\n  kable_styling(full_width = F)\n\n\n\n\nAge\nBMI\nDBP\nPulse\nSBP\n\n\n\nMean\n40.82\n29.08\n72.69\n73.53\n118.79\n\n\nStd.Dev\n12.54\n7.24\n11.34\n11.65\n15.14\n\n\nMin\n21.00\n16.70\n0.00\n40.00\n83.00\n\n\nQ1\n30.00\n24.20\n66.00\n66.00\n108.00\n\n\nMedian\n40.00\n27.90\n73.00\n72.00\n118.00\n\n\nQ3\n51.00\n32.10\n80.00\n80.00\n127.00\n\n\nMax\n64.00\n80.60\n108.00\n124.00\n209.00\n\n\nMAD\n14.83\n5.93\n10.38\n11.86\n13.34\n\n\nIQR\n21.00\n7.90\n14.00\n14.00\n19.00\n\n\nCV\n0.31\n0.25\n0.16\n0.16\n0.13\n\n\nSkewness\n0.16\n1.72\n-0.28\n0.48\n0.96\n\n\nSE.Skewness\n0.09\n0.09\n0.09\n0.09\n0.09\n\n\nKurtosis\n-1.15\n6.16\n2.59\n0.73\n3.10\n\n\nN.Valid\n750.00\n745.00\n717.00\n718.00\n717.00\n\n\nPct.Valid\n100.00\n99.33\n95.60\n95.73\n95.60\n\n\n\n\n\nThe additional statistics presented here are:\n\n\nMAD = the median absolute deviation (from the median), which can be used in a manner similar to the standard deviation or IQR to measure spread.\n\nIf the data are \\(Y_1, Y_2, ..., Y_n\\), then the MAD is defined as \\(median(|Y_i - median(Y_i)|)\\).\nTo find the MAD for a set of numbers, find the median, subtract the median from each value and find the absolute value of that difference, and then find the median of those absolute differences.\nFor non-normal data with a skewed shape but tails well approximated by the Normal, the MAD is likely to be a better (more robust) estimate of the spread than is the standard deviation.\n\n\n\nCV = the coefficient of variation, or the standard deviation divided by the mean\na measure of Skewness and its standard error. This Skewness measure refers to how much asymmetry is present in the shape of the distribution. The measure is not the same as the nonparametric skew measure that we will usually prefer and that we discuss further in Section 10.13.\nOur nonparametric skew measure is just the difference between the mean and the median, divided by the standard deviation.\nThe Wikipedia page on skewness is very detailed.\na measure of Kurtosis, which refers to how outlier-prone, or heavy-tailed the shape of the distribution is, as compared to a Normal distribution.\nthe number of valid (non-missing) observations in each variable.\n\nRecall that in Chapter 3, we saw the use of an adjustment to show only “common” summaries. We can pick and choose from the entire list of available summaries. See the summarytools package vignette for more details.\n\nnh_750 |&gt;\n  select(Age, BMI, SBP, DBP, Pulse) |&gt;\n  descr(stats = \"common\") |&gt;\n  kbl(digits = 2) |&gt;\n  kable_styling(full_width = F)\n\n\n\n\nAge\nBMI\nDBP\nPulse\nSBP\n\n\n\nMean\n40.82\n29.08\n72.69\n73.53\n118.79\n\n\nStd.Dev\n12.54\n7.24\n11.34\n11.65\n15.14\n\n\nMin\n21.00\n16.70\n0.00\n40.00\n83.00\n\n\nMedian\n40.00\n27.90\n73.00\n72.00\n118.00\n\n\nMax\n64.00\n80.60\n108.00\n124.00\n209.00\n\n\nN.Valid\n750.00\n745.00\n717.00\n718.00\n717.00\n\n\nPct.Valid\n100.00\n99.33\n95.60\n95.73\n95.60\n\n\n\n\n\n\n7.7.3 describe in the psych package\nThe psych package has an even more detailed list of numerical summaries for quantitative variables that lets us look at a group of observations at once.\n\npsych::describe(nh_750 |&gt; select(Age, BMI, SBP, DBP, Pulse))\n\n      vars   n   mean    sd median trimmed   mad  min   max range  skew\nAge      1 750  40.82 12.54   40.0   40.53 14.83 21.0  64.0  43.0  0.16\nBMI      2 745  29.08  7.24   27.9   28.31  5.93 16.7  80.6  63.9  1.72\nSBP      3 717 118.79 15.14  118.0  117.88 13.34 83.0 209.0 126.0  0.96\nDBP      4 717  72.69 11.34   73.0   72.65 10.38  0.0 108.0 108.0 -0.28\nPulse    5 718  73.53 11.65   72.0   73.11 11.86 40.0 124.0  84.0  0.48\n      kurtosis   se\nAge      -1.15 0.46\nBMI       6.16 0.27\nSBP       3.10 0.57\nDBP       2.59 0.42\nPulse     0.73 0.43\n\n\nThe additional statistics presented here, beyond those discussed previously, are:\n\n\ntrimmed = a trimmed mean (by default in this function, this removes the top and bottom 10% from the data, then computes the mean of the remaining values - the middle 80% of the full data set.)\n\nse = the standard error of the sample mean, equal to the sample sd divided by the square root of the sample size.\n\n7.7.4 The Hmisc package’s version of describe\n\n\nHmisc::describe(nh_750 |&gt; \n                  select(Age, BMI, SBP, DBP, Pulse))\n\nselect(nh_750, Age, BMI, SBP, DBP, Pulse) \n\n 5  Variables      750  Observations\n--------------------------------------------------------------------------------\nAge \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     750        0       44    0.999    40.82    14.46       22       24 \n     .25      .50      .75      .90      .95 \n      30       40       51       59       62 \n\nlowest : 21 22 23 24 25, highest: 60 61 62 63 64\n--------------------------------------------------------------------------------\nBMI \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     745        5      250        1    29.08    7.538    20.22    21.30 \n     .25      .50      .75      .90      .95 \n   24.20    27.90    32.10    37.60    41.28 \n\nlowest : 16.7 17.6 17.8 17.9 18  , highest: 59.1 62.8 63.3 69   80.6\n--------------------------------------------------------------------------------\nSBP \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     717       33       81    0.999    118.8    16.36     98.0    102.0 \n     .25      .50      .75      .90      .95 \n   108.0    118.0    127.0    137.0    144.2 \n\nlowest :  83  84  85  86  89, highest: 171 179 186 191 209\n--------------------------------------------------------------------------------\nDBP \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     717       33       66    0.999    72.69    12.43       55       59 \n     .25      .50      .75      .90      .95 \n      66       73       80       86       91 \n\nlowest :   0  25  41  42  44, highest: 104 105 106 107 108\n--------------------------------------------------------------------------------\nPulse \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     718       32       37    0.997    73.53    12.95       56       60 \n     .25      .50      .75      .90      .95 \n      66       72       80       88       94 \n\nlowest :  40  44  46  48  50, highest: 108 112 114 118 124\n--------------------------------------------------------------------------------\n\n\nThe Hmisc package’s version of describe for a distribution of data presents three new ideas, in addition to a more comprehensive list of quartiles (the 5th, 10th, 25th, 50th, 75th, 90th and 95th are shown) and the lowest and highest few observations. These are:\n\n\ndistinct - the number of different values observed in the data.\n\nInfo - a measure of how “continuous” the variable is, related to how many “ties” there are in the data, with Info taking a higher value (closer to its maximum of one) if the data are more continuous.\n\nGmd - the Gini mean difference - a robust measure of spread that is calculated as the mean absolute difference between any pairs of observations. Larger values of Gmd indicate more spread-out distributions. (Gini is usually pronounced as “Genie”.)\n\n7.7.5 Using tbl_summary() from gtsummary\n\nIf you want to produce results which look like you might expect to see in a published paper, the tbl_summary() function from the gtsummary package has many nice features. Here, we’ll just show the medians, and 25th and 75th percentiles. Recall that we looked at some other options for this tool back in Chapter 3.\n\nnh_750 |&gt; \n  select(Age, BMI, SBP, DBP, Pulse) |&gt; \n  tbl_summary()\n\n\n\n\n\nCharacteristic\n\nN = 7501\n\n\n\n\nAge\n40 (30, 51)\n\n\nBMI\n28 (24, 32)\n\n\n    Unknown\n5\n\n\nSBP\n118 (108, 127)\n\n\n    Unknown\n33\n\n\nDBP\n73 (66, 80)\n\n\n    Unknown\n33\n\n\nPulse\n72 (66, 80)\n\n\n    Unknown\n32\n\n\n\n\n1 Median (IQR)\n\n\n\n\n\n\n7.7.6 Some Other Options\nYou’ll recall that in our discussion of the Palmer Penguins, back in Chapter 3, we used several other strategies to develop graphical and numerical summaries of quantities, and all of those approaches could be used here, too.\n\nThe DataExplorer package can be used for more automated exploratory data analyses.\nSome people also like skimr which has some similar goals.\n\nNext, we’ll focus on a few tools for summarizing categorical information.\n\n\n\n\nBock, David E., Paul F. Velleman, and Richard D. De Veaux. 2004. Stats: Modelling the World. Boston MA: Pearson Addison-Wesley.\n\n\nHadley Wickham, Mine Çetinyaka-Rundel, and Garrett Grolemund. 2023. R for Data Science. Second. O’Reilly. https://r4ds.hadley.nz/.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summarizing Quantities</span>"
    ]
  },
  {
    "objectID": "07-summarizing_quantities.html#footnotes",
    "href": "07-summarizing_quantities.html#footnotes",
    "title": "7  Summarizing Quantities",
    "section": "",
    "text": "The quantiles (sometimes referred to as percentiles) can also be summarized with a boxplot.↩︎\nWe could also use !is.na in place of complete.cases to accomplish the same thing.↩︎\nSee the documentation for the modeest package’s mlv function to look at other definitions of the mode.↩︎\nWhen we divide by n-1 as we calculate the sample variance, the average of the sample variances for all possible samples is equal to the population variance. If we instead divided by n, the average sample variance across all possible samples would be a little smaller than the population variance.↩︎",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summarizing Quantities</span>"
    ]
  },
  {
    "objectID": "08-summarizing_categories.html",
    "href": "08-summarizing_categories.html",
    "title": "\n8  Summarizing Categories\n",
    "section": "",
    "text": "8.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor)\nlibrary(kableExtra)\nlibrary(gt)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Summarizing Categories</span>"
    ]
  },
  {
    "objectID": "08-summarizing_categories.html#using-the-nh_adult750-data-again",
    "href": "08-summarizing_categories.html#using-the-nh_adult750-data-again",
    "title": "\n8  Summarizing Categories\n",
    "section": "\n8.2 Using the nh_adult750 data again",
    "text": "8.2 Using the nh_adult750 data again\nTo demonstrate key ideas in this Chapter, we will again consider our sample of 750 adults ages 21-64 from NHANES 2011-12 which includes some missing values. We’ll load into the nh_750 data frame the information from the nh_adult750.Rds file we created in Section @ref(newNHANES).\n\nnh_750 &lt;- read_rds(\"data/nh_adult750.Rds\")\n\nSummarizing categorical variables numerically is mostly about building tables, and calculating percentages or proportions. We’ll save our discussion of modeling categorical data for later. Recall that in the nh_750 data set we built in Section @ref(newNHANES) we had the following categorical variables. The number of levels indicates the number of possible categories for each categorical variable.\n\n\nVariable\nDescription\nLevels\nType\n\n\n\nSex\nsex of subject\n2\nbinary\n\n\nRace\nsubject’s race\n6\nnominal\n\n\nEducation\nsubject’s educational level\n5\nordinal\n\n\nPhysActive\nParticipates in sports?\n2\nbinary\n\n\nSmoke100\nSmoked 100+ cigarettes?\n2\nbinary\n\n\nSleepTrouble\nTrouble sleeping?\n2\nbinary\n\n\nHealthGen\nSelf-report health\n5\nordinal",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Summarizing Categories</span>"
    ]
  },
  {
    "objectID": "08-summarizing_categories.html#the-summary-function-for-categorical-data",
    "href": "08-summarizing_categories.html#the-summary-function-for-categorical-data",
    "title": "\n8  Summarizing Categories\n",
    "section": "\n8.3 The summary function for Categorical data",
    "text": "8.3 The summary function for Categorical data\nWhen R recognizes a variable as categorical, it stores it as a factor. Such variables get special treatment from the summary function, in particular a table of available values (so long as there aren’t too many.)\n\nnh_750 |&gt;\n  select(Sex, Race, Education, PhysActive, Smoke100, \n         SleepTrouble, HealthGen, MaritalStatus) |&gt;\n  summary()\n\n     Sex            Race              Education   PhysActive Smoke100 \n female:388   Asian   : 70   8th Grade     : 50   No :326    No :453  \n male  :362   Black   :128   9 - 11th Grade: 76   Yes:424    Yes:297  \n              Hispanic: 63   High School   :143                       \n              Mexican : 80   Some College  :241                       \n              White   :393   College Grad  :240                       \n              Other   : 16                                            \n SleepTrouble     HealthGen        MaritalStatus\n No :555      Excellent: 84   Divorced    : 78  \n Yes:195      Vgood    :197   LivePartner : 70  \n              Good     :252   Married     :388  \n              Fair     :104   NeverMarried:179  \n              Poor     : 14   Separated   : 19  \n              NA's     : 99   Widowed     : 16",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Summarizing Categories</span>"
    ]
  },
  {
    "objectID": "08-summarizing_categories.html#tables-to-describe-one-categorical-variable",
    "href": "08-summarizing_categories.html#tables-to-describe-one-categorical-variable",
    "title": "\n8  Summarizing Categories\n",
    "section": "\n8.4 Tables to describe One Categorical Variable",
    "text": "8.4 Tables to describe One Categorical Variable\nSuppose we build a table (using the tabyl function from the janitor package) to describe the HealthGen distribution.\n\nnh_750 |&gt;\n    tabyl(HealthGen) |&gt;\n    adorn_pct_formatting()\n\n HealthGen   n percent valid_percent\n Excellent  84   11.2%         12.9%\n     Vgood 197   26.3%         30.3%\n      Good 252   33.6%         38.7%\n      Fair 104   13.9%         16.0%\n      Poor  14    1.9%          2.2%\n      &lt;NA&gt;  99   13.2%             -\n\n\nNote how the missing (&lt;NA&gt;) values are not included in the valid_percent calculation, but are in the percent calculation. Note also the use of percentage formatting.\nWhat if we want to add a total count, sometimes called the marginal total?\n\nnh_750 |&gt;\n    tabyl(HealthGen) |&gt;\n    adorn_totals() |&gt;\n    adorn_pct_formatting()\n\n HealthGen   n percent valid_percent\n Excellent  84   11.2%         12.9%\n     Vgood 197   26.3%         30.3%\n      Good 252   33.6%         38.7%\n      Fair 104   13.9%         16.0%\n      Poor  14    1.9%          2.2%\n      &lt;NA&gt;  99   13.2%             -\n     Total 750  100.0%        100.0%\n\n\nWhat about marital status, which has no missing data in our sample?\n\nnh_750 |&gt;\n    tabyl(MaritalStatus) |&gt;\n    adorn_totals() |&gt;\n    adorn_pct_formatting()\n\n MaritalStatus   n percent\n      Divorced  78   10.4%\n   LivePartner  70    9.3%\n       Married 388   51.7%\n  NeverMarried 179   23.9%\n     Separated  19    2.5%\n       Widowed  16    2.1%\n         Total 750  100.0%",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Summarizing Categories</span>"
    ]
  },
  {
    "objectID": "08-summarizing_categories.html#constructing-tables-well",
    "href": "08-summarizing_categories.html#constructing-tables-well",
    "title": "\n8  Summarizing Categories\n",
    "section": "\n8.5 Constructing Tables Well",
    "text": "8.5 Constructing Tables Well\nThe prolific Howard Wainer is responsible for many interesting books on visualization and related issues, including Wainer (2005) and Wainer (2013). These rules come from Chapter 10 of Wainer (1997).\n\nOrder the rows and columns in a way that makes sense.\nRound, a lot!\nALL is different and important\n\n\n8.5.1 Alabama First!\nWhich of these Tables is more useful to you?\n2013 Percent of Students in grades 9-12 who are obese\n\n\nState\n% Obese\n95% CI\nSample Size\n\n\n\nAlabama\n17.1\n(14.6 - 19.9)\n1,499\n\n\nAlaska\n12.4\n(10.5-14.6)\n1,167\n\n\nArizona\n10.7\n(8.3-13.6)\n1,520\n\n\nArkansas\n17.8\n(15.7-20.1)\n1,470\n\n\nConnecticut\n12.3\n(10.2-14.7)\n2,270\n\n\nDelaware\n14.2\n(12.9-15.6)\n2,475\n\n\nFlorida\n11.6\n(10.5-12.8)\n5,491\n\n\n…\n\n\n\n\n\nWisconsin\n11.6\n(9.7-13.9)\n2,771\n\n\nWyoming\n10.7\n(9.4-12.2)\n2,910\n\n\n\nor …\n\n\nState\n% Obese\n95% CI\nSample Size\n\n\n\nKentucky\n18.0\n(15.7 - 20.6)\n1,537\n\n\nArkansas\n17.8\n(15.7 - 20.1)\n1,470\n\n\nAlabama\n17.1\n(14.6 - 19.9)\n1,499\n\n\nTennessee\n16.9\n(15.1 - 18.8)\n1,831\n\n\nTexas\n15.7\n(13.9 - 17.6)\n3,039\n\n\n…\n\n\n\n\n\nMassachusetts\n10.2\n(8.5 - 12.1)\n2,547\n\n\nIdaho\n9.6\n(8.2 - 11.1)\n1,841\n\n\nMontana\n9.4\n(8.4 - 10.5)\n4,679\n\n\nNew Jersey\n8.7\n(6.8 - 11.2)\n1,644\n\n\nUtah\n6.4\n(4.8 - 8.5)\n2,136\n\n\n\nIt is a rare event when Alabama first is the best choice.\n\n8.5.2 ALL is different and important\nSummaries of rows and columns provide a measure of what is typical or usual. Sometimes a sum is helpful, at other times, consider presenting a median or other summary. The ALL category, as Wainer (1997) suggests, should be both visually different from the individual entries and set spatially apart.\nOn the whole, it’s far easier to fall into a good graph in R (at least if you have some ggplot2 skills) than to produce a good table.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Summarizing Categories</span>"
    ]
  },
  {
    "objectID": "08-summarizing_categories.html#the-mode-of-a-categorical-variable",
    "href": "08-summarizing_categories.html#the-mode-of-a-categorical-variable",
    "title": "\n8  Summarizing Categories\n",
    "section": "\n8.6 The Mode of a Categorical Variable",
    "text": "8.6 The Mode of a Categorical Variable\nA common measure applied to a categorical variable is to identify the mode, the most frequently observed value. To find the mode for variables with lots of categories (so that the summary may not be sufficient), we usually tabulate the data, and then sort by the counts of the numbers of observations, as we did with discrete quantitative variables.\n\nnh_750 |&gt;\n    group_by(HealthGen) |&gt;\n    summarise(count = n()) |&gt;\n    arrange(desc(count)) \n\n# A tibble: 6 × 2\n  HealthGen count\n  &lt;fct&gt;     &lt;int&gt;\n1 Good        252\n2 Vgood       197\n3 Fair        104\n4 &lt;NA&gt;         99\n5 Excellent    84\n6 Poor         14",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Summarizing Categories</span>"
    ]
  },
  {
    "objectID": "08-summarizing_categories.html#describe-in-the-hmisc-package",
    "href": "08-summarizing_categories.html#describe-in-the-hmisc-package",
    "title": "\n8  Summarizing Categories\n",
    "section": "\n8.7 describe in the Hmisc package",
    "text": "8.7 describe in the Hmisc package\n\nHmisc::describe(nh_750 |&gt; \n                    select(Sex, Race, Education, PhysActive, \n                           Smoke100, SleepTrouble, \n                           HealthGen, MaritalStatus))\n\nselect(nh_750, Sex, Race, Education, PhysActive, Smoke100, SleepTrouble, HealthGen, MaritalStatus) \n\n 8  Variables      750  Observations\n--------------------------------------------------------------------------------\nSex \n       n  missing distinct \n     750        0        2 \n                        \nValue      female   male\nFrequency     388    362\nProportion  0.517  0.483\n--------------------------------------------------------------------------------\nRace \n       n  missing distinct \n     750        0        6 \n                                                                \nValue         Asian    Black Hispanic  Mexican    White    Other\nFrequency        70      128       63       80      393       16\nProportion    0.093    0.171    0.084    0.107    0.524    0.021\n--------------------------------------------------------------------------------\nEducation \n       n  missing distinct \n     750        0        5 \n                                                                      \nValue           8th Grade 9 - 11th Grade    High School   Some College\nFrequency              50             76            143            241\nProportion          0.067          0.101          0.191          0.321\n                         \nValue        College Grad\nFrequency             240\nProportion          0.320\n--------------------------------------------------------------------------------\nPhysActive \n       n  missing distinct \n     750        0        2 \n                      \nValue         No   Yes\nFrequency    326   424\nProportion 0.435 0.565\n--------------------------------------------------------------------------------\nSmoke100 \n       n  missing distinct \n     750        0        2 \n                      \nValue         No   Yes\nFrequency    453   297\nProportion 0.604 0.396\n--------------------------------------------------------------------------------\nSleepTrouble \n       n  missing distinct \n     750        0        2 \n                    \nValue        No  Yes\nFrequency   555  195\nProportion 0.74 0.26\n--------------------------------------------------------------------------------\nHealthGen \n       n  missing distinct \n     651       99        5 \n                                                            \nValue      Excellent     Vgood      Good      Fair      Poor\nFrequency         84       197       252       104        14\nProportion     0.129     0.303     0.387     0.160     0.022\n--------------------------------------------------------------------------------\nMaritalStatus \n       n  missing distinct \n     750        0        6 \n                                                                           \nValue          Divorced  LivePartner      Married NeverMarried    Separated\nFrequency            78           70          388          179           19\nProportion        0.104        0.093        0.517        0.239        0.025\n                       \nValue           Widowed\nFrequency            16\nProportion        0.021\n--------------------------------------------------------------------------------",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Summarizing Categories</span>"
    ]
  },
  {
    "objectID": "08-summarizing_categories.html#cross-tabulations-of-two-variables",
    "href": "08-summarizing_categories.html#cross-tabulations-of-two-variables",
    "title": "\n8  Summarizing Categories\n",
    "section": "\n8.8 Cross-Tabulations of Two Variables",
    "text": "8.8 Cross-Tabulations of Two Variables\nIt is very common for us to want to describe the association of one categorical variable with another. For instance, is there a relationship between Education and SleepTrouble in these data?\n\nnh_750 |&gt;\n    tabyl(Education, SleepTrouble) |&gt;\n    adorn_totals(where = c(\"row\", \"col\")) \n\n      Education  No Yes Total\n      8th Grade  40  10    50\n 9 - 11th Grade  52  24    76\n    High School 102  41   143\n   Some College 173  68   241\n   College Grad 188  52   240\n          Total 555 195   750\n\n\nNote the use of adorn_totals to get the marginal counts, and how we specify that we want both the row and column totals. We can add a title for the columns with…\n\nnh_750 |&gt;\n    tabyl(Education, SleepTrouble) |&gt;\n    adorn_totals(where = c(\"row\", \"col\")) |&gt;\n    adorn_title(placement = \"combined\")\n\n Education/SleepTrouble  No Yes Total\n              8th Grade  40  10    50\n         9 - 11th Grade  52  24    76\n            High School 102  41   143\n           Some College 173  68   241\n           College Grad 188  52   240\n                  Total 555 195   750\n\n\nOften, we’ll want to show percentages in a cross-tabulation like this. To get row percentages so that we can directly see the probability of SleepTrouble = Yes for each level of Education, we can use:\n\nnh_750 |&gt;\n    tabyl(Education, SleepTrouble) |&gt;\n    adorn_totals(where = \"row\") |&gt;\n    adorn_percentages(denominator = \"row\") |&gt;\n    adorn_pct_formatting() |&gt;\n    adorn_title(placement = \"combined\")\n\n Education/SleepTrouble    No   Yes\n              8th Grade 80.0% 20.0%\n         9 - 11th Grade 68.4% 31.6%\n            High School 71.3% 28.7%\n           Some College 71.8% 28.2%\n           College Grad 78.3% 21.7%\n                  Total 74.0% 26.0%\n\n\nIf we want to compare the distribution of Education between the two levels of SleepTrouble with column percentages, we can use the following…\n\nnh_750 |&gt;\n    tabyl(Education, SleepTrouble) |&gt;\n    adorn_totals(where = \"col\") |&gt;\n    adorn_percentages(denominator = \"col\") |&gt;\n    adorn_pct_formatting() |&gt;\n    adorn_title(placement = \"combined\") \n\n Education/SleepTrouble    No   Yes Total\n              8th Grade  7.2%  5.1%  6.7%\n         9 - 11th Grade  9.4% 12.3% 10.1%\n            High School 18.4% 21.0% 19.1%\n           Some College 31.2% 34.9% 32.1%\n           College Grad 33.9% 26.7% 32.0%\n\n\nIf we want overall percentages in the cells of the table, so that the total across all combinations of Education and SleepTrouble is 100%, we can use:\n\nnh_750 |&gt;\n    tabyl(Education, SleepTrouble) |&gt;\n    adorn_totals(where = c(\"row\", \"col\")) |&gt;\n    adorn_percentages(denominator = \"all\") |&gt;\n    adorn_pct_formatting() |&gt;\n    adorn_title(placement = \"combined\") |&gt;\n    kbl(align = 'lrrrrrr') \n\n\n\nEducation/SleepTrouble\nNo\nYes\nTotal\n\n\n\n8th Grade\n5.3%\n1.3%\n6.7%\n\n\n9 - 11th Grade\n6.9%\n3.2%\n10.1%\n\n\nHigh School\n13.6%\n5.5%\n19.1%\n\n\nSome College\n23.1%\n9.1%\n32.1%\n\n\nCollege Grad\n25.1%\n6.9%\n32.0%\n\n\nTotal\n74.0%\n26.0%\n100.0%\n\n\n\n\n\nAnother common approach is to include both counts and percentages in a cross-tabulation. Let’s look at the breakdown of HealthGen by MaritalStatus.\n\nnh_750 |&gt;\n    tabyl(MaritalStatus, HealthGen) |&gt;\n    adorn_totals(where = c(\"row\")) |&gt;\n    adorn_percentages(denominator = \"row\") |&gt;\n    adorn_pct_formatting() |&gt;\n    adorn_ns(position = \"front\") |&gt;\n    adorn_title(placement = \"combined\") |&gt;\n    kbl(align = 'lrrrrrr') |&gt;\n    kable_styling(full_width = FALSE)\n\n\n\nMaritalStatus/HealthGen\nExcellent\nVgood\nGood\nFair\nPoor\nNA_\n\n\n\nDivorced\n7 (9.0%)\n19 (24.4%)\n29 (37.2%)\n11 (14.1%)\n3 (3.8%)\n9 (11.5%)\n\n\nLivePartner\n4 (5.7%)\n19 (27.1%)\n25 (35.7%)\n18 (25.7%)\n0 (0.0%)\n4 (5.7%)\n\n\nMarried\n46 (11.9%)\n101 (26.0%)\n130 (33.5%)\n41 (10.6%)\n6 (1.5%)\n64 (16.5%)\n\n\nNeverMarried\n25 (14.0%)\n52 (29.1%)\n56 (31.3%)\n24 (13.4%)\n3 (1.7%)\n19 (10.6%)\n\n\nSeparated\n2 (10.5%)\n3 (15.8%)\n4 (21.1%)\n8 (42.1%)\n0 (0.0%)\n2 (10.5%)\n\n\nWidowed\n0 (0.0%)\n3 (18.8%)\n8 (50.0%)\n2 (12.5%)\n2 (12.5%)\n1 (6.2%)\n\n\nTotal\n84 (11.2%)\n197 (26.3%)\n252 (33.6%)\n104 (13.9%)\n14 (1.9%)\n99 (13.2%)\n\n\n\n\n\nWhat if we wanted to ignore the missing HealthGen values? Most often, I filter down to the complete observations.\n\nnh_750 |&gt;\n    filter(complete.cases(MaritalStatus, HealthGen)) |&gt;\n    tabyl(MaritalStatus, HealthGen) |&gt;\n    adorn_totals(where = c(\"row\")) |&gt;\n    adorn_percentages(denominator = \"row\") |&gt;\n    adorn_pct_formatting() |&gt;\n    adorn_ns(position = \"front\") |&gt;\n    adorn_title(placement = \"combined\") |&gt;\n    kbl(align = 'lrrrrr') |&gt;\n    kable_styling(full_width = FALSE)\n\n\n\nMaritalStatus/HealthGen\nExcellent\nVgood\nGood\nFair\nPoor\n\n\n\nDivorced\n7 (10.1%)\n19 (27.5%)\n29 (42.0%)\n11 (15.9%)\n3 (4.3%)\n\n\nLivePartner\n4 (6.1%)\n19 (28.8%)\n25 (37.9%)\n18 (27.3%)\n0 (0.0%)\n\n\nMarried\n46 (14.2%)\n101 (31.2%)\n130 (40.1%)\n41 (12.7%)\n6 (1.9%)\n\n\nNeverMarried\n25 (15.6%)\n52 (32.5%)\n56 (35.0%)\n24 (15.0%)\n3 (1.9%)\n\n\nSeparated\n2 (11.8%)\n3 (17.6%)\n4 (23.5%)\n8 (47.1%)\n0 (0.0%)\n\n\nWidowed\n0 (0.0%)\n3 (20.0%)\n8 (53.3%)\n2 (13.3%)\n2 (13.3%)\n\n\nTotal\n84 (12.9%)\n197 (30.3%)\n252 (38.7%)\n104 (16.0%)\n14 (2.2%)\n\n\n\n\n\nFor more on working with tabyls, see this overview of janitor functions. There you’ll find a complete list of all of the adorn functions, for example.\nHere’s another approach, to look at the cross-classification of Race and HealthGen:\n\nxtabs(~ Race + HealthGen, data = nh_750)\n\n          HealthGen\nRace       Excellent Vgood Good Fair Poor\n  Asian           10    17   24    6    1\n  Black           15    28   40   24    4\n  Hispanic         4     9   24   13    2\n  Mexican          6    12   25   21    2\n  White           48   128  131   37    5\n  Other            1     3    8    3    0",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Summarizing Categories</span>"
    ]
  },
  {
    "objectID": "08-summarizing_categories.html#cross-classifying-three-categorical-variables",
    "href": "08-summarizing_categories.html#cross-classifying-three-categorical-variables",
    "title": "\n8  Summarizing Categories\n",
    "section": "\n8.9 Cross-Classifying Three Categorical Variables",
    "text": "8.9 Cross-Classifying Three Categorical Variables\nSuppose we are interested in Smoke100 and its relationship to PhysActive and SleepTrouble.\n\nnh_750 |&gt;\n    tabyl(Smoke100, PhysActive, SleepTrouble) |&gt;\n    adorn_title(placement = \"top\")\n\n$No\n          PhysActive    \n Smoke100         No Yes\n       No        137 219\n      Yes         93 106\n\n$Yes\n          PhysActive    \n Smoke100         No Yes\n       No         41  56\n      Yes         55  43\n\n\nThe result here is a tabyl of Smoke100 (rows) by PhysActive (columns), split into a list by SleepTrouble.\nThere are several alternative approaches for doing this, although I expect us to stick with tabyl for our work in 431. These alternatives include the use of the xtabs function:\n\nxtabs(~ Smoke100 + PhysActive + SleepTrouble, data = nh_750)\n\n, , SleepTrouble = No\n\n        PhysActive\nSmoke100  No Yes\n     No  137 219\n     Yes  93 106\n\n, , SleepTrouble = Yes\n\n        PhysActive\nSmoke100  No Yes\n     No   41  56\n     Yes  55  43\n\n\nWe can also build a flat version of this table, as follows:\n\nftable(Smoke100 ~ PhysActive + SleepTrouble, data = nh_750)\n\n                        Smoke100  No Yes\nPhysActive SleepTrouble                 \nNo         No                    137  93\n           Yes                    41  55\nYes        No                    219 106\n           Yes                    56  43\n\n\nAnd we can do this with dplyr functions and the table() function, as well, for example…\n\nnh_750 |&gt;\n    select(Smoke100, PhysActive, SleepTrouble) |&gt;\n    table() \n\n, , SleepTrouble = No\n\n        PhysActive\nSmoke100  No Yes\n     No  137 219\n     Yes  93 106\n\n, , SleepTrouble = Yes\n\n        PhysActive\nSmoke100  No Yes\n     No   41  56\n     Yes  55  43",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Summarizing Categories</span>"
    ]
  },
  {
    "objectID": "08-summarizing_categories.html#gaining-control-over-tables-in-r-the-gt-package",
    "href": "08-summarizing_categories.html#gaining-control-over-tables-in-r-the-gt-package",
    "title": "\n8  Summarizing Categories\n",
    "section": "\n8.10 Gaining Control over Tables in R: the gt package",
    "text": "8.10 Gaining Control over Tables in R: the gt package\nWith the gt package, anyone can make wonderful-looking tables using the R programming language. The gt package allows you to start with a tibble or data frame, and use it to make very detailed tables that look professional, and includes tools that enable you to include titles and subtitles, all sorts of labels, as well as footnotes and source notes.\nHere’s a fairly simple example of a cross-tabulation of part of the nh_750 data built using a few tools from the gt package.\n\ntemp_tbl &lt;- nh_750 |&gt; filter(complete.cases(PhysActive, HealthGen)) |&gt;\n  tabyl(PhysActive, HealthGen) |&gt;\n  tibble() \n\ngt(temp_tbl) |&gt;\n  tab_header(title = md(\"**Cross-Tabulation from nh_750**\"),\n             subtitle = md(\"Physical Activity vs. Overall Health\"))\n\n\n\n\n\n\nCross-Tabulation from nh_750\n\n\nPhysical Activity vs. Overall Health\n\n\nPhysActive\nExcellent\nVgood\nGood\nFair\nPoor\n\n\n\n\nNo\n24\n66\n126\n59\n10\n\n\nYes\n60\n131\n126\n45\n4\n\n\n\n\n\n\nThe gt package and its usage is described in detail at https://gt.rstudio.com/.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Summarizing Categories</span>"
    ]
  },
  {
    "objectID": "08-summarizing_categories.html#coming-up",
    "href": "08-summarizing_categories.html#coming-up",
    "title": "\n8  Summarizing Categories\n",
    "section": "\n8.11 Coming Up",
    "text": "8.11 Coming Up\nNext, we’ll make some early attempts at describing missingness in our data.\n\n\n\n\nWainer, Howard. 1997. Visual Revelations: Graphical Tales of Fate and Deception from Napoleon Bonaparte to Ross Perot. New York: Springer-Verlag.\n\n\n———. 2005. Graphic Discovery: A Trout in the Milk and Other Visual Adventures. Princeton, NJ: Princeton University Press.\n\n\n———. 2013. Medical Illuminations: Using Evidence, Visualization and Statistical Thinking to Improve Healthcare. New York: Oxford University Press.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Summarizing Categories</span>"
    ]
  },
  {
    "objectID": "09-missing_data.html",
    "href": "09-missing_data.html",
    "title": "9  Missing Data and Single Imputation",
    "section": "",
    "text": "9.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor)\nlibrary(naniar)\nlibrary(simputation)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\nWe’ll focus on tools from the naniar and simputation packages in this chapter. This chapter also requires that the mosaic package is loaded on your machine so we can use the favstats() function, but I won’t load that here.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Missing Data and Single Imputation</span>"
    ]
  },
  {
    "objectID": "09-missing_data.html#a-toy-example-n-15",
    "href": "09-missing_data.html#a-toy-example-n-15",
    "title": "9  Missing Data and Single Imputation",
    "section": "\n9.2 A Toy Example (n = 15)",
    "text": "9.2 A Toy Example (n = 15)\nIn the following tiny data set called sbp_example, we have four variables for a set of 15 subjects. In addition to a subject id, we have:\n\nthe treatment this subject received (A, B or C are the treatments),\nan indicator (1 = yes, 0 = no) of whether the subject has diabetes,\nthe subject’s systolic blood pressure at baseline\nthe subject’s systolic blood pressure after the application of the treatment\n\n\n# create some temporary variables\nsubject &lt;- 101:115\nx1 &lt;- c(\"A\", \"B\", \"C\", \"A\", \"C\", \"A\", \"A\", NA, \"B\", \"C\", \"A\", \"B\", \"C\", \"A\", \"B\")\nx2 &lt;- c(1, 0, 0, 1, NA, 1, 0, 1, NA, 1, 0, 0, 1, 1, NA)\nx3 &lt;- c(120, 145, 150, NA, 155, NA, 135, NA, 115, 170, 150, 145, 140, 160, 135)\nx4 &lt;- c(105, 135, 150, 120, 135, 115, 160, 150, 130, 155, 140, 140, 150, 135, 120)\n\nsbp_example &lt;- \n  tibble(subject, treat = factor(x1), diabetes = x2, \n         sbp.before = x3, sbp.after = x4) \n\nrm(subject, x1, x2, x3, x4) # just cleaning up\n\nsbp_example\n\n# A tibble: 15 × 5\n   subject treat diabetes sbp.before sbp.after\n     &lt;int&gt; &lt;fct&gt;    &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1     101 A            1        120       105\n 2     102 B            0        145       135\n 3     103 C            0        150       150\n 4     104 A            1         NA       120\n 5     105 C           NA        155       135\n 6     106 A            1         NA       115\n 7     107 A            0        135       160\n 8     108 &lt;NA&gt;         1         NA       150\n 9     109 B           NA        115       130\n10     110 C            1        170       155\n11     111 A            0        150       140\n12     112 B            0        145       140\n13     113 C            1        140       150\n14     114 A            1        160       135\n15     115 B           NA        135       120",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Missing Data and Single Imputation</span>"
    ]
  },
  {
    "objectID": "09-missing_data.html#identifying-missingness-with-naniar-functions",
    "href": "09-missing_data.html#identifying-missingness-with-naniar-functions",
    "title": "9  Missing Data and Single Imputation",
    "section": "\n9.3 Identifying missingness with naniar functions",
    "text": "9.3 Identifying missingness with naniar functions\nThe naniar package has many useful functions.\n\nHow many missing values do we have, overall?\n\n\nn_miss(sbp_example)\n\n[1] 7\n\n\n\nHow many variables have missing values, overall?\n\n\nn_var_miss(sbp_example)\n\n[1] 3\n\n\n\nWhich variables contain missing values?\n\n\nmiss_var_which(sbp_example)\n\n[1] \"treat\"      \"diabetes\"   \"sbp.before\"\n\n\n\nHow many missing values do we have in each variable?\n\n\nmiss_var_summary(sbp_example)\n\n# A tibble: 5 × 3\n  variable   n_miss pct_miss\n  &lt;chr&gt;       &lt;int&gt;    &lt;num&gt;\n1 diabetes        3    20   \n2 sbp.before      3    20   \n3 treat           1     6.67\n4 subject         0     0   \n5 sbp.after       0     0   \n\n\nWe are missing one treat, 3 diabetes and 3 sbp.before values.\n\nCan we plot missingness, by variable?\n\n\ngg_miss_var(sbp_example)\n\n\n\n\n\n\n\n\nHow many of the cases (rows) have missing values?\n\n\nn_case_miss(sbp_example)\n\n[1] 6\n\n\n\nHow many cases have complete data, with no missing values?\n\n\nn_case_complete(sbp_example)\n\n[1] 9\n\n\n\nCan we tabulate missingness by case?\n\n\nmiss_case_table(sbp_example)\n\n# A tibble: 3 × 3\n  n_miss_in_case n_cases pct_cases\n           &lt;int&gt;   &lt;int&gt;     &lt;dbl&gt;\n1              0       9     60   \n2              1       5     33.3 \n3              2       1      6.67\n\n\n\nWhich cases have missing values?\n\n\nmiss_case_summary(sbp_example)\n\n# A tibble: 15 × 3\n    case n_miss pct_miss\n   &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;\n 1     8      2       40\n 2     4      1       20\n 3     5      1       20\n 4     6      1       20\n 5     9      1       20\n 6    15      1       20\n 7     1      0        0\n 8     2      0        0\n 9     3      0        0\n10     7      0        0\n11    10      0        0\n12    11      0        0\n13    12      0        0\n14    13      0        0\n15    14      0        0\n\n\n\nHow can we identify the subjects with missing data?\n\n\nsbp_example |&gt; filter(!complete.cases(sbp_example))\n\n# A tibble: 6 × 5\n  subject treat diabetes sbp.before sbp.after\n    &lt;int&gt; &lt;fct&gt;    &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1     104 A            1         NA       120\n2     105 C           NA        155       135\n3     106 A            1         NA       115\n4     108 &lt;NA&gt;         1         NA       150\n5     109 B           NA        115       130\n6     115 B           NA        135       120\n\n\nWe have nine subjects with complete data, three subjects with missing diabetes (only), two subjects with missing sbp.before (only), and 1 subject who is missing both treat and sbp.before.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Missing Data and Single Imputation</span>"
    ]
  },
  {
    "objectID": "09-missing_data.html#missing-data-mechanisms",
    "href": "09-missing_data.html#missing-data-mechanisms",
    "title": "9  Missing Data and Single Imputation",
    "section": "\n9.4 Missing-data mechanisms",
    "text": "9.4 Missing-data mechanisms\nMy source for this description of mechanisms is Chapter 25 of Gelman and Hill (2007), and that chapter is available at this link.\n\n\nMCAR = Missingness completely at random. A variable is missing completely at random if the probability of missingness is the same for all units, for example, if for each subject, we decide whether to collect the diabetes status by rolling a die and refusing to answer if a “6” shows up. If data are missing completely at random, then throwing out cases with missing data does not bias your inferences.\n\nMissingness that depends only on observed predictors. A more general assumption, called missing at random or MAR, is that the probability a variable is missing depends only on available information. Here, we would have to be willing to assume that the probability of nonresponse to diabetes depends only on the other, fully recorded variables in the data. It is often reasonable to model this process as a logistic regression, where the outcome variable equals 1 for observed cases and 0 for missing. When an outcome variable is missing at random, it is acceptable to exclude the missing cases (that is, to treat them as NA), as long as the regression controls for all the variables that affect the probability of missingness.\n\nMissingness that depends on unobserved predictors. Missingness is no longer “at random” if it depends on information that has not been recorded and this information also predicts the missing values. If a particular treatment causes discomfort, a patient is more likely to drop out of the study. This missingness is not at random (unless “discomfort” is measured and observed for all patients). If missingness is not at random, it must be explicitly modeled, or else you must accept some bias in your inferences.\n\nMissingness that depends on the missing value itself. Finally, a particularly difficult situation arises when the probability of missingness depends on the (potentially missing) variable itself. For example, suppose that people with higher earnings are less likely to reveal them.\n\nEssentially, situations 3 and 4 are referred to collectively as non-random missingness, and cause more trouble for us than 1 and 2.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Missing Data and Single Imputation</span>"
    ]
  },
  {
    "objectID": "09-missing_data.html#dealing-with-missingness-three-approaches",
    "href": "09-missing_data.html#dealing-with-missingness-three-approaches",
    "title": "9  Missing Data and Single Imputation",
    "section": "\n9.5 Dealing with Missingness: Three Approaches",
    "text": "9.5 Dealing with Missingness: Three Approaches\nThere are several available methods for dealing with missing data that are MCAR or MAR, but they basically boil down to:\n\nComplete Case (or Available Case) analyses\nSingle Imputation\nMultiple Imputation",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Missing Data and Single Imputation</span>"
    ]
  },
  {
    "objectID": "09-missing_data.html#complete-case-and-available-case-analyses",
    "href": "09-missing_data.html#complete-case-and-available-case-analyses",
    "title": "9  Missing Data and Single Imputation",
    "section": "\n9.6 Complete Case (and Available Case) analyses",
    "text": "9.6 Complete Case (and Available Case) analyses\nIn Complete Case analyses, rows containing NA values are omitted from the data before analyses commence. This is the default approach for many statistical software packages, and may introduce unpredictable bias and fail to include some useful, often hard-won information.\n\nA complete case analysis can be appropriate when the number of missing observations is not large, and the missing pattern is either MCAR (missing completely at random) or MAR (missing at random.)\nTwo problems arise with complete-case analysis:\n\nIf the units with missing values differ systematically from the completely observed cases, this could bias the complete-case analysis.\nIf many variables are included in a model, there may be very few complete cases, so that most of the data would be discarded for the sake of a straightforward analysis.\n\n\nA related approach is available-case analysis where different aspects of a problem are studied with different subsets of the data, perhaps identified on the basis of what is missing in them.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Missing Data and Single Imputation</span>"
    ]
  },
  {
    "objectID": "09-missing_data.html#building-a-complete-case-analysis",
    "href": "09-missing_data.html#building-a-complete-case-analysis",
    "title": "9  Missing Data and Single Imputation",
    "section": "\n9.7 Building a Complete Case Analysis",
    "text": "9.7 Building a Complete Case Analysis\nWe can drop all of the missing values from a data set with drop_na or with na.omit or by filtering for complete.cases. Any of these approaches produces the same result - a new data set with 9 rows (after dropping the six subjects with any NA values) and 5 columns.\n\ncc.1 &lt;- na.omit(sbp_example)\ncc.2 &lt;- sbp_example |&gt; drop_na()\ncc.3 &lt;- sbp_example |&gt; filter(complete.cases(sbp_example))",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Missing Data and Single Imputation</span>"
    ]
  },
  {
    "objectID": "09-missing_data.html#single-imputation",
    "href": "09-missing_data.html#single-imputation",
    "title": "9  Missing Data and Single Imputation",
    "section": "\n9.8 Single Imputation",
    "text": "9.8 Single Imputation\nIn single imputation analyses, NA values are estimated/replaced one time with one particular data value for the purpose of obtaining more complete samples, at the expense of creating some potential bias in the eventual conclusions or obtaining slightly less accurate estimates than would be available if there were no missing values in the data.\n\nA single imputation can be just a replacement with the mean or median (for a quantity) or the mode (for a categorical variable.) However, such an approach, though easy to understand, underestimates variance and ignores the relationship of missing values to other variables.\nSingle imputation can also be done using a variety of models to try to capture information about the NA values that are available in other variables within the data set.\nThe simputation package can help us execute single imputations using a wide variety of techniques, within the pipe approach used by the tidyverse. Another approach I have used in the past is the mice package, which can also perform single imputations.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Missing Data and Single Imputation</span>"
    ]
  },
  {
    "objectID": "09-missing_data.html#single-imputation-with-the-mean-or-mode",
    "href": "09-missing_data.html#single-imputation-with-the-mean-or-mode",
    "title": "9  Missing Data and Single Imputation",
    "section": "\n9.9 Single Imputation with the Mean or Mode",
    "text": "9.9 Single Imputation with the Mean or Mode\nThe most straightforward approach to single imputation is to impute a single summary of the variable, such as the mean, median or mode.\n\nmosaic::favstats(~ sbp.before, data = sbp_example)\n\n min  Q1 median     Q3 max     mean       sd  n missing\n 115 135    145 151.25 170 143.3333 15.71527 12       3\n\n\n\nsbp_example |&gt; tabyl(diabetes, treat) |&gt;\n  adorn_totals(where = c(\"row\", \"col\"))\n\n diabetes A B C NA_ Total\n        0 2 2 1   0     5\n        1 4 0 2   1     7\n     &lt;NA&gt; 0 2 1   0     3\n    Total 6 4 4   1    15\n\n\nHere, suppose we decide to impute\n\n\nsbp.before with the mean (143.3) among non-missing values,\n\ndiabetes with its more common value, 1, and\n\ntreat with its more common value, or mode (A)\n\n\nsi.1 &lt;- sbp_example |&gt;\n    replace_na(list(sbp.before = 143.33,\n                    diabetes = 1,\n                    treat = \"A\"))\nsi.1\n\n# A tibble: 15 × 5\n   subject treat diabetes sbp.before sbp.after\n     &lt;int&gt; &lt;fct&gt;    &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1     101 A            1       120        105\n 2     102 B            0       145        135\n 3     103 C            0       150        150\n 4     104 A            1       143.       120\n 5     105 C            1       155        135\n 6     106 A            1       143.       115\n 7     107 A            0       135        160\n 8     108 A            1       143.       150\n 9     109 B            1       115        130\n10     110 C            1       170        155\n11     111 A            0       150        140\n12     112 B            0       145        140\n13     113 C            1       140        150\n14     114 A            1       160        135\n15     115 B            1       135        120",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Missing Data and Single Imputation</span>"
    ]
  },
  {
    "objectID": "09-missing_data.html#doing-single-imputation-with-simputation",
    "href": "09-missing_data.html#doing-single-imputation-with-simputation",
    "title": "9  Missing Data and Single Imputation",
    "section": "\n9.10 Doing Single Imputation with simputation\n",
    "text": "9.10 Doing Single Imputation with simputation\n\nSingle imputation is a potentially appropriate method when missingness can be assumed to be either completely at random (MCAR) or dependent only on observed predictors (MAR). We’ll use the simputation package to accomplish it.\n\nThe simputation vignette is available at https://cran.r-project.org/web/packages/simputation/vignettes/intro.html\nThe simputation reference manual is available at https://cran.r-project.org/web/packages/simputation/simputation.pdf\n\nSuppose we wanted to use:\n\na robust linear model to predict sbp.before missing values, on the basis of sbp.after and diabetes status, and\na predictive mean matching approach (which, unlike the robust linear model, will ensure that only values of diabetes that we’ve seen before will be imputed) to predict diabetes status, on the basis of sbp.after, and\na decision tree approach to predict treat status, using all other variables in the data\n\n\nset.seed(432009)\n\nimp.2 &lt;- sbp_example |&gt;\n    impute_rlm(sbp.before ~ sbp.after + diabetes) |&gt;\n    impute_pmm(diabetes ~ sbp.after) |&gt;\n    impute_cart(treat ~ .)\n\nimp.2\n\n# A tibble: 15 × 5\n   subject treat diabetes sbp.before sbp.after\n *   &lt;int&gt; &lt;fct&gt;    &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1     101 A            1       120        105\n 2     102 B            0       145        135\n 3     103 C            0       150        150\n 4     104 A            1       139.       120\n 5     105 C            1       155        135\n 6     106 A            1       136.       115\n 7     107 A            0       135        160\n 8     108 A            1       155.       150\n 9     109 B            1       115        130\n10     110 C            1       170        155\n11     111 A            0       150        140\n12     112 B            0       145        140\n13     113 C            1       140        150\n14     114 A            1       160        135\n15     115 B            1       135        120\n\n\nDetails on the many available methods in simputation are provided in its manual. These include:\n\n\nimpute_cart uses a Classification and Regression Tree approach for numerical or categorical data. There is also an impute_rf command which uses Random Forests for imputation.\n\nimpute_pmm is one of several “hot deck” options for imputation, this one is predictive mean matching, which can be used with numeric data (only). Missing values are first imputed using a predictive model. Next, these predictions are replaced with the observed values which are nearest to the prediction. Other imputation options in this group include random hot deck, sequential hot deck and k-nearest neighbor imputation.\n\nimpute_rlm is one of several regression imputation methods, including linear models, robust linear models (which use what is called M-estimation to impute numerical variables) and lasso/elastic net/ridge regression models.\n\nThe simputation package can also do EM-based multivariate imputation, and multivariate random forest imputation, and several other approaches.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Missing Data and Single Imputation</span>"
    ]
  },
  {
    "objectID": "09-missing_data.html#multiple-imputation",
    "href": "09-missing_data.html#multiple-imputation",
    "title": "9  Missing Data and Single Imputation",
    "section": "\n9.11 Multiple Imputation",
    "text": "9.11 Multiple Imputation\nMultiple imputation, where NA values are repeatedly estimated/replaced with multiple data values, for the purpose of obtaining mode complete samples and capturing details of the variation inherent in the fact that the data have missingness, so as to obtain more accurate estimates than are possible with single imputation.\n\nWe’ll postpone further discussion of multiple imputation to later in the semester, when we’re building models for relationships in our data.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Missing Data and Single Imputation</span>"
    ]
  },
  {
    "objectID": "09-missing_data.html#coming-up",
    "href": "09-missing_data.html#coming-up",
    "title": "9  Missing Data and Single Imputation",
    "section": "\n9.12 Coming Up",
    "text": "9.12 Coming Up\nNext, we’ll look an even more detailed look at how we might summarize results from another large data set, this time from the National Youth Fitness Survey.\n\n\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel-Hierarchical Models. New York: Cambridge University Press. http://www.stat.columbia.edu/~gelman/arm/.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Missing Data and Single Imputation</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html",
    "href": "10-nnyfs_foundations.html",
    "title": "10  National Youth Fitness Survey",
    "section": "",
    "text": "10.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(ggridges)\nlibrary(janitor)\nlibrary(kableExtra)\nlibrary(patchwork)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\nWe also use functions from the Hmisc and mosaic packages in this chapter, but do not load the whole packages.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#what-is-the-nhanes-nyfs",
    "href": "10-nnyfs_foundations.html#what-is-the-nhanes-nyfs",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.2 What is the NHANES NYFS?",
    "text": "10.2 What is the NHANES NYFS?\nThe nnyfs.csv and the nnyfs.Rds data files were built by Professor Love using data from the 2012 National Youth Fitness Survey.\n\nThe NHANES National Youth Fitness Survey (NNYFS) was conducted in 2012 to collect data on physical activity and fitness levels in order to provide an evaluation of the health and fitness of children in the U.S. ages 3 to 15. The NNYFS collected data on physical activity and fitness levels of our youth through interviews and fitness tests.\n\nIn the nnyfs data file (either .csv or .Rds), I’m only providing a modest fraction of the available information. More on the NNYFS (including information I’m not using) is available at https://wwwn.cdc.gov/nchs/nhanes/search/nnyfs12.aspx.\nThe data elements I’m using fall into four main groups, or components:\n\nDemographics\nDietary\n\nExamination and\nQuestionnaire\n\nWhat I did was merge a few elements from each of the available components of the NHANES National Youth Fitness Survey, reformulated (and in some cases simplified) some variables, and restricted the sample to kids who had completed elements of each of the four components.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#the-variables-included-in-nnyfs",
    "href": "10-nnyfs_foundations.html#the-variables-included-in-nnyfs",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.3 The Variables included in nnyfs\n",
    "text": "10.3 The Variables included in nnyfs\n\nThis section tells you where the data come from, and briefly describe what is collected.\n\n10.3.1 From the NNYFS Demographic Component\n\nAll of these come from the Y_DEMO file.\n\n\n\n\n\n\n\nIn nnyfs\n\nIn Y_DEMO\n\nDescription\n\n\n\nSEQN\nSEQN\nSubject ID, connects all of the files\n\n\nsex\nRIAGENDR\nReally, this is sex, not gender\n\n\nage_child\nRIDAGEYR\nAge in years at screening\n\n\nrace_eth\nRIDRETH1\nRace/Hispanic origin (collapsed to 4 levels)\n\n\neduc_child\nDMDEDUC3\nEducation Level (for children ages 6-15). 0 = Kindergarten, 9 = Ninth grade or higher\n\n\nlanguage\nSIALANG\nLanguage in which the interview was conducted\n\n\nsampling_wt\nWTMEC\nFull-sample MEC exam weight (for inference)\n\n\nincome_pov\nINDFMPIR\nRatio of family income to poverty (ceiling is 5.0)\n\n\nage_adult\nDMDHRAGE\nAge of adult who brought child to interview\n\n\neduc_adult\nDMDHREDU\nEducation level of adult who brought child\n\n\n\n10.3.2 From the NNYFS Dietary Component\n\nFrom the Y_DR1TOT file, we have a number of variables related to the child’s diet, with the following summaries mostly describing consumption “yesterday” in a dietary recall questionnaire.\n\n\n\n\n\n\n\nIn nnyfs\n\nIn Y_DR1TOT\n\nDescription\n\n\n\nrespondent\nDR1MNRSP\nwho responded to interview (child, Mom, someone else)\n\n\nsalt_used\nDBQ095Z\nuses salt, lite salt or salt substitute at the table\n\n\nenergy\nDR1TKCAL\nenergy consumed (kcal)\n\n\nprotein\nDR1TPROT\nprotein consumed (g)\n\n\nsugar\nDR1TSUGR\ntotal sugar consumed (g)\n\n\nfat\nDR1TTFAT\ntotal fat consumed (g)\n\n\ndiet_yesterday\nDR1_300\ncompare food consumed yesterday to usual amount\n\n\nwater\nDR1_320Z\ntotal plain water drank (g)\n\n\n\n10.3.3 From the NNYFS Examination Component\n\nFrom the Y_BMX file of Body Measures:\n\n\nIn nnyfs\n\nIn Y_BMX\n\nDescription\n\n\n\nheight\nBMXHT\nstanding height (cm)\n\n\nweight\nBMXWT\nweight (kg)\n\n\nbmi\nBMXBMI\nbody mass index (\\(kg/m^2\\))\n\n\nbmi_cat\nBMDBMIC\nBMI category (4 levels)\n\n\narm_length\nBMXARML\nUpper arm length (cm)\n\n\nwaist\nBMXWAIST\nWaist circumference (cm)\n\n\narm_circ\nBMXARMC\nArm circumference (cm)\n\n\ncalf_circ\nBMXCALF\nMaximal calf circumference (cm)\n\n\ncalf_skinfold\nBMXCALFF\nCalf skinfold (mm)\n\n\ntriceps_skinfold\nBMXTRI\nTriceps skinfold (mm)\n\n\nsubscapular_skinfold\nBMXSUB\nSubscapular skinfold (mm)\n\n\n\nFrom the Y_PLX file of Plank test results:\n\n\nIn nnyfs\n\nIn Y_PLX\n\nDescription\n\n\nplank_time\nMPXPLANK\n# of seconds plank position is held\n\n\n10.3.4 From the NNYFS Questionnaire Component\n\nFrom the Y_PAQ file of Physical Activity questions:\n\n\n\n\n\n\n\nIn nnyfs\n\nIn Y_PAQ\n\nDescription\n\n\n\nactive_days\nPAQ706\nDays physically active (\\(\\geq 60\\) min.) in past week\n\n\ntv_hours\nPAQ710\nAverage hours watching TV/videos past 30d\n\n\ncomputer_hours\nPAQ715\nAverage hours on computer past 30d\n\n\nphysical_last_week\nPAQ722\nAny physical activity outside of school past week\n\n\nenjoy_recess\nPAQ750\nEnjoy participating in PE/recess\n\n\n\nFrom the Y_DBQ file of Diet Behavior and Nutrition questions:\n\n\nIn nnyfs\n\nIn Y_DBQ\n\nDescription\n\n\nmeals_out\nDBD895\n# meals not home-prepared in past 7 days\n\n\nFrom the Y_HIQ file of Health Insurance questions:\n\n\nIn nnyfs\n\nIn Y_HIQ\n\nDescription\n\n\n\ninsured\nHIQ011\nCovered by Health Insurance?\n\n\ninsurance\nHIQ031\nType of Health Insurance coverage\n\n\n\nFrom the Y_HUQ file of Access to Care questions:\n\n\nIn nnyfs\n\nIn Y_HUQ\n\nDescription\n\n\n\nphys_health\nHUQ010\nGeneral health condition (Excellent - Poor)\n\n\naccess_to_care\nHUQ030\nRoutine place to get care?\n\n\ncare_source\nHUQ040\nType of place most often goes to for care\n\n\n\nFrom the Y_MCQ file of Medical Conditions questions:\n\n\nIn nnyfs\n\nIn Y_MCQ\n\nDescription\n\n\n\nasthma_ever\nMCQ010\nEver told you have asthma?\n\n\nasthma_now\nMCQ035\nStill have asthma?\n\n\n\nFrom the Y_RXQ_RX file of Prescription Medication questions:\n\n\nIn nnyfs\n\nIn Y_RXQ_RX\n\nDescription\n\n\n\nmed_use\nRXDUSE\nTaken prescription medication in last month?\n\n\nmed_count\nRXDCOUNT\n# of prescription meds taken in past month",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#reading-in-the-data",
    "href": "10-nnyfs_foundations.html#reading-in-the-data",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.4 Reading in the Data",
    "text": "10.4 Reading in the Data\nNow, I’ll take a look at the nnyfs data, which I’ve made available in a comma-separated version (nnyfs.csv), if you prefer, as well as in an R data set (nnyfs.Rds) which loads a bit faster. After loading the file, let’s get a handle on its size and contents. In my R Project for these notes, the data are contained in a separate data subdirectory.\n\nnnyfs &lt;- readRDS(\"data/nnyfs.Rds\")\n\n## size of the tibble\ndim(nnyfs)\n\n[1] 1518   45\n\n\nThere are 1518 rows (subjects) and 45 columns (variables), by which I mean that there are 1518 kids in the nnyfs data frame, and we have 45 pieces of information on each subject. So, what do we have, exactly?\n\nnnyfs \n\n# A tibble: 1,518 × 45\n    SEQN sex    age_child race_eth    educ_child language sampling_wt income_pov\n   &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt; &lt;fct&gt;            &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n 1 71917 Female        15 3_Black No…          9 English       28299.       0.21\n 2 71918 Female         8 3_Black No…          2 English       15127.       5   \n 3 71919 Female        14 2_White No…          8 English       29977.       5   \n 4 71920 Female        15 2_White No…          8 English       80652.       0.87\n 5 71921 Male           3 2_White No…         NA English       55592.       4.34\n 6 71922 Male          12 1_Hispanic           6 English       27365.       5   \n 7 71923 Male          12 2_White No…          5 English       86673.       5   \n 8 71924 Female         8 4_Other Ra…          2 English       39549.       2.74\n 9 71925 Male           7 1_Hispanic           0 English       42333.       0.46\n10 71926 Male           8 3_Black No…          2 English       15307.       1.57\n# ℹ 1,508 more rows\n# ℹ 37 more variables: age_adult &lt;dbl&gt;, educ_adult &lt;fct&gt;, respondent &lt;fct&gt;,\n#   salt_used &lt;fct&gt;, energy &lt;dbl&gt;, protein &lt;dbl&gt;, sugar &lt;dbl&gt;, fat &lt;dbl&gt;,\n#   diet_yesterday &lt;fct&gt;, water &lt;dbl&gt;, plank_time &lt;dbl&gt;, height &lt;dbl&gt;,\n#   weight &lt;dbl&gt;, bmi &lt;dbl&gt;, bmi_cat &lt;fct&gt;, arm_length &lt;dbl&gt;, waist &lt;dbl&gt;,\n#   arm_circ &lt;dbl&gt;, calf_circ &lt;dbl&gt;, calf_skinfold &lt;dbl&gt;,\n#   triceps_skinfold &lt;dbl&gt;, subscapular_skinfold &lt;dbl&gt;, active_days &lt;dbl&gt;, …\n\n\nWe can learn something about the structure of the tibble from such functions as str or glimpse.\n\nstr(nnyfs)\n\ntibble [1,518 × 45] (S3: tbl_df/tbl/data.frame)\n $ SEQN                : num [1:1518] 71917 71918 71919 71920 71921 ...\n $ sex                 : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 2 2 2 1 2 2 ...\n $ age_child           : num [1:1518] 15 8 14 15 3 12 12 8 7 8 ...\n $ race_eth            : Factor w/ 4 levels \"1_Hispanic\",\"2_White Non-Hispanic\",..: 3 3 2 2 2 1 2 4 1 3 ...\n $ educ_child          : num [1:1518] 9 2 8 8 NA 6 5 2 0 2 ...\n $ language            : Factor w/ 2 levels \"English\",\"Spanish\": 1 1 1 1 1 1 1 1 1 1 ...\n $ sampling_wt         : num [1:1518] 28299 15127 29977 80652 55592 ...\n $ income_pov          : num [1:1518] 0.21 5 5 0.87 4.34 5 5 2.74 0.46 1.57 ...\n $ age_adult           : num [1:1518] 46 46 42 53 31 42 39 31 45 56 ...\n $ educ_adult          : Factor w/ 5 levels \"1_Less than 9th Grade\",..: 2 3 5 3 3 4 2 3 2 3 ...\n $ respondent          : Factor w/ 3 levels \"Child\",\"Mom\",..: 1 2 1 1 2 1 1 1 2 1 ...\n $ salt_used           : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 2 2 2 1 2 ...\n $ energy              : num [1:1518] 2844 1725 2304 1114 1655 ...\n $ protein             : num [1:1518] 169.1 55.2 199.3 14 50.6 ...\n $ sugar               : num [1:1518] 128.2 118.7 81.4 119.2 90.3 ...\n $ fat                 : num [1:1518] 127.9 63.7 86.1 36 53.3 ...\n $ diet_yesterday      : Factor w/ 3 levels \"1_Much more than usual\",..: 2 2 2 2 2 2 1 2 2 3 ...\n $ water               : num [1:1518] 607 178 503 859 148 ...\n $ plank_time          : num [1:1518] NA 45 121 45 11 107 127 44 184 58 ...\n $ height              : num [1:1518] NA 131.6 172 167.1 90.2 ...\n $ weight              : num [1:1518] NA 38.6 58.7 92.5 12.4 66.4 56.7 22.2 20.9 28.3 ...\n $ bmi                 : num [1:1518] NA 22.3 19.8 33.1 15.2 25.9 22.5 14.4 15.9 17 ...\n $ bmi_cat             : Factor w/ 4 levels \"1_Underweight\",..: NA 4 2 4 2 4 3 2 2 2 ...\n $ arm_length          : num [1:1518] NA 27.7 38.4 35.9 18.3 34.2 33 26.5 24.2 26 ...\n $ waist               : num [1:1518] NA 71.9 79.4 96.4 46.8 90 72.3 56.1 54.5 59.7 ...\n $ arm_circ            : num [1:1518] NA 25.4 26 37.9 15.1 29.5 27.9 17.6 17.7 19.9 ...\n $ calf_circ           : num [1:1518] NA 32.3 35.3 46.8 19.4 36.9 36.8 24 24.3 27.3 ...\n $ calf_skinfold       : num [1:1518] NA 22 18.4 NA 8.4 22 18.3 7 7.2 8.2 ...\n $ triceps_skinfold    : num [1:1518] NA 19.9 15 20.6 8.6 22.8 20.5 12.9 6.9 8.8 ...\n $ subscapular_skinfold: num [1:1518] NA 17.4 9.8 22.8 5.7 24.4 12.6 6.8 4.8 6.1 ...\n $ active_days         : num [1:1518] 3 5 3 3 7 2 5 3 7 7 ...\n $ tv_hours            : num [1:1518] 2 2 1 3 2 3 0 4 2 2 ...\n $ computer_hours      : num [1:1518] 1 2 3 3 0 1 0 3 1 1 ...\n $ physical_last_week  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 2 2 2 2 2 2 2 ...\n $ enjoy_recess        : Factor w/ 5 levels \"1_Strongly Agree\",..: 1 1 3 2 NA 2 2 NA 1 1 ...\n $ meals_out           : num [1:1518] 0 2 3 2 1 1 2 1 0 2 ...\n $ insured             : Factor w/ 2 levels \"Has Insurance\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ phys_health         : Factor w/ 5 levels \"1_Excellent\",..: 1 3 1 3 1 1 3 1 2 1 ...\n $ access_to_care      : Factor w/ 2 levels \"Has Usual Care Source\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ care_source         : Factor w/ 6 levels \"Clinic or Health Center\",..: 1 2 2 2 2 2 2 2 2 2 ...\n $ asthma_ever         : Factor w/ 2 levels \"History of Asthma\",..: 2 1 2 1 2 2 2 2 2 2 ...\n $ asthma_now          : Factor w/ 2 levels \"Asthma Now\",\"No Asthma Now\": 2 1 2 1 2 2 2 2 2 2 ...\n $ med_use             : Factor w/ 2 levels \"Had Medication\",..: 2 1 2 1 2 2 2 2 2 2 ...\n $ med_count           : num [1:1518] 0 1 0 2 0 0 0 0 0 0 ...\n $ insurance           : Factor w/ 10 levels \"Medicaid\",\"Medicare\",..: 8 8 5 8 5 5 5 5 8 1 ...\n\n\nThere are a lot of variables here. Let’s run through the first few in a little detail.\n\n10.4.1 SEQN\n\nThe first variable, SEQN is just a (numerical) identifying code attributable to a given subject of the survey. This is nominal data, which will be of little interest down the line. On some occasions, as in this case, the ID numbers are sequential, in the sense that subject 71919 was included in the data base after subject 71918, but this fact isn’t particularly interesting here, because the protocol remained unchanged throughout the study.\n\n10.4.2 sex\n\nThe second variable, sex, is listed as a factor variable (R uses factor and character to refer to categorical, especially non-numeric information). Here, as we can see below, we have two levels, Female and Male.\n\nnnyfs |&gt;\n  tabyl(sex) |&gt;\n  adorn_totals() |&gt;\n  adorn_pct_formatting()\n\n    sex    n percent\n Female  760   50.1%\n   Male  758   49.9%\n  Total 1518  100.0%\n\n\n\n10.4.3 age_child\n\nThe third variable, age_child, is the age of the child at the time of their screening to be in the study, measured in years. Note that age is a continuous concept, but the measure used here (number of full years alive) is a common discrete approach to measurement. Age, of course, has a meaningful zero point, so this can be thought of as a ratio variable; a child who is 6 is half as old as one who is 12. We can tabulate the observed values, since there are only a dozen or so.\n\nnnyfs |&gt; \n  tabyl(age_child) |&gt;\n  adorn_pct_formatting()\n\n age_child   n percent\n         3 110    7.2%\n         4 112    7.4%\n         5 114    7.5%\n         6 129    8.5%\n         7 123    8.1%\n         8 112    7.4%\n         9  99    6.5%\n        10 124    8.2%\n        11 111    7.3%\n        12 137    9.0%\n        13 119    7.8%\n        14 130    8.6%\n        15  98    6.5%\n\n\nAt the time of initial screening, these children should have been between 3 and 15 years of age, so things look reasonable. Since this is a meaningful quantitative variable, we may be interested in a more descriptive summary.\n\nnnyfs |&gt; \n  select(age_child) |&gt; \n  summary()\n\n   age_child     \n Min.   : 3.000  \n 1st Qu.: 6.000  \n Median : 9.000  \n Mean   : 9.033  \n 3rd Qu.:12.000  \n Max.   :15.000  \n\n\nThese six numbers provide a nice, if incomplete, look at the ages.\n\n\nMin. = the minimum, or youngest age at the examination was 3 years old.\n\n1st Qu. = the first quartile (25th percentile) of the ages was 6. This means that 25 percent of the subjects were age 6 or less.\n\nMedian = the second quartile (50th percentile) of the ages was 9. This is often used to describe the center of the data. Half of the subjects were age 9 or less.\n\n3rd Qu. = the third quartile (75th percentile) of the ages was 12\n\nMax. = the maximum, or oldest age at the examination was 15 years.\n\nWe could get the standard deviation and a count of missing and non-missing observations with favstats from the mosaic package, among many other options (see Chapter 3 and @#sec-summ_quant for examples.)\n\nmosaic::favstats(~ age_child, data = nnyfs) |&gt;\n  kbl(digits = 1)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n3\n6\n9\n12\n15\n9\n3.7\n1518\n0\n\n\n\n\n\n10.4.4 race_eth\n\nThe fourth variable in the data set is race_eth, which is a multi-categorical variable describing the child’s race and ethnicity.\n\nnnyfs |&gt; tabyl(race_eth) |&gt; \n  adorn_pct_formatting() |&gt;\n  kbl()\n\n\n\nrace_eth\nn\npercent\n\n\n\n1_Hispanic\n450\n29.6%\n\n\n2_White Non-Hispanic\n610\n40.2%\n\n\n3_Black Non-Hispanic\n338\n22.3%\n\n\n4_Other Race/Ethnicity\n120\n7.9%\n\n\n\n\n\nAnd now, we get the idea of looking at whether our numerical summaries of the children’s ages varies by their race/ethnicity…\n\nmosaic::favstats(age_child ~ race_eth, data = nnyfs)\n\n                race_eth min   Q1 median Q3 max     mean       sd   n missing\n1             1_Hispanic   3 5.25    9.0 12  15 8.793333 3.733846 450       0\n2   2_White Non-Hispanic   3 6.00    9.0 12  15 9.137705 3.804421 610       0\n3   3_Black Non-Hispanic   3 6.00    9.0 12  15 9.038462 3.576423 338       0\n4 4_Other Race/Ethnicity   3 7.00    9.5 12  15 9.383333 3.427970 120       0\n\n\n\n10.4.5 income_pov\n\nSkipping down a bit, let’s look at the family income as a multiple of the poverty level. Here’s the summary.\n\nnnyfs |&gt; \n  select(income_pov) |&gt; \n  summary()\n\n   income_pov   \n Min.   :0.000  \n 1st Qu.:0.870  \n Median :1.740  \n Mean   :2.242  \n 3rd Qu.:3.520  \n Max.   :5.000  \n NA's   :89     \n\n\nWe see there is some missing data here. Let’s ignore that for the moment and concentrate on interpreting the results for the children with actual data. We should start with a picture.\n\nggplot(nnyfs, aes(x = income_pov)) +\n  geom_histogram(bins = 30, fill = \"white\", col = \"blue\")\n\nWarning: Removed 89 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\nThe histogram shows us that the values are truncated at 5, so that children whose actual family income is above 5 times the poverty line are listed as 5. We also see a message reminding us that some of the data are missing for this variable.\nIs there a relationship between income_pov and race_eth in these data?\n\nmosaic::favstats(income_pov ~ race_eth, data = nnyfs) |&gt;\n  kbl(digits = 1) |&gt;\n  kable_styling(full_width = FALSE)\n\n\n\nrace_eth\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n1_Hispanic\n0\n0.6\n1.0\n1.7\n5\n1.3\n1.1\n409\n41\n\n\n2_White Non-Hispanic\n0\n1.5\n3.0\n4.5\n5\n2.9\n1.6\n588\n22\n\n\n3_Black Non-Hispanic\n0\n0.8\n1.6\n2.8\n5\n2.0\n1.5\n328\n10\n\n\n4_Other Race/Ethnicity\n0\n1.2\n2.7\n4.6\n5\n2.8\n1.7\n104\n16\n\n\n\n\n\nThis deserves a picture. Let’s try a boxplot.\n\nggplot(nnyfs, aes(x = race_eth, y = income_pov)) +\n  geom_boxplot()\n\nWarning: Removed 89 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\nWe’re reminded here that several of our subjects are not included in this plot, since they have some missing information.\n\n10.4.6 bmi\n\nMoving into the body measurement data, bmi is the body-mass index of the child. The BMI is a person’s weight in kilograms divided by his or her height in meters squared. Symbolically, BMI = weight in kg / (height in m)2. This is a continuous concept, measured to as many decimal places as you like, and it has a meaningful zero point, so it’s a ratio variable.\n\nnnyfs |&gt; \n  select(bmi) |&gt; \n  summary()\n\n      bmi       \n Min.   :11.90  \n 1st Qu.:15.90  \n Median :18.10  \n Mean   :19.63  \n 3rd Qu.:21.90  \n Max.   :48.30  \n NA's   :4      \n\n\nWhy would a table of these BMI values not be a great idea, for these data? A hint is that R represents this variable as num or numeric in its depiction of the data structure, and this implies that R has some decimal values stored. Here, I’ll use the head() function and the tail() function to show the first few and the last few values of what would prove to be a very long table of bmi values.\n\nnnyfs |&gt; \n  tabyl(bmi) |&gt; \n  adorn_pct_formatting() |&gt; \n  head()\n\n  bmi n percent valid_percent\n 11.9 1    0.1%          0.1%\n 12.6 1    0.1%          0.1%\n 12.7 1    0.1%          0.1%\n 12.9 1    0.1%          0.1%\n 13.0 2    0.1%          0.1%\n 13.1 1    0.1%          0.1%\n\n\n\nnnyfs |&gt; \n  tabyl(bmi) |&gt; \n  adorn_pct_formatting() |&gt; \n  tail()\n\n  bmi n percent valid_percent\n 42.8 1    0.1%          0.1%\n 43.0 1    0.1%          0.1%\n 46.9 1    0.1%          0.1%\n 48.2 1    0.1%          0.1%\n 48.3 1    0.1%          0.1%\n   NA 4    0.3%             -\n\n\n\n10.4.7 bmi_cat\n\nNext I’ll look at the bmi_cat information. This is a four-category ordinal variable, which divides the sample according to BMI into four groups. The BMI categories use sex-specific 2000 BMI-for-age (in months) growth charts prepared by the Centers for Disease Control for the US. We can get the breakdown from a table of the variable’s values.\n\nnnyfs |&gt; \n  tabyl(bmi_cat) |&gt; \n  adorn_pct_formatting()\n\n       bmi_cat   n percent valid_percent\n 1_Underweight  41    2.7%          2.7%\n      2_Normal 920   60.6%         60.8%\n  3_Overweight 258   17.0%         17.0%\n       4_Obese 295   19.4%         19.5%\n          &lt;NA&gt;   4    0.3%             -\n\n\nIn terms of percentiles by age and sex from the growth charts, the meanings of the categories are:\n\nUnderweight (BMI &lt; 5th percentile)\nNormal weight (BMI 5th to &lt; 85th percentile)\nOverweight (BMI 85th to &lt; 95th percentile)\nObese (BMI \\(\\geq\\) 95th percentile)\n\nNote how I’ve used labels in the bmi_cat variable that include a number at the start so that the table results are sorted in a rational way. R sorts tables alphabetically, in general. We’ll use the forcats package to work with categorical variables that we store as factors eventually, but for now, we’ll keep things relatively simple.\nNote that the bmi_cat data don’t completely separate out the raw bmi data, because the calculation of percentiles requires different tables for each combination of age and sex.\n\nmosaic::favstats(bmi ~ bmi_cat, data = nnyfs) |&gt;\n  kbl(digits = 1) |&gt;\n  kable_styling(full_width = FALSE)\n\n\n\nbmi_cat\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n1_Underweight\n11.9\n13.4\n13.7\n15.0\n16.5\n14.1\n1.1\n41\n0\n\n\n2_Normal\n13.5\n15.4\n16.5\n18.7\n24.0\n17.2\n2.3\n920\n0\n\n\n3_Overweight\n16.9\n18.3\n21.4\n23.4\n27.9\n21.2\n2.9\n258\n0\n\n\n4_Obese\n17.9\n22.3\n26.2\n30.2\n48.3\n26.7\n5.7\n295\n0\n\n\n\n\n\n\n10.4.8 waist\n\nLet’s also look briefly at waist, which is the circumference of the child’s waist, in centimeters. Again, this is a numeric variable, so perhaps we’ll stick to the simple summary, rather than obtaining a table of observed values.\n\nmosaic::favstats(~ waist, data = nnyfs) \n\n  min   Q1 median   Q3   max     mean       sd    n missing\n 42.5 55.6   64.8 76.6 144.7 67.70536 15.19809 1512       6\n\n\nHere’s a histogram of the waist circumference data.\n\nggplot(nnyfs, aes(x = waist)) +\n  geom_histogram(bins = 25, fill = \"tomato\", color = \"cyan\")\n\nWarning: Removed 6 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n10.4.9 triceps_skinfold\n\nThe last variable I’ll look at for now is triceps_skinfold, which is measured in millimeters. This is one of several common locations used for the assessment of body fat using skinfold calipers, and is a frequent part of growth assessments in children. Again, this is a numeric variable according to R.\n\nmosaic::favstats(~ triceps_skinfold, data = nnyfs)\n\n min  Q1 median Q3  max     mean       sd    n missing\n   4 9.1   12.4 18 38.8 14.35725 6.758825 1497      21\n\n\nAnd here’s a histogram of the triceps skinfold data, with the fill and color flipped from what we saw in the plot of the waist circumference data a moment ago.\n\nggplot(nnyfs, aes(x = triceps_skinfold)) +\n  geom_histogram(bins = 25, fill = \"cyan\", color = \"tomato\")\n\nWarning: Removed 21 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\nOK. We’ve seen a few variables, and we’ll move on now to look more seriously at the data.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#additional-numeric-summaries",
    "href": "10-nnyfs_foundations.html#additional-numeric-summaries",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.5 Additional Numeric Summaries",
    "text": "10.5 Additional Numeric Summaries\n\n10.5.1 The Five Number Summary, Quantiles and IQR\nThe five number summary is most famous when used to form a box plot - it’s the minimum, 25th percentile, median, 75th percentile and maximum. For numerical and integer variables, the summary function produces the five number summary, plus the mean, and a count of any missing values (NA’s).\n\nnnyfs |&gt; \n  select(waist, energy, sugar) |&gt;\n  summary()\n\n     waist            energy         sugar       \n Min.   : 42.50   Min.   : 257   Min.   :  1.00  \n 1st Qu.: 55.60   1st Qu.:1368   1st Qu.: 82.66  \n Median : 64.80   Median :1794   Median :116.92  \n Mean   : 67.71   Mean   :1877   Mean   :124.32  \n 3rd Qu.: 76.60   3rd Qu.:2306   3rd Qu.:157.05  \n Max.   :144.70   Max.   :5265   Max.   :405.49  \n NA's   :6                                       \n\n\nAs an alternative, we can use the $ notation to indicate the variable we wish to study inside a data set, and we can use the fivenum function to get the five numbers used in developing a box plot. We’ll focus for a little while on the number of kilocalories consumed by each child, according to the dietary recall questionnaire. That’s the energy variable.\n\nfivenum(nnyfs$energy)\n\n[1]  257.0 1367.0 1794.5 2306.0 5265.0\n\n\n\nAs mentioned in @ref(rangeandiqr), the inter-quartile range, or IQR, is sometimes used as a competitor for the standard deviation. It’s the difference between the 75th percentile and the 25th percentile. The 25th percentile, median, and 75th percentile are referred to as the quartiles of the data set, because, together, they split the data into quarters.\n\n\nIQR(nnyfs$energy)\n\n[1] 938.5\n\n\nWe can obtain quantiles (percentiles) as we like - here, I’m asking for the 1st and 99th:\n\nquantile(nnyfs$energy, probs=c(0.01, 0.99))\n\n     1%     99% \n 566.85 4051.75",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#additional-summaries-from-favstats",
    "href": "10-nnyfs_foundations.html#additional-summaries-from-favstats",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.6 Additional Summaries from favstats\n",
    "text": "10.6 Additional Summaries from favstats\n\nIf we’re focusing on a single variable, the favstats function in the mosaic package can be very helpful. Rather than calling up the entire mosaic library here, I’ll just specify the function within the library.\n\nmosaic::favstats(~ energy, data = nnyfs)\n\n min     Q1 median   Q3  max     mean       sd    n missing\n 257 1367.5 1794.5 2306 5265 1877.157 722.3537 1518       0\n\n\nThis adds three useful results to the base summary - the standard deviation, the sample size and the number of missing observations.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#the-histogram",
    "href": "10-nnyfs_foundations.html#the-histogram",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.7 The Histogram",
    "text": "10.7 The Histogram\nObtaining a basic histogram of, for example, the energy (kilocalories consumed) in the nnyfs data is pretty straightforward.\n\nggplot(data = nnyfs, aes(x = energy)) +\n    geom_histogram(binwidth = 100, col = \"white\")\n\n\n\n\n\n\n\n\n10.7.1 Freedman-Diaconis Rule to select bin width\nIf we like, we can suggest a particular number of cells for the histogram, instead of accepting the defaults. In this case, we have \\(n\\) = 1518 observations. The Freedman-Diaconis rule can be helpful here. That rule suggests that we set the bin-width to\n\\[\nh = \\frac{2*IQR}{n^{1/3}}\n\\]\nso that the number of bins is equal to the range of the data set (maximum - minimum) divided by \\(h\\).\nFor the energy data in the nnyfs tibble, we have\n\nIQR of 938.5, \\(n\\) = 1518 and range = 5008\nThus, by the Freedman-Diaconis rule, the optimal binwidth \\(h\\) is 163.3203676, or, realistically, 163.\nAnd so the number of bins would be 30.6636586, or, realistically 31.\n\nHere, we’ll draw the graph again, using the Freedman-Diaconis rule to identify the number of bins, and also play around a bit with the fill and color of the bars.\n\nbw &lt;- 2 * IQR(nnyfs$energy) / length(nnyfs$energy)^(1/3)\nggplot(data = nnyfs, aes(x = energy)) +\n    geom_histogram(binwidth=bw, color = \"white\", fill = \"black\")\n\n\n\n\n\n\n\nThis is a nice start, but it is by no means a finished graph.\nLet’s improve the axis labels, add a title, and fill in the bars with a distinctive blue and use a black outline around each bar. I’ll just use 25 bars, because I like how that looks in this case, and optimizing the number of bins is rarely important.\n\nggplot(data = nnyfs, aes(x = energy)) +\n    geom_histogram(bins=25, color = \"black\", fill = \"dodgerblue\") + \n    labs(title = \"Histogram of Body-Mass Index Results in the nnyfs data\",\n         x = \"Energy Consumed (kcal)\", y = \"# of Subjects\")\n\n\n\n\n\n\n\n\n10.7.2 A Note on Colors\nThe simplest way to specify a color is with its name, enclosed in parentheses. My favorite list of R colors is http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf. In a pinch, you can usually find it by googling Colors in R. You can also type colors() in the R console to obtain a list of the names of the same 657 colors.\nWhen using colors to make comparisons, you may be interested in using a scale that has some nice properties. The viridis package vignette describes four color scales (viridis, magma, plasma and inferno) that are designed to be colorful, robust to colorblindness and gray scale printing, and perceptually uniform, which means (as the package authors describe it) that values close to each other have similar-appearing colors and values far away from each other have more different-appearing colors, consistently across the range of values. We can apply these colors with special functions within ggplot.\nHere’s a comparison of several histograms, looking at energy consumed as a function of whether yesterday was typical in terms of food consumption.\n\nggplot(data = nnyfs, aes(x = energy, fill = diet_yesterday)) +\n  geom_histogram(bins = 20, col = \"white\") +\n  scale_fill_viridis_d() +\n  facet_wrap(~ diet_yesterday)\n\n\n\n\n\n\n\nWe don’t really need the legend here, and perhaps we should restrict the plot to participants who responded to the diet_yesterday question, and put in a title and better axis labels?\n\nnnyfs_temp &lt;- nnyfs |&gt; \n  filter(!is.na(energy), !is.na(diet_yesterday))\n\nggplot(data = nnyfs_temp, aes(x = energy, fill = diet_yesterday)) +\n  geom_histogram(bins = 20, col = \"white\") +\n  scale_fill_viridis_d() +\n  guides(fill = \"none\") +\n  facet_wrap(~ diet_yesterday) +\n  labs(x = \"Energy consumed, in kcal\",\n       title = \"Energy Consumption and How Typical Was Yesterday's Eating\",\n       subtitle = \"NHANES National Youth Fitness Survey, no survey weighting\")",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#the-frequency-polygon-with-rug-plot",
    "href": "10-nnyfs_foundations.html#the-frequency-polygon-with-rug-plot",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.8 The Frequency Polygon with Rug Plot",
    "text": "10.8 The Frequency Polygon with Rug Plot\nAs we’ve seen, we can also plot the distribution of a single continuous variable using the freqpoly geom. We can also add a rug plot, which places a small vertical line on the horizontal axis everywhere where an observation appears in the data.\n\nggplot(data = nnyfs, aes(x = energy)) +\n    geom_freqpoly(binwidth = 150, color = \"dodgerblue\") + \n    geom_rug(color = \"red\") +\n    labs(title = \"Frequency Polygon of nnyfs Energy data\",\n         x = \"Energy (kcal)\", y = \"# of Patients\")",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#plotting-the-probability-density-function",
    "href": "10-nnyfs_foundations.html#plotting-the-probability-density-function",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.9 Plotting the Probability Density Function",
    "text": "10.9 Plotting the Probability Density Function\nWe can also produce a density function, which has the effect of smoothing out the bumps in a histogram or frequency polygon, while also changing what is plotted on the y-axis.\n\nggplot(data = nnyfs, aes(x = energy)) +\n    geom_density(kernel = \"gaussian\", color = \"dodgerblue\") + \n    labs(title = \"Density of nnyfs Energy data\",\n         x = \"Energy (kcal)\", y = \"Probability Density function\")\n\n\n\n\n\n\n\nSo, what’s a density function?\n\nA probability density function is a function of a continuous variable, x, that represents the probability of x falling within a given range. Specifically, the integral over the interval (a,b) of the density function gives the probability that the value of x is within (a,b).\nIf you’re interested in exploring more on the notion of density functions for continuous (and discrete) random variables, some nice elementary material is available at Khan Academy.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#the-boxplot",
    "href": "10-nnyfs_foundations.html#the-boxplot",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.10 The Boxplot",
    "text": "10.10 The Boxplot\nSometimes, it’s helpful to picture the five-number summary of the data in such a way as to get a general sense of the distribution. One approach is a boxplot, sometimes called a box-and-whisker plot.\n\n10.10.1 Drawing a Boxplot for One Variable in ggplot2\n\nThe ggplot2 library easily handles comparison boxplots for multiple distributions, as we’ll see in a moment. However, building a boxplot for a single distribution requires a little trickiness.\n\nggplot(nnyfs, aes(x = 1, y = energy)) + \n    geom_boxplot(fill = \"deepskyblue\") + \n    coord_flip() + \n    labs(title = \"Boxplot of Energy for kids in the NNYFS\",\n         y = \"Energy (kcal)\",\n         x = \"\") +\n    theme(axis.text.y = element_blank(),\n          axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n10.10.2 About the Boxplot\nThe boxplot is another John Tukey invention.\n\nR draws the box (here in yellow) so that its edges of the box fall at the 25th and 75th percentiles of the data, and the thick line inside the box falls at the median (50th percentile).\nThe whiskers then extend out to the largest and smallest values that are not classified by the plot as candidate outliers.\nAn outlier is an unusual point, far from the center of a distribution.\nNote that I’ve used the horizontal option to show this boxplot in this direction. Most comparison boxplots, as we’ll see below, are oriented vertically.\n\nThe boxplot’s whiskers that are drawn from the first and third quartiles (i.e. the 25th and 75th percentiles) out to the most extreme points in the data that do not meet the standard of ``candidate outliers.’’ An outlier is simply a point that is far away from the center of the data - which may be due to any number of reasons, and generally indicates a need for further investigation.\nMost software, including R, uses a standard proposed by Tukey which describes a ``candidate outlier’’ as any point above the upper fence or below the lower fence. The definitions of the fences are based on the inter-quartile range (IQR).\nIf IQR = 75th percentile - 25th percentile, then the upper fence is 75th percentile + 1.5 IQR, and the lower fence is 25th percentile - 1.5 IQR.\nSo for these energy data,\n\nthe upper fence is located at 2306 + 1.5(938.5) = 3713.75\nthe lower fence is located at 1367 - 1.5(938.5) = -40.75\n\nIn this case, we see no points identified as outliers in the low part of the distribution, but quite a few identified that way on the high side. This tends to identify about 5% of the data as a candidate outlier, if the data follow a Normal distribution.\n\nThis plot is indicating clearly that there is some asymmetry (skew) in the data, specifically right skew.\nThe standard R uses is to indicate as outliers any points that are more than 1.5 inter-quartile ranges away from the edges of the box.\n\nThe horizontal orientation I’ve chosen here clarifies the relationship of direction of skew to the plot. A plot like this, with multiple outliers on the right side is indicative of a long right tail in the distribution, and hence, positive or right skew - with the mean being larger than the median. Other indications of skew include having one side of the box being substantially wider than the other, or one side of the whiskers being substantially longer than the other. More on skew later.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#a-simple-comparison-boxplot",
    "href": "10-nnyfs_foundations.html#a-simple-comparison-boxplot",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.11 A Simple Comparison Boxplot",
    "text": "10.11 A Simple Comparison Boxplot\nBoxplots are most often used for comparison, as we’ve seen (for example) in Chapter 3 and @#sec-summ_quant. We can build boxplots using ggplot2, as well, and we’ll discuss that in detail later. For now, here’s a boxplot built to compare the energy results by the subject’s race/ethnicity.\n\nggplot(nnyfs, aes(x = factor(race_eth), y = energy, fill=factor(race_eth))) +\n  geom_boxplot() + \n  guides(fill = \"none\") +\n  labs(y = \"Energy consumed (kcal)\", x = \"Race/Ethnicity\")\n\n\n\n\n\n\n\nLet’s look at the comparison of observed energy levels across the five categories in our phys_health variable, now making use of the viridis color scheme.\n\nggplot(nnyfs, aes(x = factor(phys_health), y = energy, fill = factor(phys_health))) +\n  geom_boxplot() + \n  scale_fill_viridis_d() + \n  labs(title = \"Energy by Self-Reported Physical Health, in nnyfs data\")\n\n\n\n\n\n\n\nAs a graph, that’s not bad, but what if we want to improve it further?\nLet’s turn the boxes in the horizontal direction, and get rid of the perhaps unnecessary phys_health labels.\n\nggplot(nnyfs, aes(x = factor(phys_health), y = energy, fill = factor(phys_health))) +\n    geom_boxplot() + \n    scale_fill_viridis_d() + \n    coord_flip() + \n    guides(fill = \"none\") +\n    labs(title = \"Energy Consumed by Self-Reported Physical Health\", \n         subtitle = \"NHANES National Youth Fitness Survey, unweighted\", \n         x = \"\")",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#using-describe-in-the-psych-library",
    "href": "10-nnyfs_foundations.html#using-describe-in-the-psych-library",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.12 Using describe in the psych library",
    "text": "10.12 Using describe in the psych library\nFor additional numerical summaries, one option would be to consider using the describe function from the psych library.\n\npsych::describe(nnyfs$energy)\n\n   vars    n    mean     sd median trimmed    mad min  max range skew kurtosis\nX1    1 1518 1877.16 722.35 1794.5  1827.1 678.29 257 5265  5008  0.8     1.13\n      se\nX1 18.54\n\n\nThis package provides, in order, the following…\n\n\nn = the sample size\n\nmean = the sample mean\n\nsd = the sample standard deviation\n\nmedian = the median, or 50th percentile\n\ntrimmed = mean of the middle 80% of the data\n\nmad = median absolute deviation\n\nmin = minimum value in the sample\n\nmax = maximum value in the sample\n\nrange = max - min\n\nskew = skewness measure, described below (indicates degree of asymmetry)\n\nkurtosis = kurtosis measure, described below (indicates heaviness of tails, degree of outlier-proneness)\n\nse = standard error of the sample mean = sd / square root of sample size, useful in inference\n\n\n10.12.1 The Trimmed Mean\nThe trimmed mean trim value in R indicates proportion of observations to be trimmed from each end of the outcome distribution before the mean is calculated. The trimmed value provided by the psych::describe package describes what this particular package calls a 20% trimmed mean (bottom and top 10% of energy values are removed before taking the mean - it’s the mean of the middle 80% of the data.) I might call that a 80% trimmed mean sometimes, but that’s just me.\n\nmean(nnyfs$energy, trim=.1) \n\n[1] 1827.1\n\n\n\n10.12.2 The Median Absolute Deviation\nAn alternative to the IQR that is fancier, and a bit more robust, is the median absolute deviation, which, in large sample sizes, for data that follow a Normal distribution, will be (in expectation) equal to the standard deviation. The MAD is the median of the absolute deviations from the median, multiplied by a constant (1.4826) to yield asymptotically normal consistency.\n\nmad(nnyfs$energy)\n\n[1] 678.2895",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#sec-skew",
    "href": "10-nnyfs_foundations.html#sec-skew",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.13 Assessing Skew",
    "text": "10.13 Assessing Skew\nA relatively common idea is to assess skewness, several measures of which are available. Many models assume a Normal distribution, where, among other things, the data are symmetric around the mean.\nSkewness measures asymmetry in the distribution, where left skew (mean &lt; median) is indicated by negative skewness values, while right skew (mean &gt; median) is indicated by positive values. The skew value will be near zero for data that follow a symmetric distribution.\n\n10.13.1 Non-parametric Skewness\nA simpler measure of skew, sometimes called the nonparametric skew and closely related to Pearson’s notion of median skewness, falls between -1 and +1 for any distribution. It is just the difference between the mean and the median, divided by the standard deviation.\n\nValues greater than +0.2 are sometimes taken to indicate fairly substantial right skew, while values below -0.2 indicate fairly substantial left skew.\n\n\n(mean(nnyfs$energy) - median(nnyfs$energy))/sd(nnyfs$energy)\n\n[1] 0.114427\n\n\nThe Wikipedia page on skewness, from which some of this material is derived, provides definitions for several other skewness measures.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#assessing-kurtosis-heavy-tailedness",
    "href": "10-nnyfs_foundations.html#assessing-kurtosis-heavy-tailedness",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.14 Assessing Kurtosis (Heavy-Tailedness)",
    "text": "10.14 Assessing Kurtosis (Heavy-Tailedness)\nAnother measure of a distribution’s shape that can be found in the psych library is the kurtosis. Kurtosis is an indicator of whether the distribution is heavy-tailed or light-tailed as compared to a Normal distribution. Positive kurtosis means more of the variance is due to outliers - unusual points far away from the mean relative to what we might expect from a Normally distributed data set with the same standard deviation.\n\nA Normal distribution will have a kurtosis value near 0, a distribution with similar tail behavior to what we would expect from a Normal is said to be mesokurtic\n\nHigher kurtosis values (meaningfully higher than 0) indicate that, as compared to a Normal distribution, the observed variance is more the result of extreme outliers (i.e. heavy tails) as opposed to being the result of more modest sized deviations from the mean. These heavy-tailed, or outlier prone, distributions are sometimes called leptokurtic.\nKurtosis values meaningfully lower than 0 indicate light-tailed data, with fewer outliers than we’d expect in a Normal distribution. Such distributions are sometimes referred to as platykurtic, and include distributions without outliers, like the Uniform distribution.\n\nHere’s a table:\n\n\n\n\n\n\n\nFewer outliers than a Normal\nApproximately Normal\nMore outliers than a Normal\n\n\n\nLight-tailed\n“Normalish”\nHeavy-tailed\n\n\n\nplatykurtic (kurtosis &lt; 0)\n\nmesokurtic (kurtosis = 0)\n\nleptokurtic (kurtosis &gt; 0)\n\n\n\n\npsych::kurtosi(nnyfs$energy)\n\n[1] 1.130539\n\n\nNote that the kurtosi() function is strangely named, and is part of the psych package.\n\n10.14.1 The Standard Error of the Sample Mean\nThe standard error of the sample mean, which is the standard deviation divided by the square root of the sample size:\n\nsd(nnyfs$energy)/sqrt(length(nnyfs$energy))\n\n[1] 18.54018",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#the-describe-function-in-the-hmisc-package",
    "href": "10-nnyfs_foundations.html#the-describe-function-in-the-hmisc-package",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.15 The describe function in the Hmisc package",
    "text": "10.15 The describe function in the Hmisc package\nThe Hmisc package has lots of useful functions. It’s named for its main developer, Frank Harrell. The describe function in Hmisc knows enough to separate numerical from categorical variables, and give you separate (and detailed) summaries for each.\n\nFor a categorical variable, it provides counts of total observations (n), the number of missing values, and the number of unique categories, along with counts and percentages falling in each category.\nFor a numerical variable, it provides:\ncounts of total observations (n), the number of missing values, and the number of unique values\nan Info value for the data, which indicates how continuous the variable is (a score of 1 is generally indicative of a completely continuous variable with no ties, while scores near 0 indicate lots of ties, and very few unique values)\nthe sample Mean\nGini’s mean difference, which is a robust measure of spread, with larger values indicating greater dispersion in the data. It is defined as the mean absolute difference between any pairs of observations.\nmany sample percentiles (quantiles) of the data, specifically (5, 10, 25, 50, 75, 90, 95, 99)\neither a complete table of all observed values, with counts and percentages (if there are a modest number of unique values), or\na table of the five smallest and five largest values in the data set, which is useful for range checking\n\n\nnnyfs |&gt; \n  select(waist, energy, bmi) |&gt;\n  Hmisc::describe()\n\nselect(nnyfs, waist, energy, bmi) \n\n 3  Variables      1518  Observations\n--------------------------------------------------------------------------------\nwaist \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    1512        6      510        1    67.71     16.6    49.40    51.40 \n     .25      .50      .75      .90      .95 \n   55.60    64.80    76.60    88.70    96.84 \n\nlowest : 42.5  43.4  44.1  44.4  44.5 , highest: 125.8 126   127   132.3 144.7\n--------------------------------------------------------------------------------\nenergy \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    1518        0     1137        1     1877    796.1      849     1047 \n     .25      .50      .75      .90      .95 \n    1368     1794     2306     2795     3195 \n\nlowest :  257  260  326  349  392, highest: 4382 4529 5085 5215 5265\n--------------------------------------------------------------------------------\nbmi \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    1514        4      225        1    19.63    5.269    14.30    14.90 \n     .25      .50      .75      .90      .95 \n   15.90    18.10    21.90    26.27    30.20 \n\nlowest : 11.9 12.6 12.7 12.9 13  , highest: 42.8 43   46.9 48.2 48.3\n--------------------------------------------------------------------------------\n\n\nMore on the Info value in Hmisc::describe is available here",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#summarizing-data-within-subgroups",
    "href": "10-nnyfs_foundations.html#summarizing-data-within-subgroups",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.16 Summarizing data within subgroups",
    "text": "10.16 Summarizing data within subgroups\nSuppose we want to understand how the subjects whose diet involved consuming much more than usual yesterday compare to those who consumer their usual amount, or to those who consumed much less than usual, in terms of the energy they consumed, as well as the protein. We might start by looking at the medians and means.\n\nnnyfs |&gt;\n    group_by(diet_yesterday) |&gt;\n    select(diet_yesterday, energy, protein) |&gt;\n    summarise_all(list(median = median, mean = mean))\n\n# A tibble: 4 × 5\n  diet_yesterday         energy_median protein_median energy_mean protein_mean\n  &lt;fct&gt;                          &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n1 1_Much more than usual          2098           69.4       2150.         75.1\n2 2_Usual                         1794           61.3       1858.         67.0\n3 3_Much less than usual          1643           53.9       1779.         60.1\n4 &lt;NA&gt;                            4348          155.        4348         155. \n\n\nPerhaps we should restrict ourselves to the people who were not missing the diet_yesterday category, and look now at their sugar and water consumption.\n\nnnyfs |&gt;\n    filter(complete.cases(diet_yesterday)) |&gt;\n    group_by(diet_yesterday) |&gt;\n    select(diet_yesterday, energy, protein, sugar, water) |&gt;\n    summarise_all(list(median))\n\n# A tibble: 3 × 5\n  diet_yesterday         energy protein sugar water\n  &lt;fct&gt;                   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1_Much more than usual   2098    69.4  137.  500 \n2 2_Usual                  1794    61.3  114.  385.\n3 3_Much less than usual   1643    53.9  115.  311.\n\n\nIt looks like the children in the “Much more than usual” category consumed more energy, protein, sugar and water than the children in the other two categories. Let’s draw a picture of this.\n\ntemp_dat &lt;- nnyfs |&gt;\n    filter(complete.cases(diet_yesterday)) |&gt;\n    mutate(diet_yesterday = fct_recode(diet_yesterday,\n        \"Much more\" = \"1_Much more than usual\",\n        \"Usual diet\" = \"2_Usual\",\n        \"Much less\" = \"3_Much less than usual\"))\n\np1 &lt;- ggplot(temp_dat, aes(x = diet_yesterday, y = energy)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = diet_yesterday), width = 0.2) +\n    theme_light() + \n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Energy Comparison\")\n\np2 &lt;- ggplot(temp_dat, aes(x = diet_yesterday, y = protein)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = diet_yesterday), width = 0.2) +\n    theme_light() + \n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Protein Comparison\")\n\np3 &lt;- ggplot(temp_dat, aes(x = diet_yesterday, y = sugar)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = diet_yesterday), width = 0.2) +\n    theme_light() + \n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Sugar Comparison\")\n\np4 &lt;- ggplot(temp_dat, aes(x = diet_yesterday, y = water)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = diet_yesterday), width = 0.2) +\n    theme_light() + \n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Water Comparison\")\n\np1 + p2 + p3 + p4\n\n\n\n\n\n\n\nWe can see that there is considerable overlap in these distributions, regardless of what we’re measuring.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#another-example",
    "href": "10-nnyfs_foundations.html#another-example",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.17 Another Example",
    "text": "10.17 Another Example\nSuppose now that we ask a different question. Do kids in larger categories of BMI have larger waist circumferences?\n\nnnyfs |&gt;\n    group_by(bmi_cat) |&gt;\n    summarise(mean = mean(waist), sd = sd(waist), \n              median = median(waist), \n              skew_1 = round((mean(waist) - median(waist)) / \n                                 sd(waist),2))\n\n# A tibble: 5 × 5\n  bmi_cat        mean    sd median skew_1\n  &lt;fct&gt;         &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1_Underweight  55.2  7.58   54.5   0.09\n2 2_Normal       NA   NA      NA    NA   \n3 3_Overweight   72.3 11.9    74    -0.14\n4 4_Obese        NA   NA      NA    NA   \n5 &lt;NA&gt;           NA   NA      NA    NA   \n\n\nOops. Looks like we need to filter for cases with complete data on both BMI category and waist circumference in order to get meaningful results. We should add a count, too.\n\nnnyfs |&gt;\n    filter(complete.cases(bmi_cat, waist)) |&gt;\n    group_by(bmi_cat) |&gt;\n    summarise(count = n(), mean = mean(waist), \n              sd = sd(waist), median = median(waist), \n       skew_1 = \n         round((mean(waist) - median(waist)) / sd(waist),2))\n\n# A tibble: 4 × 6\n  bmi_cat       count  mean    sd median skew_1\n  &lt;fct&gt;         &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1_Underweight    41  55.2  7.58   54.5   0.09\n2 2_Normal        917  61.2  9.35   59.5   0.19\n3 3_Overweight    258  72.3 11.9    74    -0.14\n4 4_Obese         294  85.6 17.1    86.8  -0.07\n\n\nOr, we could use something like favstats from the mosaic package, which automatically accounts for missing data, and omits it when calculating summary statistics within each group.\n\nmosaic::favstats(waist ~ bmi_cat, data = nnyfs) |&gt;\n    kbl(digits = 1) |&gt;\n    kable_styling(full_width = FALSE)\n\n\n\nbmi_cat\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n1_Underweight\n42.5\n49.3\n54.5\n62.4\n68.5\n55.2\n7.6\n41\n0\n\n\n2_Normal\n44.1\n53.9\n59.5\n68.4\n89.2\n61.2\n9.4\n917\n3\n\n\n3_Overweight\n49.3\n62.3\n74.0\n81.2\n105.3\n72.3\n11.9\n258\n0\n\n\n4_Obese\n52.1\n72.7\n86.8\n96.8\n144.7\n85.6\n17.1\n294\n1\n\n\n\n\n\nWhile patients in the heavier groups generally had higher waist circumferences, the standard deviations suggest there may be some meaningful overlap. Let’s draw the picture, in this case a comparison boxplot accompanying a violin plot.\n\nnnyfs_temp2 &lt;- nnyfs |&gt;\n    filter(complete.cases(bmi_cat, waist))\n\nggplot(nnyfs_temp2, aes(x = bmi_cat, y = waist)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = bmi_cat), width = 0.2) +\n    theme_light() + \n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Waist Circumference by BMI Category\")\n\n\n\n\n\n\n\nThe data transformation with dplyr cheat sheet found under the Help menu in RStudio is a great resource. And, of course, for more details, visit Hadley Wickham and Grolemund (2023).",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#boxplots-to-relate-an-outcome-to-a-categorical-predictor",
    "href": "10-nnyfs_foundations.html#boxplots-to-relate-an-outcome-to-a-categorical-predictor",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.18 Boxplots to Relate an Outcome to a Categorical Predictor",
    "text": "10.18 Boxplots to Relate an Outcome to a Categorical Predictor\nBoxplots are much more useful when comparing samples of data. For instance, consider this comparison boxplot describing the triceps skinfold results across the four levels of BMI category.\n\nggplot(nnyfs, aes(x = bmi_cat, y = triceps_skinfold,\n                  fill = bmi_cat)) +\n    geom_boxplot() +\n    scale_fill_viridis_d() +\n    theme_light()\n\nWarning: Removed 21 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\nAgain, we probably want to omit those missing values (both in bmi_cat and triceps_skinfold) and also eliminate the repetitive legend (guides) on the right.\n\nnnyfs_temp3 &lt;- nnyfs |&gt; \n  filter(complete.cases(bmi_cat, triceps_skinfold)) \n\nggplot(nnyfs_temp3, \n       aes(x = bmi_cat, y = triceps_skinfold, fill = bmi_cat)) +\n    geom_boxplot() +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    theme_light() +\n    labs(x = \"BMI Category\", y = \"Triceps Skinfold in mm\",\n         title = \"Triceps Skinfold increases with BMI category\",\n         subtitle = \"NNYFS children\")\n\n\n\n\n\n\n\nAs always, the boxplot shows the five-number summary (minimum, 25th percentile, median, 75th percentile and maximum) in addition to highlighting candidate outliers.\n\n10.18.1 Augmenting the Boxplot with the Sample Mean\nOften, we want to augment such a plot, perhaps by adding a little diamond to show the sample mean within each category, so as to highlight skew (in terms of whether the mean is meaningfully different from the median.)\n\nnnyfs_temp3 &lt;- nnyfs |&gt; \n  filter(complete.cases(bmi_cat, triceps_skinfold)) \n\nggplot(nnyfs_temp3, \n       aes(x = bmi_cat, y = triceps_skinfold, fill = bmi_cat)) +\n    geom_boxplot() +\n    stat_summary(fun=\"mean\", geom=\"point\", \n                 shape=23, size=3, fill=\"white\") +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    theme_light() +\n    labs(x = \"BMI Category\", y = \"Triceps Skinfold in mm\",\n         title = \"Triceps Skinfold increases with BMI category\",\n         subtitle = \"NNYFS children\")",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#building-a-violin-plot",
    "href": "10-nnyfs_foundations.html#building-a-violin-plot",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.19 Building a Violin Plot",
    "text": "10.19 Building a Violin Plot\nThere are a number of other plots which compare distributions of data sets. An interesting one is called a violin plot. A violin plot is a kernel density estimate, mirrored to form a symmetrical shape.\n\nnnyfs_temp3 &lt;- nnyfs |&gt; \n  filter(complete.cases(bmi_cat, triceps_skinfold)) \n\nggplot(nnyfs_temp3, aes(x=bmi_cat, y=triceps_skinfold, fill = bmi_cat)) + \n    geom_violin(trim=FALSE) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Triceps Skinfold by BMI Category\")\n\n\n\n\n\n\n\nTraditionally, these plots are shown with overlaid boxplots and a white dot at the median, like this example, now looking at waist circumference again.\n\nnnyfs_temp2 &lt;- nnyfs |&gt;\n    filter(complete.cases(bmi_cat, waist))\n\nggplot(nnyfs_temp2, aes(x = bmi_cat, y = waist, fill = bmi_cat)) + \n    geom_violin(trim=FALSE) +\n    geom_boxplot(width=.1, outlier.colour=NA, \n                 color = c(rep(\"white\",2), rep(\"black\",2))) +\n    stat_summary(fun=median, geom=\"point\", \n                 fill=\"white\", shape=21, size=3) + \n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Waist Circumference by BMI Category\")\n\n\n\n\n\n\n\n\n10.19.1 Adding Notches to a Boxplot\nNotches are used in boxplots to help visually assess whether the medians of the distributions across the various groups actually differ to a statistically detectable extent. Think of them as confidence regions around the medians. If the notches do not overlap, as in this situation, this provides some evidence that the medians in the populations represented by these samples may be different.\n\nnnyfs_temp3 &lt;- nnyfs |&gt; \n  filter(complete.cases(bmi_cat, triceps_skinfold)) \n\nggplot(nnyfs_temp3, aes(x = bmi_cat, y = triceps_skinfold)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = bmi_cat), width = 0.3, notch = TRUE) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    theme_light() +\n    labs(x = \"BMI Category\", y = \"Triceps Skinfold in mm\",\n         title = \"Triceps Skinfold increases with BMI category\",\n         subtitle = \"NNYFS children\")\n\n\n\n\n\n\n\nThere is no overlap between the notches for each of the four categories, so we might reasonably conclude that the true median triceps skinfold values across the four categories are statistically significantly different.\nFor an example where the notches do overlap, consider the comparison of plank times by BMI category.\n\nnnyfs_temp4 &lt;- nnyfs |&gt; \n  filter(complete.cases(bmi_cat, plank_time)) \n\nggplot(nnyfs_temp4, aes(x=bmi_cat, y=plank_time)) +\n    geom_violin(aes(fill = bmi_cat)) +\n    geom_boxplot(width = 0.3, notch=TRUE) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") + \n    theme_light() +\n    labs(title = \"Plank Times by BMI category\", \n         x = \"\", y = \"Plank Time (in seconds)\")\n\n\n\n\n\n\n\nThe overlap in the notches (for instance between Underweight and Normal) suggests that the median plank times in the population of interest don’t necessarily differ in a meaningful way by BMI category, other than perhaps the Obese group which may have a shorter time.\nThese data are somewhat right skewed. Would a logarithmic transformation in the plot help us see the patterns more clearly?\n\nnnyfs_temp4 &lt;- nnyfs |&gt; \n  filter(complete.cases(bmi_cat, plank_time)) \n\nggplot(nnyfs_temp4, aes(x=bmi_cat, y = log(plank_time))) +\n    geom_violin() +\n    geom_boxplot(aes(fill = bmi_cat), width = 0.3, notch=TRUE) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") + \n    theme_light() +\n    labs(title = \"log(Plank Times) by BMI category\", \n         x = \"\", y = \"Natural Log of Plank Time\")",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#using-multiple-histograms-to-make-comparisons",
    "href": "10-nnyfs_foundations.html#using-multiple-histograms-to-make-comparisons",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.20 Using Multiple Histograms to Make Comparisons",
    "text": "10.20 Using Multiple Histograms to Make Comparisons\nWe can make an array of histograms to describe multiple groups of data, using ggplot2 and the notion of faceting our plot.\n\nnnyfs_temp3 &lt;- nnyfs |&gt; \n  filter(complete.cases(bmi_cat, triceps_skinfold)) \n\nggplot(nnyfs_temp3, aes(x=triceps_skinfold, fill = bmi_cat)) +\n    geom_histogram(binwidth = 2, color = \"black\") + \n    facet_wrap(~ bmi_cat) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Triceps Skinfold by BMI Category\")",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#using-multiple-density-plots-to-make-comparisons",
    "href": "10-nnyfs_foundations.html#using-multiple-density-plots-to-make-comparisons",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.21 Using Multiple Density Plots to Make Comparisons",
    "text": "10.21 Using Multiple Density Plots to Make Comparisons\nOr, we can make a series of density plots to describe multiple groups of data.\n\nnnyfs_temp3 &lt;- nnyfs |&gt; \n  filter(complete.cases(bmi_cat, triceps_skinfold)) \n\nggplot(nnyfs_temp3, aes(x=triceps_skinfold, fill = bmi_cat)) +\n    geom_density(color = \"black\") + \n    facet_wrap(~ bmi_cat) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Triceps Skinfold by BMI Category\")\n\n\n\n\n\n\n\nOr, we can plot all of the densities on top of each other with semi-transparent fills.\n\nnnyfs_temp3 &lt;- nnyfs |&gt; \n  filter(complete.cases(bmi_cat, triceps_skinfold)) \n\nggplot(nnyfs_temp3, aes(x=triceps_skinfold, fill = bmi_cat)) +\n    geom_density(alpha=0.3) + \n    scale_fill_viridis_d() + \n    labs(title = \"Triceps Skinfold by BMI Category\")\n\n\n\n\n\n\n\nThis really works better when we are comparing only two groups, like females to males.\n\nnnyfs_temp5 &lt;- nnyfs |&gt; \n  filter(complete.cases(sex, triceps_skinfold)) \n\nggplot(nnyfs_temp5, aes(x=triceps_skinfold, fill = sex)) +\n    geom_density(alpha=0.5) + \n    labs(title = \"Triceps Skinfold by Sex\")",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#a-ridgeline-plot",
    "href": "10-nnyfs_foundations.html#a-ridgeline-plot",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.22 A Ridgeline Plot",
    "text": "10.22 A Ridgeline Plot\nSome people don’t like violin plots - for example, see https://simplystatistics.org/2017/07/13/the-joy-of-no-more-violin-plots/. An alternative plot is available as part of the ggridges package. This shows the distribution of several groups simultaneously, especially when you have lots of subgroup categories, and is called a ridgeline plot.\n\nnnyfs_temp6 &lt;- nnyfs |&gt; \n  filter(complete.cases(waist, bmi_cat)) \n\nggplot(nnyfs_temp6, aes(x = waist, y = bmi_cat, height = ..density..)) +\n    geom_density_ridges(scale = 0.85) + \n    theme_light() +\n    labs(title = \"Ridgeline Plot: Waist Circumference by BMI category (nnyfs)\",\n         x = \"Waist Circumference\", y = \"BMI Category\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\nPicking joint bandwidth of 3.47\n\n\n\n\n\n\n\n\nAnd here’s a ridgeline plot for the triceps skinfolds. We’ll start by sorting the subgroups by the median value of our outcome (triceps skinfold) in this case, though it turns out not to matter. We’ll also add some color.\n\nnnyfs_temp3 &lt;- nnyfs |&gt; \n  filter(complete.cases(bmi_cat, triceps_skinfold)) |&gt;\n  mutate(bmi_cat = fct_reorder(bmi_cat,\n                               triceps_skinfold, \n                               .fun = median))\n\nggplot(nnyfs_temp3, aes(x = triceps_skinfold, y = bmi_cat, \n                  fill = bmi_cat, height = ..density..)) +\n    ggridges::geom_density_ridges(scale = 0.85) + \n    scale_fill_viridis_d(option = \"magma\") +\n    guides(fill = \"none\") +\n    labs(title = \"Ridgeline Plot of Triceps Skinfold by BMI Category (nnyfs)\",\n         x = \"Triceps Skinfold\", y = \"BMI Category\") +\n    theme_light()\n\nPicking joint bandwidth of 1.37\n\n\n\n\n\n\n\n\nFor one last example, we’ll look at age by BMI category, so that sorting the BMI subgroups by the median matters, and we’ll try an alternate color scheme, and a theme specially designed for the ridgeline plot.\n\nnnyfs_temp7 &lt;- nnyfs |&gt; \n  filter(complete.cases(bmi_cat, age_child)) |&gt;\n  mutate(bmi_cat = reorder(bmi_cat, age_child, median))\n\nggplot(nnyfs_temp7, aes(x = age_child, y = bmi_cat, \n                        fill = bmi_cat, height = ..density..)) +\n    geom_density_ridges(scale = 0.85) + \n    scale_fill_brewer(palette = \"YlOrRd\") +\n    guides(fill = \"none\") +\n    labs(title = \"Ridgeline Plot of Age at Exam by BMI category (nnyfs)\",\n         x = \"Age of Child at Exam\", y = \"BMI Category\") +\n    theme_ridges()\n\nPicking joint bandwidth of 1.15",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#what-summaries-to-report",
    "href": "10-nnyfs_foundations.html#what-summaries-to-report",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.23 What Summaries to Report",
    "text": "10.23 What Summaries to Report\nIt is usually helpful to focus on the shape, center and spread of a distribution. Bock, Velleman and DeVeaux provide some useful advice:\n\nIf the data are skewed, report the median and IQR (or the three middle quantiles). You may want to include the mean and standard deviation, but you should point out why the mean and median differ. The fact that the mean and median do not agree is a sign that the distribution may be skewed. A histogram will help you make that point.\nIf the data are symmetric, report the mean and standard deviation, and possibly the median and IQR as well.\nIf there are clear outliers and you are reporting the mean and standard deviation, report them with the outliers present and with the outliers removed. The differences may be revealing. The median and IQR are not likely to be seriously affected by outliers.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "10-nnyfs_foundations.html#coming-up",
    "href": "10-nnyfs_foundations.html#coming-up",
    "title": "10  National Youth Fitness Survey",
    "section": "\n10.24 Coming Up",
    "text": "10.24 Coming Up\nNext, we’ll look at the issue of Normalilty, and in particular how to assess whether a particular batch of data is well-approximated by the Normal distribution.\n\n\n\n\nHadley Wickham, Mine Çetinyaka-Rundel, and Garrett Grolemund. 2023. R for Data Science. Second. O’Reilly. https://r4ds.hadley.nz/.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>National Youth Fitness Survey</span>"
    ]
  },
  {
    "objectID": "11-normality.html",
    "href": "11-normality.html",
    "title": "11  Assessing Normality",
    "section": "",
    "text": "11.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(patchwork)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\nWe also use the favstat function from the mosaic package in this chapter, but do not load the whole package.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing Normality</span>"
    ]
  },
  {
    "objectID": "11-normality.html#introduction",
    "href": "11-normality.html#introduction",
    "title": "11  Assessing Normality",
    "section": "\n11.2 Introduction",
    "text": "11.2 Introduction\nData are well approximated by a Normal distribution if the shape of the data’s distribution is a good match for a Normal distribution with mean and standard deviation equal to the sample statistics.\n\nthe data are symmetrically distributed about a single peak, located at the sample mean\nthe spread of the distribution is well characterized by a Normal distribution with standard deviation equal to the sample standard deviation\nthe data show outlying values (both in number of candidate outliers, and size of the distance between the outliers and the center of the distribution) that are similar to what would be predicted by a Normal model.\n\nWe have several tools for assessing Normality of a single batch of data, including:\n\na histogram with superimposed Normal distribution\nhistogram variants (like the boxplot) which provide information on the center, spread and shape of a distribution\nthe Empirical Rule for interpretation of a standard deviation\na specialized normal Q-Q plot (also called a normal probability plot or normal quantile-quantile plot) designed to reveal differences between a sample distribution and what we might expect from a normal distribution of a similar number of values with the same mean and standard deviation",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing Normality</span>"
    ]
  },
  {
    "objectID": "11-normality.html#empirical-rule-interpretation-of-the-standard-deviation",
    "href": "11-normality.html#empirical-rule-interpretation-of-the-standard-deviation",
    "title": "11  Assessing Normality",
    "section": "\n11.3 Empirical Rule Interpretation of the Standard Deviation",
    "text": "11.3 Empirical Rule Interpretation of the Standard Deviation\nFor a set of measurements that follows a Normal distribution, the interval:\n\nMean \\(\\pm\\) Standard Deviation contains approximately 68% of the measurements;\nMean \\(\\pm\\) 2(Standard Deviation) contains approximately 95% of the measurements;\nMean \\(\\pm\\) 3(Standard Deviation) contains approximately all (99.7%) of the measurements.\n\nAgain, most data sets do not follow a Normal distribution. We will occasionally think about transforming or re-expressing our data to obtain results which are better approximated by a Normal distribution, in part so that a standard deviation can be more meaningful.\nFor the energy data we have been studying, here again are some summary statistics…\n\nnnyfs &lt;- read_rds(\"data/nnyfs.Rds\")\n\n\nmosaic::favstats(nnyfs$energy)\n\n min     Q1 median   Q3  max     mean       sd    n missing\n 257 1367.5 1794.5 2306 5265 1877.157 722.3537 1518       0\n\n\nThe mean is 1877 and the standard deviation is 722, so if the data really were Normally distributed, we’d expect to see:\n\nAbout 68% of the data in the range (1155, 2600). In fact, 1085 of the 1518 energy values are in this range, or 71.5%.\nAbout 95% of the data in the range (432, 3322). In fact, 1450 of the 1518 energy values are in this range, or 95.5%.\nAbout 99.7% of the data in the range (-290, 4044). In fact, 1502 of the 1518 energy values are in this range, or 98.9%.\n\nSo, based on this Empirical Rule approximation, do the energy data seem to be well approximated by a Normal distribution?",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing Normality</span>"
    ]
  },
  {
    "objectID": "11-normality.html#describing-outlying-values-with-z-scores",
    "href": "11-normality.html#describing-outlying-values-with-z-scores",
    "title": "11  Assessing Normality",
    "section": "\n11.4 Describing Outlying Values with Z Scores",
    "text": "11.4 Describing Outlying Values with Z Scores\nThe maximum energy consumption value here is 5265. One way to gauge how extreme this is (or how much of an outlier it is) uses that observation’s Z score, the number of standard deviations away from the mean that the observation falls.\nHere, the maximum value, 5265 is 4.69 standard deviations above the mean, and thus has a Z score of 4.7.\nA negative Z score would indicate a point below the mean, while a positive Z score indicates, as we’ve seen, a point above the mean. The minimum body-mass index, 257 is 2.24 standard deviations below the mean, so it has a Z score of -2.2.\nRecall that the Empirical Rule suggests that if a variable follows a Normal distribution, it would have approximately 95% of its observations falling inside a Z score of (-2, 2), and 99.74% falling inside a Z score range of (-3, 3).\n\n11.4.1 Fences and Z Scores\nNote the relationship between the fences (Tukey’s approach to identifying points which fall within the whiskers of a boxplot, as compared to candidate outliers) and the Z scores.\nThe upper inner fence in this case falls at 3713.75, which indicates a Z score of 2.5, while the lower inner fence falls at -40.25, which indicates a Z score of -2.7. It is neither unusual nor inevitable for the inner fences to fall at Z scores near -2.0 and +2.0.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing Normality</span>"
    ]
  },
  {
    "objectID": "11-normality.html#comparing-a-histogram-to-a-normal-distribution",
    "href": "11-normality.html#comparing-a-histogram-to-a-normal-distribution",
    "title": "11  Assessing Normality",
    "section": "\n11.5 Comparing a Histogram to a Normal Distribution",
    "text": "11.5 Comparing a Histogram to a Normal Distribution\nMost of the time, when we want to understand whether our data are well approximated by a Normal distribution, we will use a graph to aid in the decision.\nOne option is to build a histogram with a Normal density function (with the same mean and standard deviation as our data) superimposed. This is one way to help visualize deviations between our data and what might be expected from a Normal distribution.\n\nres &lt;- mosaic::favstats(~ energy, data = nnyfs)\nbin_w &lt;- 50 # specify binwidth\n\nggplot(nnyfs, aes(x=energy)) +\n    geom_histogram(aes(y = ..density..), binwidth = bin_w, \n                   fill = \"papayawhip\", color = \"seagreen\") +\n    stat_function(fun = dnorm, \n                  args = list(mean = res$mean, sd = res$sd), \n                  lwd = 1.5, col = \"blue\") +\n    geom_text(aes(label = paste(\"Mean\", round(res$mean,1), \n                                \", SD\", round(res$sd,1))),\n              x = 4000, y = 0.0006, \n              color=\"blue\", fontface = \"italic\") + \n    labs(title = \"nnyfs energy values with Normal Density Superimposed\", \n         x = \"Energy (kcal)\", y = \"Probability Density Function\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\nWarning in geom_text(aes(label = paste(\"Mean\", round(res$mean, 1), \", SD\", : All aesthetics have length 1, but the data has 1518 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\nDoes it seem as though the Normal model (as shown in the blue density curve) is an effective approximation to the observed distribution shown in the bars of the histogram?\nWe’ll return shortly to the questions:\n\nDoes a Normal distribution model fit our data well? and\n\nIf the data aren’t Normal, but we want to use a Normal model anyway, what should we do?\n\n\n11.5.1 Histogram of energy with Normal model (with Counts)\nBut first, we’ll demonstrate an approach to building a histogram of counts (rather than a probability density) and then superimposing a Normal model.\n\nres &lt;- mosaic::favstats(~ energy, data = nnyfs)\nbin_w &lt;- 50 # specify binwidth\n\nggplot(nnyfs, aes(x = energy)) +\n  geom_histogram(binwidth = bin_w, \n                 fill = \"papayawhip\", \n                 col = \"navy\") +\n  stat_function(\n    fun = function(x) dnorm(x, mean = res$mean, \n                            sd = res$sd) * res$n * bin_w,\n    col = \"blue\", linewidth = 2) +\n    geom_text(aes(label = paste(\"Mean\", round(res$mean,1), \n                                \", SD\", round(res$sd,1))),\n              x = 4000, y = 50, \n              color=\"blue\", fontface = \"italic\") + \n    labs(title = \"Histogram of energy, with Normal Model\", \n         x = \"Energy consumed (kcal)\", y = \"# of subjects\")\n\nWarning in geom_text(aes(label = paste(\"Mean\", round(res$mean, 1), \", SD\", : All aesthetics have length 1, but the data has 1518 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing Normality</span>"
    ]
  },
  {
    "objectID": "11-normality.html#does-a-normal-model-work-well-for-the-waist-circumference",
    "href": "11-normality.html#does-a-normal-model-work-well-for-the-waist-circumference",
    "title": "11  Assessing Normality",
    "section": "\n11.6 Does a Normal model work well for the waist circumference?",
    "text": "11.6 Does a Normal model work well for the waist circumference?\nNow, suppose we instead look at the waist data, remembering to filter the data to the complete cases before plotting. Do these data appear to follow a Normal distribution?\n\nres &lt;- mosaic::favstats(~ waist, data = nnyfs)\nbin_w &lt;- 5 # specify binwidth\n\nnnyfs_ccw &lt;- nnyfs |&gt; \n  filter(complete.cases(waist))\n\nggplot(nnyfs_ccw, aes(x = waist)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"antiquewhite\", \n                   col = \"navy\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"darkred\", linewidth = 2) +\n    geom_text(aes(label = paste(\"Mean\", round(res$mean,1), \n                                \", SD\", round(res$sd,1))),\n              x = 100, y = 200, \n              color=\"darkred\", fontface = \"italic\") + \n    labs(title = \"Histogram of waist, with Normal Model\", \n         x = \"Waist Circumference (cm)\", y = \"# of subjects\")\n\nWarning in geom_text(aes(label = paste(\"Mean\", round(res$mean, 1), \", SD\", : All aesthetics have length 1, but the data has 1512 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nmosaic::favstats(~ waist, data = nnyfs)\n\n  min   Q1 median   Q3   max     mean       sd    n missing\n 42.5 55.6   64.8 76.6 144.7 67.70536 15.19809 1512       6\n\n\nIf we rerun this after excluding the missing values, only the last column changes.\n\nmosaic::favstats(~ waist, data = nnyfs_ccw)\n\n  min   Q1 median   Q3   max     mean       sd    n missing\n 42.5 55.6   64.8 76.6 144.7 67.70536 15.19809 1512       0\n\n\nThe mean is 67.71 and the standard deviation is 15.2 so if the waist data really were Normally distributed, we’d expect to see:\n\nAbout 68% of the data in the range (52.51, 82.9). In fact, 1076 of the 1512 Age values are in this range, or 71.2%.\nAbout 95% of the data in the range (37.31, 98.1). In fact, 1443 of the 1512 Age values are in this range, or 95.4%.\nAbout 99.7% of the data in the range (22.11, 113.3). In fact, 1500 of the 1512 Age values are in this range, or 99.2%.\n\nHow does the Normal approximation work for waist circumference, according to the Empirical Rule?",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing Normality</span>"
    ]
  },
  {
    "objectID": "11-normality.html#the-normal-q-q-plot",
    "href": "11-normality.html#the-normal-q-q-plot",
    "title": "11  Assessing Normality",
    "section": "\n11.7 The Normal Q-Q Plot",
    "text": "11.7 The Normal Q-Q Plot\nA normal probability plot (or normal quantile-quantile plot) of the energy results from the nnyfs data, developed using ggplot2 is shown below. In this case, this is a picture of 1518 energy consumption assessments. The idea of a normal Q-Q plot is that it plots the observed sample values (on the vertical axis) and then, on the horizontal, the expected or theoretical quantiles that would be observed in a standard normal distribution (a Normal distribution with mean 0 and standard deviation 1) with the same number of observations.\nA Normal Q-Q plot will follow a straight line when the data are (approximately) Normally distributed. When the data have a different shape, the plot will reflect that.\n\nggplot(nnyfs, aes(sample = energy)) +\n    geom_qq() + geom_qq_line(col = \"red\") +\n    theme_light() +\n    labs(title = \"Normal Q-Q plot for energy data\")",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing Normality</span>"
    ]
  },
  {
    "objectID": "11-normality.html#interpreting-the-normal-q-q-plot",
    "href": "11-normality.html#interpreting-the-normal-q-q-plot",
    "title": "11  Assessing Normality",
    "section": "\n11.8 Interpreting the Normal Q-Q Plot",
    "text": "11.8 Interpreting the Normal Q-Q Plot\nThe purpose of a Normal Q-Q plot is to help point out distinctions from a Normal distribution. A Normal distribution is symmetric and has certain expectations regarding its tails. The Normal Q-Q plot can help us identify data as well approximated by a Normal distribution, or not, because of:\n\nskew (including distinguishing between right skew and left skew)\nbehavior in the tails (which could be heavy-tailed [more outliers than expected] or light-tailed)\n\n\n11.8.1 Data from a Normal distribution shows up as a straight line in a Normal Q-Q plot\nWe’ll demonstrate the looks that we can obtain from a Normal Q-Q plot in some simulations. First, here is an example of a Normal Q-Q plot, and its associated histogram, for a sample of 200 observations simulated from a Normal distribution.\n\nset.seed(123431) # so the results can be replicated\n                                          \n# simulate 200 observations from a Normal(20, 5) distribution and place them \n# in the d variable within the temp_1 tibble\ntemp_1 &lt;- tibble(d = rnorm(200, mean = 20, sd = 5)) \n                                          \n# left plot - basic Normal Q-Q plot of simulated data\np1 &lt;- ggplot(temp_1, aes(sample = d)) +\n    geom_qq() + geom_qq_line(col = \"red\") +\n    theme_light() +\n    labs(y = \"Ordered Simulated Sample Data\")\n\n# right plot - histogram with superimposed normal distribution\nres &lt;- mosaic::favstats(~ d, data = temp_1)\nbin_w &lt;- 2 # specify binwidth\n\np2 &lt;- ggplot(temp_1, aes(x = d)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"papayawhip\", \n                   col = \"seagreen\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"blue\", linewidth = 1.5) +\n    geom_text(aes(label = paste(\"Mean\", round(res$mean,1), \n                                \", SD\", round(res$sd,1))),\n              x = 25, y = 35, \n              color=\"blue\", fontface = \"italic\") + \n    labs(x = \"Simulated Sample Data\", y = \"\")\n\np1 + p2 + \n  plot_annotation(title = \"200 observations from a simulated Normal\") \n\nWarning in geom_text(aes(label = paste(\"Mean\", round(res$mean, 1), \", SD\", : All aesthetics have length 1, but the data has 200 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n# uses patchwork package to combine plots\n\nThese simulated data appear to be well-modeled by the Normal distribution, because the points on the Normal Q-Q plot follow the diagonal reference line. In particular,\n\nthere is no substantial curve (such as we’d see with data that were skewed)\nthere is no particularly surprising behavior (curves away from the line) at either tail, so there’s no obvious problem with outliers\n\n11.8.2 Skew is indicated by monotonic curves in the Normal Q-Q plot\nData that come from a skewed distribution appear to curve away from a straight line in the Q-Q plot.\n\nset.seed(123431) # so the results can be replicated\n\n# simulate 200 observations from a beta(5, 2) distribution into the e1 variable\n# simulate 200 observations from a beta(1, 5) distribution into the e2 variable\ntemp_2 &lt;- tibble(e1 = rbeta(200, 5, 2), e2 = rbeta(200, 1, 5)) \n\np1 &lt;- ggplot(temp_2, aes(sample = e1)) +\n    geom_qq(col = \"orchid\") + geom_qq_line(col = \"blue\") +\n    theme_light() +\n    labs(y = \"Ordered Sample e1\",\n         title = \"Beta(5, 2) sample: Left Skewed\")\n\np2 &lt;- ggplot(temp_2, aes(sample = e2)) +\n    geom_qq(col = \"darkorange\") + geom_qq_line(col = \"blue\") +\n    theme_light() +\n    labs(y = \"Ordered Sample e2\",\n         title = \"Beta(1, 5) sample: Right Skewed\")\n\np1 + p2 + plot_annotation(title = \"200 observations from simulated Betas\")\n\n\n\n\n\n\n\nNote the bends away from a straight line in each sample. The non-Normality may be easier to see in a histogram.\n\nres1 &lt;- mosaic::favstats(~ e1, data = temp_2)\nbin_w1 &lt;- 0.025 # specify binwidth\n\np1 &lt;- ggplot(temp_2, aes(x = e1)) +\n    geom_histogram(binwidth = bin_w1, \n                   fill = \"orchid\", \n                   col = \"black\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res1$mean, \n                                sd = res1$sd) * \n            res1$n * bin_w1,\n        col = \"blue\", linewidth = 1.5) +\nlabs(x = \"Sample e1\", y = \"\",\n     title = \"Beta(5,2) sample: Left Skew\")\n\nres2 &lt;- mosaic::favstats(~ e2, data = temp_2)\nbin_w2 &lt;- 0.025 # specify binwidth\n\np2 &lt;- ggplot(temp_2, aes(x = e2)) +\n    geom_histogram(binwidth = bin_w2, \n                   fill = \"darkorange\", \n                   col = \"black\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res2$mean, \n                                sd = res2$sd) * \n            res2$n * bin_w2,\n        col = \"blue\", linewidth = 1.5) +\nlabs(x = \"Sample e1\", y = \"\",\n     title = \"Beta(1,5) sample: Right Skew\")\n\np1 + p2 + plot_annotation(caption = \"Histograms with Normal curve superimposed\")\n\n\n\n\n\n\n\n\n11.8.3 Direction of Skew\nIn each of these pairs of plots, we see the same basic result.\n\nThe left plot (for data e1) shows left skew, with a longer tail on the left hand side and more clustered data at the right end of the distribution.\nThe right plot (for data e2) shows right skew, with a longer tail on the right hand side, the mean larger than the median, and more clustered data at the left end of the distribution.\n\n11.8.4 Outlier-proneness is indicated by “s-shaped” curves in a Normal Q-Q plot\n\nHeavy-tailed but symmetric distributions are indicated by reverse “S”-shapes, as shown on the left below.\nLight-tailed but symmetric distributions are indicated by “S” shapes in the plot, as shown on the right below.\n\n\nset.seed(4311) # so the results can be replicated\n\n# sample 200 observations from each of two probability distributions\ntemp_3 &lt;- tibble(s1 = rcauchy(200, location=10, scale = 1),\n                     s2 = runif(200, -30, 30)) \n\np1 &lt;- ggplot(temp_3, aes(sample = s1)) +\n    geom_qq(col = \"slateblue\") + geom_qq_line(col = \"black\") +\n    theme_light() +\n    labs(y = \"Ordered Sample s1\",\n         title = \"Heavy-Tailed Symmetric Sample s1\")\n\np2 &lt;- ggplot(temp_3, aes(sample = s2)) +\n    geom_qq(col = \"dodgerblue\") + geom_qq_line(col = \"black\") +\n    theme_light() +\n    labs(y = \"Ordered Sample s2\",\n         title = \"Light-Tailed Symmetric Sample s2\")\n\np1 + p2 + plot_annotation(title = \"200 observations from simulated distributions\")\n\n\n\n\n\n\n\nAnd, we can also visualize these simulations with histograms, although they’re less helpful for understanding tail behavior than they are for skew.\n\nres1 &lt;- mosaic::favstats(~ s1, data = temp_3)\nbin_w1 &lt;- 20 # specify binwidth\n\np1 &lt;- ggplot(temp_3, aes(x = s1)) +\n    geom_histogram(binwidth = bin_w1, \n                   fill = \"slateblue\", \n                   col = \"white\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res1$mean, \n                                sd = res1$sd) * \n            res1$n * bin_w1,\n        col = \"blue\") +\nlabs(x = \"Sample s1\", y = \"\",\n     title = \"Cauchy sample: Heavy Tails\")\n\nres2 &lt;- mosaic::favstats(~ s2, data = temp_3)\nbin_w2 &lt;- 2 # specify binwidth\n\np2 &lt;- ggplot(temp_3, aes(x = s2)) +\n    geom_histogram(binwidth = bin_w2, \n                   fill = \"dodgerblue\", \n                   col = \"white\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res2$mean, \n                                sd = res2$sd) * \n            res2$n * bin_w2,\n        col = \"blue\") +\nlabs(x = \"Sample s2\", y = \"\",\n     title = \"Uniform sample: Light Tails\")\n\np1 + p2 + plot_annotation(caption = \"Histograms with Normal curve superimposed\")\n\n\n\n\n\n\n\nInstead, boxplots (here augmented with violin plots) can be more helpful when thinking about light-tailed vs. heavy-tailed distributions.\n\np1 &lt;- ggplot(temp_3, aes(x = \"s1\", y = s1)) +\n    geom_violin(col = \"slateblue\") + \n    geom_boxplot(fill = \"slateblue\", width = 0.2) +\n    theme_light() +\n    coord_flip() +\n    labs(y = \"Ordered Sample s1\", x = \"\",\n         title = \"Heavy-Tailed Symmetric Sample s1\")\n\np2 &lt;- ggplot(temp_3, aes(x = \"s2\", y = s2)) +\n    geom_violin(col = \"dodgerblue\") + \n    geom_boxplot(fill = \"dodgerblue\", width = 0.2) +\n    theme_light() +\n    coord_flip() +\n    labs(y = \"Ordered Sample s2\", x = \"\",\n         title = \"Light-Tailed Symmetric Sample s2\")\n\np1 + p2 + plot_annotation(title = \"200 observations from simulated distributions\")\n\n\n\n\n\n\n\n\nrm(temp_1, temp_2, temp_3, p1, p2, \n   res, res1, res2, bin_w, bin_w1, bin_w2) # cleaning up",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing Normality</span>"
    ]
  },
  {
    "objectID": "11-normality.html#can-a-normal-distribution-fit-the-nnyfs-energy-data-well",
    "href": "11-normality.html#can-a-normal-distribution-fit-the-nnyfs-energy-data-well",
    "title": "11  Assessing Normality",
    "section": "\n11.9 Can a Normal Distribution Fit the nnyfs energy data Well?",
    "text": "11.9 Can a Normal Distribution Fit the nnyfs energy data Well?\nThe energy data we’ve been studying shows meaningful signs of right skew.\n\np1 &lt;- ggplot(nnyfs, aes(sample = energy)) +\n    geom_qq(col = \"coral\", size = 2) + \n    geom_qq_line(col = \"blue\") +\n    theme_light() +\n    labs(title = \"Energy Consumed\",\n         y = \"Sorted Energy data\")\n\nres &lt;- mosaic::favstats(~ energy, data = nnyfs)\nbin_w &lt;- 250 # specify binwidth\n\np2 &lt;- ggplot(nnyfs, aes(x = energy)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"coral\", \n                   col = \"white\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"blue\", linewidth = 1.5) +\nlabs(x = \"Energy (kcal consumed)\", y = \"\",\n     title = \"Energy Consumed\")\n\n\n\np1 + p2\n\n\n\n\n\n\n\n\nSkewness is indicated by the curve in the Normal Q-Q plot. Curving up and away from the line in both tails suggests right skew, as does the histogram.\n\nWhat if we plotted not the original energy values (all of which are positive) but instead plotted the square roots of the energy values?\n\nCompare these two plots - the left describes the distribution of the original energy data from the NNYFS data frame, and the right plot shows the distribution of the square root of those values.\n\n\np1 &lt;- ggplot(nnyfs, aes(sample = energy)) +\n    geom_qq(col = \"coral\", size = 2) + \n    geom_qq_line(col = \"blue\") +\n    theme_light() +\n    labs(title = \"Energy Consumed\",\n         y = \"Sorted Energy data\")\n\np2 &lt;- ggplot(nnyfs, aes(sample = sqrt(energy))) +\n    geom_qq(col = \"darkcyan\", size = 2) + \n    geom_qq_line(col = \"blue\") +\n    theme_light() +\n    labs(title = \"Square Root of Energy\",\n         y = \"Sorted Square Root of Energy\")\n\np1 + p2\n\n\n\n\n\n\n\n\nThe left plot shows substantial right or positive skew\nThe right plot shows there’s much less skew after the square root has been taken.\n\nOur conclusion is that a Normal model is a far better fit to the square root of the energy values than it is to the raw energy values.\nThe effect of taking the square root may be clearer from the histograms below, with Normal models superimposed.\n\nres &lt;- mosaic::favstats(~ energy, data = nnyfs)\nbin_w &lt;- 250 # specify binwidth\n\np1 &lt;- ggplot(nnyfs, aes(x = energy)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"coral\", \n                   col = \"white\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"black\", linewidth = 1.5) +\nlabs(x = \"Energy (kcal consumed)\", y = \"\",\n     title = \"Energy Consumed\")\n\n\nres2 &lt;- mosaic::favstats(~ sqrt(energy), data = nnyfs)\nbin_w2 &lt;- 5 # specify binwidth\n\np2 &lt;- ggplot(nnyfs, aes(x = sqrt(energy))) +\n    geom_histogram(binwidth = bin_w2, \n                   fill = \"darkcyan\", \n                   col = \"white\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res2$mean, \n                                sd = res2$sd) * \n            res2$n * bin_w2,\n        col = \"black\", linewidth = 1.5) +\nlabs(x = \"Square Root of Energy\", y = \"\",\n     title = \"Square Root of Energy\")\n\n\np1 + p2 + plot_annotation(title = \"Comparing energy to sqrt(energy)\")\n\n\n\n\n\n\nrm(p1, p2, bin_w, bin_w2, res, res2) # cleanup\n\nWhen we are confronted with a variable that is not Normally distributed but that we wish was Normally distributed, it is sometimes useful to consider whether working with a transformation of the data will yield a more helpful result, as the square root does in this instance.\nThe rest of this Chapter provides some guidance about choosing from a class of power transformations that can reduce the impact of non-Normality in unimodal data.\n\nWhen we are confronted with a variable that is not Normally distributed but that we wish was Normally distributed, it is sometimes useful to consider whether working with a transformation of the data will yield a more helpful result.\nMany statistical methods, including t tests and analyses of variance, assume Normal distributions.\nWe’ll discuss using R to assess a range of what are called Box-Cox power transformations, via plots, mainly.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing Normality</span>"
    ]
  },
  {
    "objectID": "11-normality.html#the-ladder-of-power-transformations",
    "href": "11-normality.html#the-ladder-of-power-transformations",
    "title": "11  Assessing Normality",
    "section": "\n11.10 The Ladder of Power Transformations",
    "text": "11.10 The Ladder of Power Transformations\nThe key notion in re-expression of a single variable to obtain a distribution better approximated by the Normal or re-expression of an outcome in a simple regression model is that of a ladder of power transformations, which applies to any unimodal data.\n\n\nPower\nTransformation\n\n\n\n3\nx3\n\n\n\n2\nx2\n\n\n\n1\nx (unchanged)\n\n\n0.5\nx0.5 = \\(\\sqrt{x}\\)\n\n\n\n0\nln x\n\n\n-0.5\nx-0.5 = 1/\\(\\sqrt{x}\\)\n\n\n\n-1\nx-1 = 1/x\n\n\n-2\nx-2 = 1/x2",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing Normality</span>"
    ]
  },
  {
    "objectID": "11-normality.html#using-the-ladder",
    "href": "11-normality.html#using-the-ladder",
    "title": "11  Assessing Normality",
    "section": "\n11.11 Using the Ladder",
    "text": "11.11 Using the Ladder\nAs we move further away from the identity function (power = 1) we change the shape more and more in the same general direction.\n\nFor instance, if we try a logarithm, and this seems like too much of a change, we might try a square root instead.\nNote that this ladder (which like many other things is due to John Tukey) uses the logarithm for the “power zero” transformation rather than the constant, which is what x0 actually is.\nIf the variable x can take on negative values, we might take a different approach. If x is a count of something that could be zero, we often simply add 1 to x before transformation.\n\nThe ladder of power transformations is particularly helpful when we are confronted with data that shows skew.\n\nTo handle right skew (where the mean exceeds the median) we usually apply powers below 1.\nTo handle left skew (where the median exceeds the mean) we usually apply powers greater than 1.\n\nThe most common transformations are the square (power 2), the square root (power 1/2), the logarithm (power 0) and the inverse (power -1), and I usually restrict myself to those options in practical work.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing Normality</span>"
    ]
  },
  {
    "objectID": "11-normality.html#protein-consumption-in-the-nnyfs-data",
    "href": "11-normality.html#protein-consumption-in-the-nnyfs-data",
    "title": "11  Assessing Normality",
    "section": "\n11.12 Protein Consumption in the NNYFS data",
    "text": "11.12 Protein Consumption in the NNYFS data\nHere are the protein consumption (in grams) results from the NNYFS data.\n\nmosaic::favstats(~ protein, data = nnyfs)\n\n  min    Q1 median     Q3    max     mean       sd    n missing\n 4.18 45.33 61.255 82.565 241.84 66.90148 30.96319 1518       0\n\n\n\np1 &lt;- ggplot(nnyfs, aes(x = \"Protein\", y = protein)) +\n    geom_violin() +\n    geom_boxplot(width = 0.2, fill = \"salmon\", \n                 outlier.color = \"red\") +\n    \n    labs(title = \"NNYFS Protein consumption\",\n         x = \"\", y = \"Protein Consumption (g)\")\n\np2 &lt;- ggplot(nnyfs, aes(sample = protein)) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    \n    labs(title = \"NNYFS Protein Consumption\",\n         y = \"Protein Consumption (g)\")\n\np1 + p2\n\n\n\n\n\n\n\nThe key point here is that we see several signs of meaningful right skew, and we’ll want to consider a transformation that might make a Normal model more plausible.\n\n11.12.1 Using patchwork to compose plots\nAs we mentioned previously, I feel that the slickest approach to composing how a series of plots are placed together is available in the patchwork package. Here’s another example.\n\nres &lt;- mosaic::favstats(~ protein, data = nnyfs)\nbin_w &lt;- 5 # specify binwidth\n\np1 &lt;- ggplot(nnyfs, aes(x = protein)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"salmon\", \n                   col = \"white\") +\n    \n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"darkred\", linewidth = 2) +\n    labs(title = \"Histogram with Normal fit\", \n         x = \"Protein Consumption (g)\", y = \"# of subjects\")\n\n\np2 &lt;- ggplot(nnyfs, aes(sample = protein)) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    \n    labs(title = \"Normal Q-Q plot\",\n         y = \"Protein Consumption (g)\")\n\np3 &lt;- ggplot(nnyfs, aes(x = \"\", y = protein)) +\n    geom_violin() +\n    geom_boxplot(width = 0.2, fill = \"salmon\", \n                 outlier.color = \"red\") +\n    \n    coord_flip() +\n    labs(title = \"Boxplot with Violin\",\n         x = \"\", y = \"Protein Consumption (g)\")\n\np1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) +\n    plot_annotation(title = \"NNYFS Protein Consumption\")\n\n\n\n\n\n\n\nAgain, the patchwork package repository at https://patchwork.data-imaginist.com/index.html has lots of nice examples to work from.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing Normality</span>"
    ]
  },
  {
    "objectID": "11-normality.html#can-we-transform-the-protein-data",
    "href": "11-normality.html#can-we-transform-the-protein-data",
    "title": "11  Assessing Normality",
    "section": "\n11.13 Can we transform the protein data?",
    "text": "11.13 Can we transform the protein data?\nAs we’ve seen, the protein data are right skewed, and all of the values are strictly positive. If we want to use the tools of the Normal distribution to describe these data, we might try taking a step “down” our ladder from power 1 (raw data) to lower powers.\n\n11.13.1 The Square Root\nWould a square root applied to the protein data help alleviate that right skew?\n\nres &lt;- mosaic::favstats(~ sqrt(protein), data = nnyfs)\nbin_w &lt;- 1 # specify binwidth\n\np1 &lt;- ggplot(nnyfs, aes(x = sqrt(protein))) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"salmon\", \n                   col = \"white\") +\n    \n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"darkred\", linewidth = 2) +\n    labs(title = \"Histogram with Normal fit\", \n         x = \"Square Root of Protein Consumption (g)\", y = \"# of subjects\")\n\n\np2 &lt;- ggplot(nnyfs, aes(sample = sqrt(protein))) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    \n    labs(title = \"Normal Q-Q plot\",\n         y = \"Square Root of Protein Consumption (g)\")\n\np3 &lt;- ggplot(nnyfs, aes(x = \"\", y = sqrt(protein))) +\n    geom_violin() +\n    geom_boxplot(width = 0.2, fill = \"salmon\", \n                 outlier.color = \"red\") +\n    \n    coord_flip() +\n    labs(title = \"Boxplot with Violin\",\n         x = \"\", y = \"Square Root of Protein Consumption (g)\")\n\np1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) +\n    plot_annotation(title = \"Square Root of NNYFS Protein Consumption\")\n\n\n\n\n\n\n\nThat looks like a more symmetric distribution, certainly, although we still have some outliers on the right side of the distribution. Should we take another step down the ladder?\n\n11.13.2 The Logarithm\nWe might also try a logarithm of the energy circumference data. We can use either the natural logarithm (log, in R) or the base-10 logarithm (log10, in R) - either will have the same impact on skew.\n\nres &lt;- mosaic::favstats(~ log(protein), data = nnyfs)\nbin_w &lt;- 0.5 # specify binwidth\n\np1 &lt;- ggplot(nnyfs, aes(x = log(protein))) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"salmon\", \n                   col = \"white\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"darkred\", linewidth = 2) +\n    labs(title = \"Histogram with Normal fit\", \n         x = \"Log of Protein Consumption (g)\", y = \"# of subjects\")\n\n\np2 &lt;- ggplot(nnyfs, aes(sample = log(protein))) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Normal Q-Q plot\",\n         y = \"Log of Protein Consumption (g)\")\n\np3 &lt;- ggplot(nnyfs, aes(x = \"\", y = log(protein))) +\n    geom_violin() +\n    geom_boxplot(width = 0.2, fill = \"salmon\", \n                 outlier.color = \"red\") +\n    coord_flip() +\n    labs(title = \"Boxplot with Violin\",\n         x = \"\", y = \"Log of Protein Consumption (g)\")\n\np1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) +\n    plot_annotation(title = \"Logarithm of NNYFS Protein Consumption\")\n\n\n\n\n\n\n\nNow, it looks like we may have gone too far in the other direction. It looks like the square root is a sensible choice to try to improve the fit of a Normal model to the protein consumption data.\n\n11.13.3 This course uses Natural Logarithms, unless otherwise specified\nIn this course, we will assume the use of natural logarithms unless we specify otherwise. Following R’s convention, we will use log for natural logarithms.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing Normality</span>"
    ]
  },
  {
    "objectID": "11-normality.html#what-if-we-considered-all-9-available-transformations",
    "href": "11-normality.html#what-if-we-considered-all-9-available-transformations",
    "title": "11  Assessing Normality",
    "section": "\n11.14 What if we considered all 9 available transformations?",
    "text": "11.14 What if we considered all 9 available transformations?\n\np1 &lt;- ggplot(nnyfs, aes(sample = protein^3)) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Cube (power 3)\",\n         y = \"Protein, Cubed\")\n\np2 &lt;- ggplot(nnyfs, aes(sample = protein^2)) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Square (power 2)\",\n         y = \"Protein, Squared\")\n\np3 &lt;- ggplot(nnyfs, aes(sample = protein)) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Original Data\",\n         y = \"Protein (g)\")\n\n\np4 &lt;- ggplot(nnyfs, aes(sample = sqrt(protein))) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"sqrt (power 0.5)\",\n         y = \"Square Root of Protein\")\n\np5 &lt;- ggplot(nnyfs, aes(sample = log(protein))) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"log (power 0)\",\n         y = \"Natural Log of Protein\")\n\np6 &lt;- ggplot(nnyfs, aes(sample = protein^(-0.5))) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"1/sqrt (power -0.5)\",\n         y = \"1/Square Root(Protein)\")\n\n\np7 &lt;- ggplot(nnyfs, aes(sample = 1/protein)) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Inverse (power -1)\",\n         y = \"1/Protein\")\n\np8 &lt;- ggplot(nnyfs, aes(sample = 1/(protein^2))) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"1/Square (power -2)\",\n         y = \"1 /(Protein squared)\")\n\np9 &lt;- ggplot(nnyfs, aes(sample = 1/(protein^3))) +\n    geom_qq(col = \"salmon\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"1/Cube (power -3)\",\n         y = \"1/(Protein cubed)\")\n\n\np1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 +\n    plot_layout(nrow = 3) +\n    plot_annotation(title = \"Transformations of NNYFS Protein Consumption\")\n\n\n\n\n\n\n\nThe square root still appears to be the best choice of transformation here, even after we consider all 8 transformation of the raw data.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing Normality</span>"
    ]
  },
  {
    "objectID": "11-normality.html#a-simulated-data-set",
    "href": "11-normality.html#a-simulated-data-set",
    "title": "11  Assessing Normality",
    "section": "\n11.15 A Simulated Data Set",
    "text": "11.15 A Simulated Data Set\n\nset.seed(431); \nsim_2 &lt;- \n    tibble(sample2 = 100*rbeta(n = 125, shape1 = 5, shape2 = 2))\n\nIf we’d like to transform these data so as to better approximate a Normal distribution, where should we start? What transformation do you suggest?\n\nres &lt;- mosaic::favstats(~ sample2, data = sim_2)\nbin_w &lt;- 4 # specify binwidth\n\np1 &lt;- ggplot(sim_2, aes(x = sample2)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"royalblue\", \n                   col = \"white\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"darkred\", linewidth = 2) +\n    labs(title = \"Histogram with Normal fit\", \n         x = \"Simulated Data\", y = \"# of subjects\")\n\n\np2 &lt;- ggplot(sim_2, aes(sample = sample2)) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Normal Q-Q plot\",\n         y = \"Simulated Data\")\n\np3 &lt;- ggplot(sim_2, aes(x = \"\", y = sample2)) +\n    geom_violin() +\n    geom_boxplot(width = 0.3, fill = \"royalblue\", \n                 outlier.color = \"royalblue\") +\n    coord_flip() +\n    labs(title = \"Boxplot with Violin\",\n         x = \"\", y = \"Simulated Data\")\n\np1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) +\n    plot_annotation(title = \"Simulated Data\")\n\n\n\n\n\n\n\nGiven the left skew in the data, it looks like a step up in the ladder is warranted, perhaps by looking at the square of the data?\n\nres &lt;- mosaic::favstats(~ sample2^2, data = sim_2)\nbin_w &lt;- 600 # specify binwidth\n\np1 &lt;- ggplot(sim_2, aes(x = sample2^2)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"royalblue\", \n                   col = \"white\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"darkred\", size = 2) +\n    labs(title = \"Histogram with Normal fit\", \n         x = \"Squared Simulated Data\", y = \"# of subjects\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\np2 &lt;- ggplot(sim_2, aes(sample = sample2^2)) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Normal Q-Q plot\",\n         y = \"Squared Simulated Data\")\n\np3 &lt;- ggplot(sim_2, aes(x = \"\", y = sample2^2)) +\n    geom_violin() +\n    geom_boxplot(width = 0.3, fill = \"royalblue\", \n                 outlier.color = \"royalblue\") +\n    coord_flip() +\n    labs(title = \"Boxplot with Violin\",\n         x = \"\", y = \"Squared Simulated Data\")\n\np1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) +\n    plot_annotation(title = \"Squared Simulated Data\")\n\n\n\n\n\n\n\nLooks like at best a modest improvement. How about cubing the data, instead?\n\nres &lt;- mosaic::favstats(~ sample2^3, data = sim_2)\nbin_w &lt;- 100000 # specify binwidth\n\np1 &lt;- ggplot(sim_2, aes(x = sample2^3)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"royalblue\", \n                   col = \"white\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"darkred\", linewidth = 2) +\n    labs(title = \"Histogram with Normal fit\", \n         x = \"Cubed Simulated Data\", y = \"# of subjects\")\n\n\np2 &lt;- ggplot(sim_2, aes(sample = sample2^3)) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Normal Q-Q plot\",\n         y = \"Cubed Simulated Data\")\n\np3 &lt;- ggplot(sim_2, aes(x = \"\", y = sample2^3)) +\n    geom_violin() +\n    geom_boxplot(width = 0.3, fill = \"royalblue\", \n                 outlier.color = \"royalblue\") +\n    coord_flip() +\n    labs(title = \"Boxplot with Violin\",\n         x = \"\", y = \"Cubed Simulated Data\")\n\np1 + p2 - p3 + plot_layout(ncol = 1, height = c(3, 1)) +\n    plot_annotation(title = \"Cubed Simulated Data\")\n\n\n\n\n\n\n\nThe newly transformed (cube of the) data appears more symmetric, although somewhat light-tailed. Perhaps a Normal model would be more appropriate now, although the standard deviation is likely to overstate the variation we see in the data due to the light tails. Again, I wouldn’t be thrilled using a cube in practical work, as it is so hard to interpret, but it does look like a reasonable choice here.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing Normality</span>"
    ]
  },
  {
    "objectID": "11-normality.html#what-if-we-considered-all-9-available-transformations-1",
    "href": "11-normality.html#what-if-we-considered-all-9-available-transformations-1",
    "title": "11  Assessing Normality",
    "section": "\n11.16 What if we considered all 9 available transformations?",
    "text": "11.16 What if we considered all 9 available transformations?\n\np1 &lt;- ggplot(sim_2, aes(sample = sample2^3)) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Cube (power 3)\")\n\np2 &lt;- ggplot(sim_2, aes(sample = sample2^2)) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Square (power 2)\")\n\np3 &lt;- ggplot(sim_2, aes(sample = sample2)) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Original Data\")\n\np4 &lt;- ggplot(sim_2, aes(sample = sqrt(sample2))) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"sqrt (power 0.5)\")\n\np5 &lt;- ggplot(sim_2, aes(sample = log(sample2))) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"log (power 0)\")\n\np6 &lt;- ggplot(sim_2, aes(sample = sample2^(0.5))) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"1/sqrt (power -0.5)\")\n\np7 &lt;- ggplot(sim_2, aes(sample = 1/sample2)) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"Inverse (power -1)\")\n\np8 &lt;- ggplot(sim_2, aes(sample = 1/(sample2^2))) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"1/Square (power -2)\")\n\np9 &lt;- ggplot(sim_2, aes(sample = 1/(sample2^3))) +\n    geom_qq(col = \"royalblue\") + \n    geom_qq_line(col = \"black\") +\n    labs(title = \"1/Cube (power -3)\")\n\np1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 +\n    plot_layout(nrow = 3) +\n    plot_annotation(title = \"Transformations of Simulated Sample\")\n\n\n\n\n\n\n\nAgain, either the cube or the square looks like best choice here, in terms of creating a more symmetric (albeit light-tailed) distribution.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing Normality</span>"
    ]
  },
  {
    "objectID": "11-normality.html#coming-up",
    "href": "11-normality.html#coming-up",
    "title": "11  Assessing Normality",
    "section": "\n11.17 Coming Up",
    "text": "11.17 Coming Up\nNext, we’ll spend some time thinking about scatterplots, correlation and straight line regression models.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing Normality</span>"
    ]
  },
  {
    "objectID": "12-straightlinemodels.html",
    "href": "12-straightlinemodels.html",
    "title": "\n12  Straight Line Models\n",
    "section": "",
    "text": "12.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(broom)\nlibrary(janitor)\nlibrary(kableExtra)\nlibrary(modelsummary)\nlibrary(patchwork)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Straight Line Models</span>"
    ]
  },
  {
    "objectID": "12-straightlinemodels.html#assessing-a-scatterplot",
    "href": "12-straightlinemodels.html#assessing-a-scatterplot",
    "title": "\n12  Straight Line Models\n",
    "section": "\n12.2 Assessing A Scatterplot",
    "text": "12.2 Assessing A Scatterplot\nLet’s consider the relationship of protein and fat consumption for children in the nnyfs data.\n\nnnyfs &lt;- read_rds(\"data/nnyfs.Rds\")\n\nWe’ll begin our investigation, as we always should, by drawing a relevant picture. For the association of two quantitative variables, a scatterplot is usually the right start. Each subject in the nnyfs data is represented by one of the points below. To the plot, I’ve also used geom_smooth to add a straight line regression model, which we’ll discuss later.\n\nggplot(data = nnyfs, aes(x = protein, y = fat)) +\n    geom_point(shape = 1, size = 2, col = \"sienna\") +\n    geom_smooth(method = \"lm\", formula = y ~ x, \n                se = FALSE, col = \"steelblue\") +\n    labs(title = \"Protein vs. Fat consumption in NNYFS data\",\n         subtitle = \"with fitted straight line regression model\",\n         x = \"Protein (in g)\", y = \"Fat (in g)\")\n\n\n\n\n\n\n\nHere, I’ve arbitrarily decided to place fat on the vertical axis, and protein on the horizontal. Fitting a prediction model to this scatterplot will then require that we predict fat on the basis of protein.\nIn this case, the pattern appears to be:\n\n\ndirect, or positive, in that the values of the \\(x\\) variable (protein) increase, so do the values of the \\(y\\) variable (fat). Essentially, it appears that subjects who consumed more protein also consumed more fat, but we don’t know cause and effect here.\nfairly linear in that most of the points cluster around what appears to be a pattern which is well-fitted by a straight line.\nmoderately strong in that the range of values for fat associated with any particular value of protein is fairly tight. If we know someone’s protein consumption, that should meaningfully improve our ability to predict their fat consumption, among the subjects in these data.\nthat we see some unusual or outlier values, further away from the general pattern of most subjects shown in the data.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Straight Line Models</span>"
    ]
  },
  {
    "objectID": "12-straightlinemodels.html#highlighting-an-unusual-point",
    "href": "12-straightlinemodels.html#highlighting-an-unusual-point",
    "title": "\n12  Straight Line Models\n",
    "section": "\n12.3 Highlighting an unusual point",
    "text": "12.3 Highlighting an unusual point\nConsider the subject with protein consumption close to 200 g, whose fat consumption is below 100 g. That’s well below the prediction of the linear model for example. We can identify the subject because it is the only person with protein &gt; 190 and fat &lt; 100 with BMI &gt; 35 and waist.circ &lt; 70. So I’ll create a subset of the nnyfs data containing the point that meets that standard, and then add a red point and a label to the plot.\n\n# identify outlier and place it in nnyfs_temp1 tibble\nnnyfs_temp1 &lt;- nnyfs |&gt;\n  filter(protein &gt; 190 & fat &lt; 100)\n\nggplot(data = nnyfs, aes(x = protein, y = fat)) +\n    geom_point(shape = 1, size = 2, col = \"sienna\") +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x, col = \"steelblue\") +\n    geom_point(data = nnyfs_temp1, size = 2, col = \"red\") +\n    geom_text(data = nnyfs_temp1, label = nnyfs_temp1$SEQN, \n              vjust = -1, col = \"red\") +\n    labs(title = \"Protein vs. Fat consumption in NNYFS\",\n         subtitle = \"with regression line, and an outlier labeled by SEQN\",\n         x = \"Protein (in g)\", y = \"Fat (in g)\")\n\n\n\n\n\n\n\nWhile this subject is hardly the only unusual point in the data set, it is one of the more unusual ones, in terms of its vertical distance from the regression line. We can identify the subject by printing (part of) the tibble we created.\n\nnnyfs_temp1 |&gt; \n  select(SEQN, sex, race_eth, age_child, protein, fat) |&gt; kable()\n\n\n\nSEQN\nsex\nrace_eth\nage_child\nprotein\nfat\n\n\n71919\nFemale\n2_White Non-Hispanic\n14\n199.33\n86.08\n\n\n\n\nNow, does it seem to you like a straight line model will describe this protein-fat relationship well?",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Straight Line Models</span>"
    ]
  },
  {
    "objectID": "12-straightlinemodels.html#adding-a-scatterplot-smooth-using-loess",
    "href": "12-straightlinemodels.html#adding-a-scatterplot-smooth-using-loess",
    "title": "\n12  Straight Line Models\n",
    "section": "\n12.4 Adding a Scatterplot Smooth using loess",
    "text": "12.4 Adding a Scatterplot Smooth using loess\nNext, we’ll use the loess procedure to fit a smooth curve to the data, which attempts to capture the general pattern.\n\nggplot(data = nnyfs, aes(x = protein, y = fat)) +\n    geom_point(shape = 1, size = 2, col = \"sienna\") +\n    geom_smooth(method = \"loess\", se = FALSE, formula = y ~ x, col = \"red\") +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x, col = \"steelblue\") +\n    labs(title = \"Protein vs. Fat consumption in NNYFS\",\n         subtitle = \"with loess smooth (red) and linear fit (blue)\",\n         x = \"Protein (in g)\", y = \"Fat (in g)\")\n\n\n\n\n\n\n\nThis “loess” smooth curve is fairly close to the straight line fit, indicating that perhaps a linear regression model might fit the data well.\nA loess smooth is a method of fitting a local polynomial regression model that R uses as its default smooth for scatterplots with fewer than 1000 observations. Think of the loess as a way of fitting a curve to data by tracking (at point x) the points within a neighborhood of point x, with more emphasis given to points near x. It can be adjusted by tweaking two specific parameters, in particular:\n\na span parameter (defaults to 0.75) which is also called \\(\\alpha\\) in the literature, that controls the degree of smoothing (essentially, how large the neighborhood should be), and\na degree parameter (defaults to 2) which specifies the degree of polynomial to be used. Normally, this is either 1 or 2 - more complex functions are rarely needed for simple scatterplot smoothing.\n\nIn addition to the curve, smoothing procedures can also provide confidence intervals around their main fitted line. Consider the following plot, which adjusts the span and also adds in the confidence intervals.\n\np1 &lt;- ggplot(data = nnyfs, aes(x = protein, y = fat)) +\n    geom_point(shape = 1, size = 2, col = \"sienna\") +\n    geom_smooth(method = \"loess\", span = 0.75, se = TRUE, \n                col = \"red\", formula = y ~ x) +\n    labs(title = \"loess smooth (span = 0.75)\",\n         x = \"Protein (in g)\", y = \"Fat (in g)\")\n\np2 &lt;- ggplot(data = nnyfs, aes(x = protein, y = fat)) +\n    geom_point(shape = 1, size = 2, col = \"sienna\") +\n    geom_smooth(method = \"loess\", span = 0.2, se = TRUE, \n                col = \"red\", formula = y ~ x) +\n    labs(title = \"loess smooth (span = 0.2)\",\n         x = \"Protein (in g)\", y = \"Fat (in g)\")\n\np1 + p2 + \n    plot_annotation(title = \"Impact of adjusting loess smooth span: NNYFS\")\n\n\n\n\n\n\n\nBy reducing the size of the span, the plot on the right shows a somewhat less “smooth” function than the plot on the left.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Straight Line Models</span>"
    ]
  },
  {
    "objectID": "12-straightlinemodels.html#equation-for-a-linear-model",
    "href": "12-straightlinemodels.html#equation-for-a-linear-model",
    "title": "\n12  Straight Line Models\n",
    "section": "\n12.5 Equation for a Linear Model",
    "text": "12.5 Equation for a Linear Model\nReturning to the linear regression model, how can we, mathematically, characterize that line? As with any straight line, our model equation requires us to specify two parameters: a slope and an intercept (sometimes called the y-intercept.)\nTo identify the equation R used to fit this line (using the method of least squares), we use the lm command\n\nm &lt;- lm(fat ~ protein, data = nnyfs)\nm\n\n\nCall:\nlm(formula = fat ~ protein, data = nnyfs)\n\nCoefficients:\n(Intercept)      protein  \n    18.8945       0.7251  \n\n\nSo the fitted line contained in model m can be specified as\n\\[\n\\operatorname{\\widehat{fat}} = 18.89 + 0.73(\\operatorname{protein})\n\\]",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Straight Line Models</span>"
    ]
  },
  {
    "objectID": "12-straightlinemodels.html#summarizing-the-fit-of-a-linear-model",
    "href": "12-straightlinemodels.html#summarizing-the-fit-of-a-linear-model",
    "title": "\n12  Straight Line Models\n",
    "section": "\n12.6 Summarizing the Fit of a Linear Model",
    "text": "12.6 Summarizing the Fit of a Linear Model\nA detailed summary of the fitted linear regression model is also available.\n\nsummary(m)\n\n\nCall:\nlm(formula = fat ~ protein, data = nnyfs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-77.798 -14.841  -2.449  13.601 110.597 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  18.8945     1.5330   12.32   &lt;2e-16 ***\nprotein       0.7251     0.0208   34.87   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.08 on 1516 degrees of freedom\nMultiple R-squared:  0.4451,    Adjusted R-squared:  0.4447 \nF-statistic:  1216 on 1 and 1516 DF,  p-value: &lt; 2.2e-16\n\n\n\n12.6.1 Using modelsummary()\n\nAnother available approach to provide a summary is to use the modelsummary() function from the modelsummary package. Although this is mostly used for comparing multiple models, we can obtain good results for just one, like this.\n\nmodelsummary(m)\n\n \n\n  \n    \n\ntinytable_yq21ser598oc28k4rris\n\n\n      \n\n \n                (1)\n              \n\n\n(Intercept)\n                  18.895   \n                \n\n           \n                  (1.533)  \n                \n\nprotein    \n                  0.725    \n                \n\n           \n                  (0.021)  \n                \n\nNum.Obs.   \n                  1518     \n                \n\nR2         \n                  0.445    \n                \n\nR2 Adj.    \n                  0.445    \n                \n\nAIC        \n                  14094.2  \n                \n\nBIC        \n                  14110.1  \n                \n\nLog.Lik.   \n                  -7044.082\n                \n\nF          \n                  1215.779 \n                \n\nRMSE       \n                  25.06    \n                \n\n\n\n\n    \n\n\nThe modelsummary vignette provides a lot of information on how to amplify these results.\n\n12.6.2 Plotting coefficients with modelplot()\n\nAlso from the modelsummary package, the modelplot() function can be of some help in understanding the size of our coefficients (and their standard errors).\n\nmodelplot(m) +\n  labs(title = \"Model m coefficients\")",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Straight Line Models</span>"
    ]
  },
  {
    "objectID": "12-straightlinemodels.html#summaries-with-the-broom-package",
    "href": "12-straightlinemodels.html#summaries-with-the-broom-package",
    "title": "\n12  Straight Line Models\n",
    "section": "\n12.7 Summaries with the broom package",
    "text": "12.7 Summaries with the broom package\nIf we want to use them to do anything else, the way we’ll usually summarize the estimated coefficients of a linear model is to use the broom package’s tidy function to put the coefficient estimates into a tibble.\n\ntidy(lm(fat ~ protein, data = nnyfs),\n            conf.int = TRUE, conf.level = 0.95) |&gt;\n    kbl(digits = 3) |&gt;\n    kable_styling(full_width = FALSE)\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n18.895\n1.533\n12.325\n0\n15.887\n21.902\n\n\nprotein\n0.725\n0.021\n34.868\n0\n0.684\n0.766\n\n\n\n\n\nWe can also summarize the quality of fit in a linear model using the broom package’s glance function. For now, we’ll focus our attention on just one of the many summaries available for a linear model from glance: the R-squared value.\n\nglance(lm(fat ~ protein, data = nnyfs)) |&gt; \n  select(r.squared) |&gt;\n  kbl(digits = 3) |&gt;\n  kable_styling(full_width = FALSE)\n\n\n\nr.squared\n\n\n0.445\n\n\n\n\nWe’ll spend a lot of time working with these regression summaries in this course.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Straight Line Models</span>"
    ]
  },
  {
    "objectID": "12-straightlinemodels.html#key-takeaways-from-a-simple-regression",
    "href": "12-straightlinemodels.html#key-takeaways-from-a-simple-regression",
    "title": "\n12  Straight Line Models\n",
    "section": "\n12.8 Key Takeaways from a Simple Regression",
    "text": "12.8 Key Takeaways from a Simple Regression\nFor now, it will suffice to understand the following:\n\nThe outcome variable in this model is fat, and the predictor variable is protein.\nThe straight line model for these data fitted by least squares is fat = 18.895 + 0.725 protein\n\nThe slope of protein is positive, which indicates that as protein increases, we expect that fat will also increase. Specifically, we expect that for every additional gram of protein consumed, the fat consumption will be 0.725 gram larger.\nThe multiple R-squared (squared correlation coefficient) is 0.445, which implies that 44.5% of the variation in fat is explained using this linear model with protein.\nThis also implies that the Pearson correlation between fat and protein is the square root of 0.445, or 0.667. More on the Pearson correlation soon.\n\nSo, if we plan to use a simple (least squares) linear regression model to describe fat consumption as a function of protein consumption in the NNYFS data, does it look like a least squares (or linear regression) model will be an effective choice?\nOne way to study this is through looking at correlation: which is our next subject.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Straight Line Models</span>"
    ]
  },
  {
    "objectID": "13-correlation.html",
    "href": "13-correlation.html",
    "title": "\n13  Correlation\n",
    "section": "",
    "text": "13.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor)\nlibrary(kableExtra)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "13-correlation.html#our-data",
    "href": "13-correlation.html#our-data",
    "title": "\n13  Correlation\n",
    "section": "\n13.2 Our Data",
    "text": "13.2 Our Data\nIn this chapter, we will make use of the nnyfs data, as well as several simulated examples stored on our 431-data page as .csv files.\n\nnnyfs &lt;- read_rds(\"data/nnyfs.Rds\")\n\ncorrex1 &lt;- read_csv(\"data/correx1.csv\", show_col_types = FALSE) \ncorrex2 &lt;- read_csv(\"data/correx2.csv\", show_col_types = FALSE) \ncorrex3 &lt;- read_csv(\"data/correx3.csv\", show_col_types = FALSE)",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "13-correlation.html#measuring-correlation",
    "href": "13-correlation.html#measuring-correlation",
    "title": "\n13  Correlation\n",
    "section": "\n13.3 Measuring Correlation",
    "text": "13.3 Measuring Correlation\nTwo correlation measures are worth our immediate attention.\n\nThe one most often used is called the Pearson correlation coefficient, and is symbolized with the letter r or sometimes the Greek letter rho (\\(\\rho\\)).\nAnother tool is the Spearman rank correlation coefficient, also occasionally symbolized by \\(\\rho\\).\n\nFor the nnyfs data, the Pearson correlation of fat and protein can be found using the cor() function.\n\nnnyfs |&gt; \n  select(fat, protein) |&gt; \n  cor()\n\n              fat   protein\nfat     1.0000000 0.6671209\nprotein 0.6671209 1.0000000\n\n\nNote that the correlation of any variable with itself is 1, and that the correlation of fat with protein is the same regardless of whether you enter fat first or protein first.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "13-correlation.html#the-pearson-correlation-coefficient",
    "href": "13-correlation.html#the-pearson-correlation-coefficient",
    "title": "\n13  Correlation\n",
    "section": "\n13.4 The Pearson Correlation Coefficient",
    "text": "13.4 The Pearson Correlation Coefficient\nSuppose we have \\(n\\) observations on two variables, called X and Y. The Pearson correlation coefficient assesses how well the relationship between X and Y can be described using a linear function.\n\nThe Pearson correlation is dimension-free.\nIt falls between -1 and +1, with the extremes corresponding to situations where all the points in a scatterplot fall exactly on a straight line with negative and positive slopes, respectively.\nA Pearson correlation of zero corresponds to the situation where there is no linear association.\nUnlike the estimated slope in a regression line, the sample correlation coefficient is symmetric in X and Y, so it does not depend on labeling one of them (Y) the response variable, and one of them (X) the predictor.\n\nSuppose we have \\(n\\) observations on two variables, called \\(X\\) and \\(Y\\), where \\(\\bar{X}\\) is the sample mean of \\(X\\) and \\(s_x\\) is the standard deviation of \\(X\\). The Pearson correlation coefficient \\(r_{XY}\\) is:\n\\[\nr_{XY} = \\frac{1}{n-1} \\sum\\limits_{i=1}^n (\\frac{x_i - \\bar{x}}{s_x}) (\\frac{y_i - \\bar{y}}{s_y})\n\\]",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "13-correlation.html#studying-correlation-through-six-examples",
    "href": "13-correlation.html#studying-correlation-through-six-examples",
    "title": "\n13  Correlation\n",
    "section": "\n13.5 Studying Correlation through Six Examples",
    "text": "13.5 Studying Correlation through Six Examples\nThe correx1 data file we read in at the start of this Chapter contains six different sets of (x,y) points, identified by the set variable.\n\nsummary(correx1)\n\n     set                  x                y         \n Length:277         Min.   : 5.897   Min.   : 7.308  \n Class :character   1st Qu.:29.487   1st Qu.:30.385  \n Mode  :character   Median :46.154   Median :46.923  \n                    Mean   :46.529   Mean   :49.061  \n                    3rd Qu.:63.333   3rd Qu.:68.077  \n                    Max.   :98.205   Max.   :95.385  \n\n\n\n13.5.1 Data Set Alex\nLet’s start by working with the Alex data set.\n\nggplot(filter(correx1, set == \"Alex\"), aes(x = x, y = y)) + \n    geom_point() +\n    labs(title = \"correx1: Data Set Alex\")\n\n\n\n\n\n\n\n\nggplot(filter(correx1, set == \"Alex\"), aes(x = x, y = y)) + \n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x, col = \"blue\") +\n    labs(title = \"correx1: Alex, with loess smooth\")\n\n\n\n\n\n\n\n\nsetA &lt;- filter(correx1, set == \"Alex\")\n\nggplot(setA, aes(x = x, y = y)) + \n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    labs(title = \"correx1: Alex, with Fitted Linear Model\") +\n    annotate(\"text\", x = 75, y = 75, col = \"purple\", size = 6,\n             label = paste(\"Pearson r = \", round_half_up(cor(setA$x, setA$y),3))) +\n    annotate(\"text\", x = 50, y = 15,  col = \"red\", size = 5,\n             label = paste(\"y = \", round_half_up(coef(lm(setA$y ~ setA$x))[1],3), \n                           round_half_up(coef(lm(setA$y ~ setA$x))[2],2), \"x\"))\n\n\n\n\n\n\n\n\n13.5.2 Data Set Bonnie\n\nsetB &lt;- filter(correx1, set == \"Bonnie\")\n\nggplot(setB, aes(x = x, y = y)) + \n    geom_point() +\n    labs(title = \"correx1: Data Set Bonnie\")\n\n\n\n\n\n\n\n\nggplot(setB, aes(x = x, y = y)) + \n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    labs(title = \"correx1: Bonnie, with Fitted Linear Model\") +\n    annotate(\"text\", x = 25, y = 65, col = \"purple\", size = 6,\n             label = paste(\"Pearson r = \", round_half_up(cor(setB$x, setB$y),2))) +\n    annotate(\"text\", x = 50, y = 15,  col = \"red\", size = 5,\n             label = paste(\"y = \", round_half_up(coef(lm(setB$y ~ setB$x))[1],3), \n                           \" + \",\n                           round_half_up(coef(lm(setB$y ~ setB$x))[2],2), \"x\"))\n\n\n\n\n\n\n\n\n13.5.3 Correlations for All Six Data Sets in correx1\n\nLet’s look at the Pearson correlations associated with each of the six data sets contained in the correx1 example.\n\ntab1 &lt;- correx1 |&gt;\n    group_by(set) |&gt;\n    summarise(\"Pearson r\" = round_half_up(cor(x, y, use=\"complete\"),2))\n\ntab1 |&gt;\n  kbl() |&gt;\n  kable_styling(full_width = FALSE)\n\n\n\nset\nPearson r\n\n\n\nAlex\n-0.97\n\n\nBonnie\n0.80\n\n\nColin\n-0.80\n\n\nDanielle\n0.00\n\n\nEarl\n-0.01\n\n\nFiona\n0.00\n\n\n\n\n\n\n13.5.4 Data Set Colin\nIt looks like the picture for Colin should be very similar (in terms of scatter) to the picture for Bonnie, except that Colin will have a negative slope, rather than the positive one Bonnie has. Is that how this plays out?\n\nsetBC &lt;- filter(correx1, set == \"Bonnie\" | set == \"Colin\")\n\nggplot(setBC, aes(x = x, y = y)) + \n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    facet_wrap(~ set)\n\n\n\n\n\n\n\nUh, oh. It looks like the point in Colin at the top right is twisting what would otherwise be a very straight regression model with an extremely strong negative correlation. There’s no better way to look for outliers than to examine the scatterplot.\n\n13.5.5 Draw the Picture!\nWe’ve seen that Danielle, Earl and Fiona all show Pearson correlations of essentially zero. However, the three data sets look very different in a scatterplot.\n\nggplot(correx1, aes(x = x, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x) +\n    facet_wrap(~ set)\n\n\n\n\n\n\n\nWhen we learn that the correlation is zero, we tend to assume we have a picture like the Danielle data set. If Danielle were our real data, we might well think that x would be of little use in predicting y.\n\nBut what if our data looked like Earl? In the Earl data set, x is incredibly helpful in predicting y, but we can’t use a straight line model - instead, we need a non-linear modeling approach.\nYou’ll recall that the Fiona data set also had a Pearson correlation of zero. But here, the picture is rather more interesting.\n\nSo, remember, draw the appropriate scatterplot whenever you make use of a correlation coefficient.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "13-correlation.html#estimating-correlation-from-scatterplots",
    "href": "13-correlation.html#estimating-correlation-from-scatterplots",
    "title": "\n13  Correlation\n",
    "section": "\n13.6 Estimating Correlation from Scatterplots",
    "text": "13.6 Estimating Correlation from Scatterplots\nThe correx2 data set is designed to help you calibrate yourself a bit in terms of estimating a correlation from a scatterplot. There are 11 data sets buried within the correx2 example, and they are labeled by their Pearson correlation coefficients, ranging from r = 0.01 to r = 0.999\n\ncorrex2 |&gt;\n    group_by(set) |&gt;\n    summarise(cor = round(cor(x, y, use=\"complete\"),3))\n\n# A tibble: 11 × 2\n   set       cor\n   &lt;chr&gt;   &lt;dbl&gt;\n 1 Set 01  0.01 \n 2 Set 10  0.102\n 3 Set 20  0.202\n 4 Set 30  0.301\n 5 Set 40  0.403\n 6 Set 50  0.499\n 7 Set 60  0.603\n 8 Set 70  0.702\n 9 Set 80  0.799\n10 Set 90  0.902\n11 Set 999 0.999\n\n\nHere is a plot of the 11 data sets, showing the increase in correlation from 0.01 (in Set 01) to 0.999 (in Set 999).\n\nggplot(correx2, aes(x = x, y = y)) +\n    geom_point() + \n    facet_wrap(~ set) +\n    labs(title = \"Pearson Correlations from 0.01 to 0.999\")\n\n\n\n\n\n\n\nNote that R will allow you to fit a straight line model to any of these relationships, no matter how appropriate it might be to do so.\n\nggplot(correx2, aes(x = x, y = y)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    facet_wrap(~ set) +\n    labs(title = \"R will fit a straight line to anything.\")\n\n\n\n\n\n\n\n\nggplot(correx2, aes(x = x, y = y)) +\n    geom_point() + \n    geom_smooth(col = \"blue\") +\n    facet_wrap(~ set) +\n    labs(title = \"Even if a loess smooth suggests non-linearity.\")\n\n\n\n\n\n\n\n\nggplot(correx2, aes(x = x, y = y, color = factor(group))) +\n    geom_point() + \n    guides(color = \"none\") +\n    facet_wrap(~ set) +\n    labs(title = \"Note: The same 10 points (in red) are in each plot.\")\n\n\n\n\n\n\n\nNote that the same 10 points are used in each of the data sets. It’s always possible that a lurking subgroup of the data within a scatterplot follows a very strong linear relationship. This is why it’s so important (and difficult) not to go searching for such a thing without a strong foundation of logic, theory and prior empirical evidence.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "13-correlation.html#the-spearman-rank-correlation",
    "href": "13-correlation.html#the-spearman-rank-correlation",
    "title": "\n13  Correlation\n",
    "section": "\n13.7 The Spearman Rank Correlation",
    "text": "13.7 The Spearman Rank Correlation\nThe Spearman rank correlation coefficient is a rank-based measure of statistical dependence that assesses how well the relationship between X and Y can be described using a monotone function even if that relationship is not linear.\n\nA monotone function preserves order, that is, Y must either be strictly increasing as X increases, or strictly decreasing as X increases.\nA Spearman correlation of 1.0 indicates simply that as X increases, Y always increases.\nLike the Pearson correlation, the Spearman correlation is dimension-free, and falls between -1 and +1.\nA positive Spearman correlation corresponds to an increasing (but not necessarily linear) association between X and Y, while a negative Spearman correlation corresponds to a decreasing (but again not necessarily linear) association.\n\n\n13.7.1 Spearman Formula\nTo calculate the Spearman rank correlation, we take the ranks of the X and Y data, and then apply the usual Pearson correlation. To find the ranks, sort X and Y into ascending order, and then number them from 1 (smallest) to n (largest). In the event of a tie, assign the average rank to the tied subjects.\n\n13.7.2 Comparing Pearson and Spearman Correlations\nLet’s look at the nnyfs data again.\n\nnnyfs |&gt; select(fat, protein) |&gt; cor()\n\n              fat   protein\nfat     1.0000000 0.6671209\nprotein 0.6671209 1.0000000\n\ncor_sp &lt;- nnyfs |&gt; \n  select(fat, protein) \n\ncor_sp |&gt; cor(method = \"spearman\")\n\n              fat   protein\nfat     1.0000000 0.6577489\nprotein 0.6577489 1.0000000\n\n\nThe Spearman and Pearson correlations are not especially different in this case.\n\n13.7.3 Spearman vs. Pearson Example 1\nThe next few plots describe relationships where we anticipate the Pearson and Spearman correlations might differ in their conclusions.\nExample 1 shows a function where the Pearson correlation is 0.925 (a strong but not perfect linear relation), but the Spearman correlation is 1 because the relationship is monotone, even though it is not perfectly linear.\n\nggplot(correx3, aes(x = x1, y = y1)) + \n    geom_point() +\n    labs(title = \"Spearman vs. Pearson, Example 1\") +\n    annotate(\"text\", x = -10, y = 20, \n             label = paste(\"Pearson r = \", \n                 round_half_up(cor(correx3$x1, correx3$y1, \n                                   use = \"complete.obs\"),3),\n                 \", Spearman r = \", \n                 round_half_up(cor(correx3$x1, correx3$y1, method = \"spearman\",\n                                   use = \"complete.obs\"),3)))\n\n\n\n\n\n\n\nSo, a positive Spearman correlation corresponds to an increasing (but not necessarily linear) association between x and y.\n\n13.7.4 Spearman vs. Pearson Example 2\nExample 2 shows that a negative Spearman correlation corresponds to a decreasing (but, again, not necessarily linear) association between x and y.\n\nggplot(correx3, aes(x = x2, y = y2)) + \n    geom_point(col = \"purple\") +\n    geom_smooth(method = \"loess\", formula = y ~ x, se = FALSE) +\n    labs(title = \"Spearman vs. Pearson, Example 2\") +\n    annotate(\"text\", x = 0, y = 20, col = \"purple\",\n             label = paste(\"Pearson r = \", \n                 round_half_up(cor(correx3$x2, correx3$y2, \n                                   use = \"complete.obs\"),2),\n                 \", Spearman r = \", \n                 round_half_up(cor(correx3$x2, correx3$y2, method = \"spearman\",\n                                   use = \"complete.obs\"),2)))\n\n\n\n\n\n\n\n\n13.7.5 Spearman vs. Pearson Example 3\nThe Spearman correlation is less sensitive than the Pearson correlation is to strong outliers that are unusual on either the X or Y axis, or both. That is because the Spearman rank coefficient limits the outlier to the value of its rank.\nIn Example 3, for instance, the Spearman correlation reacts much less to the outliers around X = 12 than does the Pearson correlation.\n\nggplot(correx3, aes(x = x3, y = y3)) + \n    geom_point(col = \"blue\") +\n    labs(title = \"Spearman vs. Pearson, Example 3\") +\n    annotate(\"text\", x = 5, y = -15, col = \"blue\",\n             label = paste(\"Pearson r = \", \n                 round_half_up(cor(correx3$x3, correx3$y3, \n                                   use = \"complete.obs\"),2),\n                 \", Spearman r = \", \n                 round_half_up(cor(correx3$x3, correx3$y3, method = \"spearman\",\n                                   use = \"complete.obs\"),2)))\n\n\n\n\n\n\n\n\n13.7.6 Spearman vs. Pearson Example 4\nThe use of a Spearman correlation is no substitute for looking at the data. For non-monotone data like what we see in Example 4, neither the Spearman nor the Pearson correlation alone provides much guidance, and just because they are (essentially) telling you the same thing, that doesn’t mean what they’re telling you is all that helpful.\n\nggplot(correx3, aes(x = x4, y = y4)) +\n    geom_point(col = \"purple\") +\n    geom_smooth(method = \"loess\", formula = y ~ x, se = FALSE) +\n    labs(title = \"Spearman vs. Pearson, Example 4\") +\n    annotate(\"text\", x = 10, y = 20, col = \"purple\",\n             label = paste(\"Pearson r = \", \n                 round_half_up(cor(correx3$x4, correx3$y4, \n                                   use = \"complete.obs\"),2),\n                 \", Spearman r = \", \n                 round_half_up(cor(correx3$x4, correx3$y4, method = \"spearman\",\n                                   use = \"complete.obs\"),2)))",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "13-correlation.html#coming-up",
    "href": "13-correlation.html#coming-up",
    "title": "\n13  Correlation\n",
    "section": "\n13.8 Coming Up",
    "text": "13.8 Coming Up\nNext, we’ll look at some options for improving the fit of a linear model to a scatterplot that shows a strong, but not a linear association.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "14-linear_trans.html",
    "href": "14-linear_trans.html",
    "title": "\n14  Linearizing Transformations\n",
    "section": "",
    "text": "14.1 Linearize The Association between Quantitative Variables\nConfronted with a scatterplot describing a monotone association between two quantitative variables, we may decide the data are not well approximated by a straight line, and thus, that a least squares regression may not be not sufficiently useful. In these circumstances, we have at least two options, which are not mutually exclusive:",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Linearizing Transformations</span>"
    ]
  },
  {
    "objectID": "14-linear_trans.html#linearize-the-association-between-quantitative-variables",
    "href": "14-linear_trans.html#linearize-the-association-between-quantitative-variables",
    "title": "\n14  Linearizing Transformations\n",
    "section": "",
    "text": "Let the data be as they may, and summarize the scatterplot using tools like loess curves, polynomial functions, or cubic splines to model the relationship.\nConsider re-expressing the data (often we start with re-expressions of the outcome data [the Y variable]) using a transformation so that the transformed data may be modeled effectively using a straight line.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Linearizing Transformations</span>"
    ]
  },
  {
    "objectID": "14-linear_trans.html#setup-packages-used-here",
    "href": "14-linear_trans.html#setup-packages-used-here",
    "title": "\n14  Linearizing Transformations\n",
    "section": "\n14.2 Setup: Packages Used Here",
    "text": "14.2 Setup: Packages Used Here\n\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(broom)\nlibrary(car)\nlibrary(patchwork)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Linearizing Transformations</span>"
    ]
  },
  {
    "objectID": "14-linear_trans.html#the-box-cox-plot",
    "href": "14-linear_trans.html#the-box-cox-plot",
    "title": "\n14  Linearizing Transformations\n",
    "section": "\n14.3 The Box-Cox Plot",
    "text": "14.3 The Box-Cox Plot\nAs before, Tukey’s ladder of power transformations can guide our exploration.\n\n\n\n\n\n\n\n\n\n\n\n\nPower (\\(\\lambda\\))\n-2\n-1\n-1/2\n0\n1/2\n1\n2\n\n\nTransformation\n1/y2\n\n1/y\n1/\\(\\sqrt{y}\\)\n\nlog y\n\\(\\sqrt{y}\\)\ny\ny2\n\n\n\nThe Box-Cox plot, from the boxCox function in the car package, sifts through the ladder of options to suggest a transformation (for Y) to best linearize the outcome-predictor(s) relationship.\n\n14.3.1 A Few Caveats\n\nThese methods work well with monotone data, where a smooth function of Y is either strictly increasing, or strictly decreasing, as X increases.\nSome of these transformations require the data to be positive. We can rescale the Y data by adding a constant to every observation in a data set without changing shape.\nWe can use a natural logarithm (log in R), a base 10 logarithm (log10) or even sometimes a base 2 logarithm (log2) to good effect in Tukey’s ladder. All affect the association’s shape in the same way, so we’ll stick with log (base e).\nSome re-expressions don’t lead to easily interpretable results. Not many things that make sense in their original units also make sense in inverse square roots. There are times when we won’t care, but often, we will.\nIf our primary interest is in making predictions, we’ll generally be more interested in getting good predictions back on the original scale, and we can back-transform the point and interval estimates to accomplish this.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Linearizing Transformations</span>"
    ]
  },
  {
    "objectID": "14-linear_trans.html#a-simulated-example",
    "href": "14-linear_trans.html#a-simulated-example",
    "title": "\n14  Linearizing Transformations\n",
    "section": "\n14.4 A Simulated Example",
    "text": "14.4 A Simulated Example\n\nset.seed(999); \n  x.rand &lt;- rbeta(80, 2, 5) * 20 + 3\nset.seed(1000); \n  y.rand &lt;- abs(50 + 0.75*x.rand^(3) \n                - 0.65*x.rand + rnorm(80, 0, 200))\n\nscatter1 &lt;- tibble(x = x.rand, y = y.rand) \nrm(x.rand, y.rand)\n\nggplot(scatter1, aes(x = x, y = y)) +\n    geom_point(shape = 1, size = 3) +\n    ## add loess smooth\n    geom_smooth(method = \"loess\", se = FALSE, \n                col = \"dodgerblue\", formula = y ~ x) +\n    ## then add linear fit\n    geom_smooth(method = \"lm\", se = FALSE, \n                col = \"red\", formula = y ~ x, linetype = \"dashed\") +\n    labs(title = \"Simulated scatter1 example: Y vs. X\")\n\n\n\n\n\n\n\nHaving simulated data that produces a curved scatterplot, I will now use the Box-Cox plot to lead my choice of an appropriate power transformation for Y in order to “linearize” the association of Y and X.\n\nboxCox(scatter1$y ~ scatter1$x) \n\n\n\n\n\n\npowerTransform(scatter1$y ~ scatter1$x)\n\nEstimated transformation parameter \n       Y1 \n0.4368753 \n\n\nThe Box-Cox plot peaks at the value \\(\\lambda\\) = 0.44, which is pretty close to \\(\\lambda\\) = 0.5. Now, 0.44 isn’t on Tukey’s ladder, but 0.5 is.\n\n\n\n\n\n\n\n\n\n\n\n\nPower (\\(\\lambda\\))\n-2\n-1\n-1/2\n0\n1/2\n1\n2\n\n\nTransformation\n1/y2\n\n1/y\n1/\\(\\sqrt{y}\\)\n\nlog y\n\\(\\sqrt{y}\\)\ny\ny2\n\n\n\nIf we use \\(\\lambda\\) = 0.5, on Tukey’s ladder of power transformations, it suggests we look at the relationship between the square root of Y and X, as shown next.\n\np1 &lt;- ggplot(scatter1, aes(x = x, y = y)) +\n    geom_point(size = 2) +\n    geom_smooth(method = \"loess\", se = FALSE, \n                formula = y ~ x, col = \"dodgerblue\") +\n    geom_smooth(method = \"lm\", se = FALSE, \n                formula = y ~ x, col = \"red\", linetype = \"dashed\") +\n    labs(title = \"scatter1: Y vs. X\")\n\np2 &lt;- ggplot(scatter1, aes(x = x, y = sqrt(y))) +\n    geom_point(size = 2) +\n    geom_smooth(method = \"loess\", se = FALSE, \n                formula = y ~ x, col = \"dodgerblue\") +\n    geom_smooth(method = \"lm\", se = FALSE, \n                formula = y ~ x, col = \"red\", linetype = \"dashed\") +\n    labs(title = \"scatter1: Square Root of Y vs. X\")\n\np1 + p2\n\n\n\n\n\n\n\nBy eye, I think the square root plot better matches the linear fit.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Linearizing Transformations</span>"
    ]
  },
  {
    "objectID": "14-linear_trans.html#checking-on-a-transformation-or-re-expression",
    "href": "14-linear_trans.html#checking-on-a-transformation-or-re-expression",
    "title": "\n14  Linearizing Transformations\n",
    "section": "\n14.5 Checking on a Transformation or Re-Expression",
    "text": "14.5 Checking on a Transformation or Re-Expression\nIn addition to plotting the impact of the transformation, we can do at least two other things to check on our transformation.\n\nWe can calculate the correlation of our original and re-expressed associations.\nWe can go ahead and fit the regression models using each approach and compare the plots of studentized residuals against fitted values from the data to see if the re-expression reduces the curve in that residual plot, as well.\n\nThe last of these options is by far the most important in practice, and it’s the one we’ll focus on going forward, but we’ll demonstrate both of these new approaches here.\n\n14.5.1 Checking the Correlation Coefficients\nHere, we calculate the correlation of original and re-expressed associations.\n\ncor(scatter1$y, scatter1$x)\n\n[1] 0.891198\n\ncor(sqrt(scatter1$y), scatter1$x)\n\n[1] 0.9144307\n\n\nThe Pearson correlation is a little stronger after the transformation. as we’d expect.\n\n14.5.2 Comparing the Residual Plots\nWe can fit the regression models, obtain plots of residuals against fitted values, and compare them to see which one has less indication of a curve in the residuals.\n\nmodel.orig &lt;- lm(scatter1$y ~ scatter1$x)\nmodel.sqrt &lt;- lm(sqrt(scatter1$y) ~ scatter1$x)\n\np1 &lt;- ggplot(augment(model.orig), aes(x = scatter1$x, y = .resid)) + \n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x, se = FALSE) +\n    geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE, col = \"red\") +\n    labs(title = \"Y vs X Residual Plot\")\n\np2 &lt;- ggplot(augment(model.sqrt), aes(x = scatter1$x, y = .resid)) + \n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x, se = FALSE) +\n    geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE, col = \"red\") +\n    labs(title = \"Square Root Model Residuals\")\n\np1 + p2\n\n\n\n\n\n\n\nWhat we’re looking for in such a plot is the absence of a curve, among other things, we want to see “fuzzy football” shapes.\nAs compared to the original residual plot, the square root version, is a modest improvement in this regard. It does look a bit less curved, and a bit more like a random cluster of points, so that’s nice. Usually, we can do a little better in real data, as shown in the next example from the NNYFS data we introduced in Chapter 10.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Linearizing Transformations</span>"
    ]
  },
  {
    "objectID": "14-linear_trans.html#an-example-from-the-nnyfs-data",
    "href": "14-linear_trans.html#an-example-from-the-nnyfs-data",
    "title": "\n14  Linearizing Transformations\n",
    "section": "\n14.6 An Example from the NNYFS data",
    "text": "14.6 An Example from the NNYFS data\n\nnnyfs &lt;- read_rds(\"data/nnyfs.Rds\")\n\nUsing the subjects in the nnyfs data with complete data on the two variables of interest, let’s look at the relationship between arm circumference (the outcome, shown on the Y axis) and arm length (the predictor, shown on the X axis.)\n\nnnyfs_c &lt;- nnyfs |&gt; \n    filter(complete.cases(arm_circ, arm_length)) |&gt;\n    select(SEQN, arm_circ, arm_length)\n\n\n14.6.1 Pearson correlation and scatterplot\nHere is the Pearson correlation between these two variables.\n\nnnyfs_c |&gt; select(arm_length, arm_circ) |&gt; cor()\n\n           arm_length  arm_circ\narm_length  1.0000000 0.8120242\narm_circ    0.8120242 1.0000000\n\n\nHere’s the resulting scatterplot.\n\nggplot(nnyfs_c, aes(x = arm_length, y = arm_circ)) +\n    geom_point(alpha = 0.2) +\n    geom_smooth(method = \"loess\", formula = y ~ x, \n                se = FALSE, color = \"blue\") +\n    geom_smooth(method = \"lm\", formula = y ~ x, \n                se = FALSE, color = \"red\")\n\n\n\n\n\n\n\nWhile the Pearson correlation is still quite strong, note that the loess smooth (shown in blue) bends up from the straight line model (shown in red) at both the low and high end of arm length.\nNote also the use of alpha = 0.2 to show the points with greater transparency than they would be shown normally (the default setting is no transparency with alpha = 1.)\n\n14.6.2 Plotting the Residuals\nNow, let’s build a plot of residuals from the straight line model plotted against the arm length. We can obtain these residuals using the augment() function from the broom package.\n\nm1 &lt;- lm(arm_circ ~ arm_length, data = nnyfs_c)\n\nnnyfs_c_aug1 &lt;- augment(m1, data = nnyfs_c)\n\nnnyfs_c_aug1\n\n# A tibble: 1,511 × 9\n    SEQN arm_circ arm_length .fitted .resid     .hat .sigma   .cooksd .std.resid\n   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 71918     25.4       27.7    21.9  3.51  0.000695   3.21 0.000416       1.09 \n 2 71919     26         38.4    30.4 -4.38  0.00253    3.21 0.00237       -1.37 \n 3 71920     37.9       35.9    28.4  9.50  0.00167    3.20 0.00735        2.96 \n 4 71921     15.1       18.3    14.4  0.669 0.00304    3.21 0.0000663      0.209\n 5 71922     29.5       34.2    27.0  2.45  0.00124    3.21 0.000362       0.764\n 6 71923     27.9       33      26.1  1.80  0.00100    3.21 0.000159       0.562\n 7 71924     17.6       26.5    20.9 -3.34  0.000788   3.21 0.000427      -1.04 \n 8 71925     17.7       24.2    19.1 -1.41  0.00113    3.21 0.000110      -0.441\n 9 71926     19.9       26      20.5 -0.642 0.000844   3.21 0.0000169     -0.200\n10 71927     17.3       20      15.8  1.52  0.00234    3.21 0.000263       0.474\n# ℹ 1,501 more rows\n\n\nOK. So the residuals are now stored in the .resid variable. We can create a residual plot, as follows.\n\nggplot(nnyfs_c_aug1, aes(x = arm_length, y = .resid)) +\n    geom_point(alpha = 0.2) +\n    geom_smooth(method = \"loess\", col = \"purple\",\n                formula = y ~ x, se = FALSE) +\n    labs(title = \"Residuals show a curve.\")\n\n\n\n\n\n\n\n\n14.6.3 Using the Box-Cox approach to identify a transformation\n\nboxCox(nnyfs_c$arm_circ ~ nnyfs_c$arm_length) \n\n\n\n\n\n\npowerTransform(nnyfs_c$arm_circ ~ nnyfs_c$arm_length)\n\nEstimated transformation parameter \n        Y1 \n-0.9783135 \n\n\nThis suggests that we should transform the arm_circ data by taking its inverse (power = -1.) Let’s take a look at that result.\n\n14.6.4 Plots after Inverse Transformation\nLet’s build (on the left) the revised scatterplot and (on the right) the revised residual plot after transforming the outcome (arm_circ) by taking its inverse.\n\nnnyfs_c &lt;- nnyfs_c |&gt;\n    mutate(inv_arm_circ = 1/arm_circ)\n\np1 &lt;- ggplot(nnyfs_c, aes(x = arm_length, y = inv_arm_circ)) +\n    geom_point(alpha = 0.2) +\n    geom_smooth(method = \"loess\", formula = y ~ x, \n                se = FALSE, color = \"blue\") +\n    geom_smooth(method = \"lm\", formula = y ~ x, \n                se = FALSE, color = \"red\") +\n    labs(title = \"Transformation reduces curve\")\n\nm2 &lt;- lm(inv_arm_circ ~ arm_length, data = nnyfs_c)\n\nnnyfs_c_aug2 &lt;- augment(m2, data = nnyfs_c)\n\np2 &lt;- ggplot(nnyfs_c_aug2, aes(x = arm_length, y = .resid)) +\n    geom_point(alpha = 0.2) +\n    geom_smooth(method = \"loess\", col = \"purple\",\n                formula = y ~ x, se = FALSE) +\n    labs(title = \"Residuals much improved\")\n\np1 + p2 + \n    plot_annotation(title = \"Evaluating the Inverse Transformation\")",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Linearizing Transformations</span>"
    ]
  },
  {
    "objectID": "14-linear_trans.html#coming-up",
    "href": "14-linear_trans.html#coming-up",
    "title": "\n14  Linearizing Transformations\n",
    "section": "\n14.7 Coming Up",
    "text": "14.7 Coming Up\nThe rest of Part A of these Course Notes walk through additional case studies, showing some new ideas, but primarily providing context for some tools we’ve already seen.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Linearizing Transformations</span>"
    ]
  },
  {
    "objectID": "15-crabs.html",
    "href": "15-crabs.html",
    "title": "\n15  Studying Crab Claws\n",
    "section": "",
    "text": "15.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(janitor)\nlibrary(broom)\nlibrary(knitr)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\nWe will also use the describe function from the psych package.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Studying Crab Claws</span>"
    ]
  },
  {
    "objectID": "15-crabs.html#the-data",
    "href": "15-crabs.html#the-data",
    "title": "\n15  Studying Crab Claws\n",
    "section": "\n15.2 The Data",
    "text": "15.2 The Data\nThe available data are the mean closing forces (in Newtons) and the propodus heights (mm) of the claws on 38 crabs that came from three different species. The propodus is the segment of the crab’s clawed leg with an immovable finger and palm.\n\nThis was part of a study of the effects that predatory intertidal crab species have on populations of snails. The three crab species under study are:\n\n14 Hemigraspus nudus, also called the purple shore crab (14 crabs)\n12 Lophopanopeus bellus, also called the black-clawed pebble crab, and\n12 Cancer productus, one of several species of red rock crabs (12)\n\n\ncrabs &lt;- read_csv(\"data/crabs.csv\", show_col_types = FALSE) \n\ncrabs\n\n# A tibble: 38 × 4\n    crab species              force height\n   &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;  &lt;dbl&gt;\n 1     1 Hemigraspus nudus      4      8  \n 2     2 Lophopanopeus bellus  15.1    7.9\n 3     3 Cancer productus       5      6.7\n 4     4 Lophopanopeus bellus   2.9    6.6\n 5     5 Hemigraspus nudus      3.2    5  \n 6     6 Hemigraspus nudus      9.5    7.9\n 7     7 Cancer productus      22.5    9.4\n 8     8 Hemigraspus nudus      7.4    8.3\n 9     9 Cancer productus      14.6   11.2\n10    10 Lophopanopeus bellus   8.7    8.6\n# ℹ 28 more rows\n\n\nThe species information is stored here as a character variable. How many different crabs are we talking about in each species?\n\ncrabs |&gt; tabyl(species)\n\n              species  n   percent\n     Cancer productus 12 0.3157895\n    Hemigraspus nudus 14 0.3684211\n Lophopanopeus bellus 12 0.3157895\n\n\nAs it turns out, we’re going to want to treat the species information as a factor with three levels, rather than as a character variable.\n\ncrabs &lt;- crabs |&gt;\n    mutate(species = factor(species))\n\nHere’s a quick summary of the data. Take care to note the useless results for the first two variables. At least the function flags with a * those variables it thinks are non-numeric.\n\npsych::describe(crabs)\n\n         vars  n  mean    sd median trimmed   mad min  max range skew kurtosis\ncrab        1 38 19.50 11.11  19.50   19.50 14.08   1 38.0  37.0 0.00    -1.30\nspecies*    2 38  2.00  0.81   2.00    2.00  1.48   1  3.0   2.0 0.00    -1.50\nforce       3 38 12.13  8.98   8.70   11.53  9.04   2 29.4  27.4 0.47    -1.25\nheight      4 38  8.81  2.23   8.25    8.78  2.52   5 13.1   8.1 0.19    -1.14\n           se\ncrab     1.80\nspecies* 0.13\nforce    1.46\nheight   0.36\n\n\nActually, we’re more interested in these results after grouping by species.\n\ncrabs |&gt;\n    group_by(species) |&gt;\n    summarise(n = n(), median(force), median(height))\n\n# A tibble: 3 × 4\n  species                  n `median(force)` `median(height)`\n  &lt;fct&gt;                &lt;int&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n1 Cancer productus        12            19.7            11.0 \n2 Hemigraspus nudus       14             3.7             7.9 \n3 Lophopanopeus bellus    12            14.8             8.15",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Studying Crab Claws</span>"
    ]
  },
  {
    "objectID": "15-crabs.html#association-of-size-and-force",
    "href": "15-crabs.html#association-of-size-and-force",
    "title": "\n15  Studying Crab Claws\n",
    "section": "\n15.3 Association of Size and Force",
    "text": "15.3 Association of Size and Force\nSuppose we want to describe force on the basis of height, across all 38 crabs. We’ll add titles and identify the three species of crab, using shape and color.\n\nggplot(crabs, aes(x = height, y = force, color = species, shape = species)) +\n    geom_point(size = 3) +\n    labs(title = \"Crab Claw Force by Size\", \n         x = \"Claw's propodus height (mm)\", y = \"Mean closing force (N)\")\n\n\n\n\n\n\n\nA faceted plot for each species really highlights the difference in force between the Hemigraspus nudus and the other two species of crab.\n\nggplot(crabs, aes(x = height, y = force, color = species)) +\n    geom_point(size = 3) +\n    facet_wrap(~ species) +\n    guides(color = \"none\") +\n    labs(title = \"Crab Claw Force by Size\", \n         x = \"Claw's propodus height (mm)\", y = \"Mean closing force (N)\")",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Studying Crab Claws</span>"
    ]
  },
  {
    "objectID": "15-crabs.html#loess_smooth",
    "href": "15-crabs.html#loess_smooth",
    "title": "\n15  Studying Crab Claws\n",
    "section": "\n15.4 The loess smooth",
    "text": "15.4 The loess smooth\nWe can obtain a smoothed curve (using several different approaches) to summarize the pattern presented by the data in any scatterplot. For instance, we might build such a plot for the complete set of 38 crabs, adding in a non-linear smooth function (called a loess smooth.)\n\nggplot(crabs, aes(x = height, y = force)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", se = FALSE, formula = y ~ x) +\n    labs(title = \"Crab Claw Force by Size\", \n         x = \"Claw's propodus height (mm)\", y = \"Mean closing force (N)\")\n\n\n\n\n\n\n\nAs we have discussed previously, a loess smooth fits a curve to data by tracking (at point x) the points within a neighborhood of point x, with more emphasis given to points near x. It can be adjusted by tweaking the span and degree parameters.\nIn addition to the curve, smoothing procedures can also provide confidence intervals around their main fitted line. Consider the following plot of the crabs information, which adjusts the span (from its default of 0.75) and also adds in the confidence intervals.\n\nggplot(crabs, aes(x = height, y = force)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x, span = 0.5, se = TRUE) +\n    labs(title = \"Crab Claw Force by Size\", \n         x = \"Claw's propodus height (mm)\", y = \"Mean closing force (N)\")\n\n\n\n\n\n\n\nBy reducing the size of the span, our resulting picture shows a much less smooth function that we generated previously.\n\n15.4.1 Smoothing within Species\nWe can, of course, produce the plot above with separate smooths for each of the three species of crab.\n\nggplot(crabs, aes(x = height, y = force, group = species, color = species)) +\n    geom_point(size = 3) +\n    geom_smooth(method = \"loess\", formula = y ~ x, se = FALSE) +\n    labs(title = \"Crab Claw Force by Size\", \n         x = \"Claw's propodus height (mm)\", y = \"Mean closing force (N)\")\n\n\n\n\n\n\n\nIf we want to add in the confidence intervals (here I’ll show them at 90% rather than the default of 95%) then this plot should be faceted. Note that by default, what is displayed when se = TRUE are 95% prediction intervals - the level function in stat_smooth [which can be used in place of geom_smooth] is used here to change the coverage percentage from 95% to 90%.\n\nggplot(crabs, aes(x = height, y = force, group = species, color = species)) +\n    geom_point() +\n    stat_smooth(method = \"loess\", formula = y ~ x, level = 0.90, se = TRUE) +\n    guides(color = \"none\") +\n    labs(title = \"Crab Claw Force by Size\", \n         caption = \"with loess smooths and 90% confidence intervals\",\n         x = \"Claw's propodus height (mm)\", y = \"Mean closing force (N)\") +\n    facet_wrap(~ species)\n\n\n\n\n\n\n\nMore on these and other confidence intervals later, especially in part B.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Studying Crab Claws</span>"
    ]
  },
  {
    "objectID": "15-crabs.html#fitting-a-linear-regression-model",
    "href": "15-crabs.html#fitting-a-linear-regression-model",
    "title": "\n15  Studying Crab Claws\n",
    "section": "\n15.5 Fitting a Linear Regression Model",
    "text": "15.5 Fitting a Linear Regression Model\nSuppose we plan to use a simple (least squares) linear regression model to describe force as a function of height. Is a least squares model likely to be an effective choice here?\nThe plot below shows the regression line predicting closing force as a function of propodus height. Here we annotate the plot to show the actual fitted regression line, which required fitting it with the lm statement prior to developing the graph.\n\nmod &lt;- lm(force ~ height, data = crabs)\n\nggplot(crabs, aes(x = height, y = force)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x,  color = \"red\") +\n    labs(title = \"Crab Claw Force by Size with Linear Regression Model\", \n         x = \"Claw's propodus height (mm)\", y = \"Mean closing force (N)\") +\n    annotate(\"text\", x = 11, y = 0, color = \"red\", fontface = \"italic\",\n             label = paste( \"Force = \", round_half_up(coef(mod)[1],3), \" + \", \n                            round_half_up(coef(mod)[2],3), \" Height\" ))\n\n\n\n\n\n\n\nThe lm function, again, specifies the linear model we fit to predict force using height. Here’s the summary.\n\nsummary(lm(force ~ height, data = crabs))\n\n\nCall:\nlm(formula = force ~ height, data = crabs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.7945  -3.8113  -0.2394   4.1444  16.8814 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -11.0869     4.6224  -2.399   0.0218 *  \nheight        2.6348     0.5089   5.177 8.73e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.892 on 36 degrees of freedom\nMultiple R-squared:  0.4268,    Adjusted R-squared:  0.4109 \nF-statistic:  26.8 on 1 and 36 DF,  p-value: 8.73e-06\n\n\nAgain, the key things to realize are:\n\nThe outcome variable in this model is force, and the predictor variable is height.\nThe straight line model for these data fitted by least squares is force = -11.087 + 2.635 height.\nThe slope of height is positive, which indicates that as height increases, we expect that force will also increase. Specifically, we expect that for every additional mm of height, the force will increase by 2.635 Newtons.\nThe multiple R-squared (squared correlation coefficient) is 0.427, which implies that 42.7% of the variation in force is explained using this linear model with height. It also implies that the Pearson correlation between force and height is the square root of 0.427, or 0.653.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Studying Crab Claws</span>"
    ]
  },
  {
    "objectID": "15-crabs.html#is-a-linear-model-appropriate",
    "href": "15-crabs.html#is-a-linear-model-appropriate",
    "title": "\n15  Studying Crab Claws\n",
    "section": "\n15.6 Is a Linear Model Appropriate?",
    "text": "15.6 Is a Linear Model Appropriate?\nThe zoology (at least as described in Ramsey and Schafer (2002)) suggests that the actual nature of the relationship would be represented by a log-log relationship, where the log of force is predicted by the log of height.\nThis log-log model is an appropriate model when we think that percentage increases in X (height, here) lead to constant percentage increases in Y (here, force).\nTo see the log-log model in action, we plot the log of force against the log of height. We could use either base 10 (log10 in R) or natural (log in R) logarithms.\n\nggplot(crabs, aes(x = log(height), y = log(force))) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y ~ x) + \n    labs(title = \"Log-Log Model for Crabs data\")\n\n\n\n\n\n\n\nThe correlations between the raw force and height and between their logarithms turn out to be quite similar, and because the log transformation is monotone in these data, there’s actually no change at all in the Spearman correlations.\n\n\nCorrelation of\nPearson r\nSpearman r\n\n\n\nforce and height\n0.653\n0.657\n\n\nlog(force) and log(height)\n0.662\n0.657\n\n\n\n\n15.6.1 The log-log model\n\ncrab_loglog &lt;- lm(log(force) ~ log(height), data = crabs)\n\nsummary(crab_loglog)\n\n\nCall:\nlm(formula = log(force) ~ log(height), data = crabs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5657 -0.4450  0.1884  0.4798  1.2422 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -2.7104     0.9251  -2.930  0.00585 ** \nlog(height)   2.2711     0.4284   5.302 5.96e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6748 on 36 degrees of freedom\nMultiple R-squared:  0.4384,    Adjusted R-squared:  0.4228 \nF-statistic: 28.11 on 1 and 36 DF,  p-value: 5.96e-06\n\n\nOur regression equation is log(force) = -2.71 + 2.271 log(height).\nSo, for example, if we found a crab with propodus height = 10 mm, our prediction for that crab’s claw force (in Newtons) based on this log-log model would be…\n\nlog(force) = -2.71 + 2.271 log(10)\nlog(force) = -2.71 + 2.271 x 2.3025851\nlog(force) = 2.5190953\nand so predicted force = exp(2.5190953) = 12.4173582 Newtons, which, naturally, we would round to 12.417 Newtons to match the data set’s level of precision.\n\n15.6.2 How does this compare to our original linear model?\n\ncrab_linear &lt;- lm(force ~ height, data = crabs)\n\nsummary(crab_linear)\n\n\nCall:\nlm(formula = force ~ height, data = crabs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.7945  -3.8113  -0.2394   4.1444  16.8814 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -11.0869     4.6224  -2.399   0.0218 *  \nheight        2.6348     0.5089   5.177 8.73e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.892 on 36 degrees of freedom\nMultiple R-squared:  0.4268,    Adjusted R-squared:  0.4109 \nF-statistic:  26.8 on 1 and 36 DF,  p-value: 8.73e-06\n\n\nThe linear regression equation is force = -11.087 + 2.635 height.\nSo, for example, if we found a crab with propodus height = 10 mm, our prediction for that crab’s claw force (in Newtons) based on this linear model would be…\n\nforce = -11.0869025 + 2.6348232 x 10\nforce = -11.0869025 + 26.3482321\nso predicted force = 15.2613297, which we would round to 15.261 Newtons.\n\nSo, it looks like the two models give meaningfully different predictions.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Studying Crab Claws</span>"
    ]
  },
  {
    "objectID": "15-crabs.html#making-predictions-with-a-model",
    "href": "15-crabs.html#making-predictions-with-a-model",
    "title": "\n15  Studying Crab Claws\n",
    "section": "\n15.7 Making Predictions with a Model",
    "text": "15.7 Making Predictions with a Model\nThe broom package’s augment function provides us with a consistent method for obtaining predictions (also called fitted values) for a new crab or for our original data. Suppose we want to predict the force level for two new crabs: one with height = 10 mm, and another with height = 12 mm.\n\nnewcrab &lt;- tibble(crab = c(\"Crab_A\", \"Crab_B\"), height = c(10, 12))\n\naugment(crab_linear, newdata = newcrab)\n\n# A tibble: 2 × 3\n  crab   height .fitted\n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Crab_A     10    15.3\n2 Crab_B     12    20.5\n\n\nShould we want to obtain a prediction interval, we can use the predict function:\n\npredict(crab_linear, newdata = newcrab, interval = \"prediction\", level = 0.95)\n\n       fit      lwr      upr\n1 15.26133 1.048691 29.47397\n2 20.53098 5.994208 35.06774\n\n\nWe’d interpret this result as saying that the linear model’s predicted force associated with a single new crab claw with propodus height 10 mm is 15.3 Newtons, and that a 95% prediction interval for the true value of such a force for such a claw is between 1.0 and 29.5 Newtons. More on prediction intervals later.\n\n15.7.1 Predictions After a Transformation\nWe can also get predictions from the log-log model. The default choice is a 95% prediction interval.\n\npredict(crab_loglog, newdata = newcrab, interval = \"prediction\")\n\n       fit      lwr      upr\n1 2.519095 1.125900 3.912291\n2 2.933174 1.515548 4.350800\n\n\nOf course, these predictions describe the log(force) for such a crab claw. To get the prediction in terms of simple force, we’d need to back out of the logarithm, by exponentiating our point estimate and the prediction interval endpoints.\n\nexp(predict(crab_loglog, newdata = newcrab, interval = \"prediction\"))\n\n       fit      lwr      upr\n1 12.41736 3.082989 50.01341\n2 18.78716 4.551916 77.54044\n\n\nWe’d interpret this result as saying, for the first new crab, that the log-log model’s predicted force associated with a single new crab claw with propodus height 10 mm is 12.4 Newtons, and that a 95% prediction interval for the true value of such a force for such a claw is between 3.1 and 50.0 Newtons.\n\n15.7.2 Comparing Model Predictions\nSuppose we wish to build a plot of force vs height with a straight line for the linear model’s predictions, and a new curve for the log-log model’s predictions, so that we can compare and contrast the implications of the two models on a common scale. The predict function, when not given a new data frame, will use the existing predictor values that are in our crabs data. Such predictions are often called fitted values.\nTo put the two sets of predictions on the same scale despite the differing outcomes in the two models, we’ll exponentiate the results of the log-log model, and build a little data frame containing the heights and the predicted forces from that model.\n\nloglogdat &lt;- tibble(height = crabs$height, force = exp(predict(crab_loglog)))\n\nA cleaner way to do this might be to use the augment function directly from broom:\n\naugment(crab_loglog)\n\n# A tibble: 38 × 8\n   `log(force)` `log(height)` .fitted   .resid   .hat .sigma  .cooksd .std.resid\n          &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1         1.39          2.08   2.01  -6.26e-1 0.0280  0.676 1.28e- 2  -0.941   \n 2         2.71          2.07   1.98   7.31e-1 0.0287  0.673 1.79e- 2   1.10    \n 3         1.61          1.90   1.61  -1.15e-4 0.0499  0.684 8.06e-10  -0.000175\n 4         1.06          1.89   1.58  -5.11e-1 0.0530  0.679 1.69e- 2  -0.778   \n 5         1.16          1.61   0.945  2.18e-1 0.142   0.683 1.01e- 2   0.349   \n 6         2.25          2.07   1.98   2.68e-1 0.0287  0.683 2.39e- 3   0.402   \n 7         3.11          2.24   2.38   7.35e-1 0.0301  0.673 1.90e- 2   1.11    \n 8         2.00          2.12   2.10  -9.44e-2 0.0266  0.684 2.75e- 4  -0.142   \n 9         2.68          2.42   2.78  -9.55e-2 0.0561  0.684 6.30e- 4  -0.146   \n10         2.16          2.15   2.18  -1.32e-2 0.0263  0.684 5.34e- 6  -0.0199  \n# ℹ 28 more rows\n\n\nNow, we’re ready to use the geom_smooth approach to plot the linear fit, and geom_line (which also fits curves) to display the log-log fit.\n\nggplot(crabs, aes(x = height, y = force)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, \n                formula = y ~ x, col=\"blue\", linetype = 2) +\n    geom_line(data = loglogdat, col = \"red\", linetype = 2, size = 1) +\n    annotate(\"text\", 7, 12, label = \"Linear Model\", col = \"blue\") +\n    annotate(\"text\", 10, 8, label = \"Log-Log Model\", col = \"red\") +\n    labs(title = \"Comparing the Linear and Log-Log Models for Crab Claw data\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nBased on these 38 crabs, we see some modest differences between the predictions of the two models, with the log-log model predicting generally lower closing force for a given propodus height than would be predicted by a linear model.\n\n\n\n\nRamsey, Fred L., and Daniel W. Schafer. 2002. The Statistical Sleuth: A Course in Methods of Data Analysis. Second. Pacific Grove, CA: Duxbury.\n\n\nYamada, SB, and EG Boulding. 1998. “Claw Morphology, Prey Size Selection and Foraging Efficiency in Generalist and Specialist Shell-Breaking Crabs.” Journal of Experimental Marine Biology and Ecology 220: 191–211. http://www.science.oregonstate.edu/~yamadas/SylviaCV/BehrensYamada_Boulding1998.pdf.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Studying Crab Claws</span>"
    ]
  },
  {
    "objectID": "16-hydrate.html",
    "href": "16-hydrate.html",
    "title": "16  Dehydration Recovery",
    "section": "",
    "text": "16.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(knitr)\nlibrary(broom)\nlibrary(GGally)\nlibrary(ggstance)\nlibrary(kableExtra)\nlibrary(modelsummary)\nlibrary(patchwork)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\nWe will also use the favstats function from the mosaic package.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dehydration Recovery</span>"
    ]
  },
  {
    "objectID": "16-hydrate.html#the-data",
    "href": "16-hydrate.html#the-data",
    "title": "16  Dehydration Recovery",
    "section": "\n16.2 The Data",
    "text": "16.2 The Data\nThe hydrate data describe the degree of recovery that takes place 90 minutes following treatment of moderate to severe dehydration, for 36 children diagnosed at a hospital’s main pediatric clinic.\nUpon diagnosis and study entry, patients were treated with an electrolytic solution at one of seven dose levels (0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0 mEq/l) in a frozen, flavored, ice popsicle. The degree of rehydration was determined using a subjective scale based on physical examination and parental input, converted to a 0 to 100 point scale, representing the percent of recovery (recov.score). Each child’s age (in years) and weight (in pounds) are also available.\nFirst, we’ll check ranges (and for missing data) in the hydrate file.\n\nhydrate &lt;- read_csv(\"data/hydrate.csv\")\n\nsummary(hydrate)\n\n       id         recov.score          dose            age        \n Min.   : 1.00   Min.   : 44.00   Min.   :0.000   Min.   : 3.000  \n 1st Qu.: 9.75   1st Qu.: 61.50   1st Qu.:1.000   1st Qu.: 5.000  \n Median :18.50   Median : 71.50   Median :1.500   Median : 6.500  \n Mean   :18.50   Mean   : 71.56   Mean   :1.569   Mean   : 6.667  \n 3rd Qu.:27.25   3rd Qu.: 80.00   3rd Qu.:2.500   3rd Qu.: 8.000  \n Max.   :36.00   Max.   :100.00   Max.   :3.000   Max.   :11.000  \n     weight     \n Min.   :22.00  \n 1st Qu.:34.50  \n Median :47.50  \n Mean   :46.89  \n 3rd Qu.:57.25  \n Max.   :76.00  \n\n\nThere are no missing values, and all of the ranges make sense. There are no especially egregious problems to report.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dehydration Recovery</span>"
    ]
  },
  {
    "objectID": "16-hydrate.html#a-scatterplot-matrix",
    "href": "16-hydrate.html#a-scatterplot-matrix",
    "title": "16  Dehydration Recovery",
    "section": "\n16.3 A Scatterplot Matrix",
    "text": "16.3 A Scatterplot Matrix\nNext, we’ll use a scatterplot matrix to summarize relationships between the outcome recov.score and the key predictor dose as well as the ancillary predictors age and weight, which are of less interest, but are expected to be related to our outcome. The one below uses the ggpairs function in the GGally package, as introduced in Part A of the Notes. We place the outcome in the bottom row, and the key predictor immediately above it, with age and weight in the top rows, using the select function within the `ggpairs call.\n\nhydrate |&gt;\n  select(age, weight, dose, recov.score) |&gt;\n  ggpairs()\n\n\n\n\n\n\n\nWhat can we conclude here?\n\nIt looks like recov.score has a moderately strong negative relationship with both age and weight (with correlations in each case around -0.5), but a positive relationship with dose (correlation = 0.36).\nThe distribution of recov.score looks to be pretty close to Normal. No potential predictors (age, weight and dose) show substantial non-Normality.\n\nage and weight, as we’d expect, show a very strong and positive linear relationship, with r = 0.94\nNeither age nor weight shows a meaningful relationship with dose. (r = 0.16)",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dehydration Recovery</span>"
    ]
  },
  {
    "objectID": "16-hydrate.html#are-the-recovery-scores-well-described-by-a-normal-model",
    "href": "16-hydrate.html#are-the-recovery-scores-well-described-by-a-normal-model",
    "title": "16  Dehydration Recovery",
    "section": "\n16.4 Are the recovery scores well described by a Normal model?",
    "text": "16.4 Are the recovery scores well described by a Normal model?\nNext, we’ll do a more thorough graphical summary of our outcome, recovery score.\n\np1 &lt;- ggplot(hydrate, aes(sample = recov.score)) +\n  geom_qq(col = '#440154') + geom_qq_line(col = \"red\") + \n  theme(aspect.ratio = 1) + \n  labs(title = \"Normal Q-Q plot: hydrate\")\n\np2 &lt;- ggplot(hydrate, aes(x = recov.score)) +\n  geom_histogram(aes(y = stat(density)), \n                 bins = 10, fill = '#440154', col = '#FDE725') +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(hydrate$recov.score), \n                            sd = sd(hydrate$recov.score)),\n                col = \"red\", lwd = 1.5) +\n  labs(title = \"Density Function: hydrate\")\n\np3 &lt;- ggplot(hydrate, aes(x = recov.score, y = \"\")) +\n  geom_boxplot(fill = '#440154', outlier.color = '#440154') + \n  labs(title = \"Boxplot: hydrate\", y = \"\")\n\np1 + (p2 / p3 + plot_layout(heights = c(4,1)))\n\nWarning: `stat(density)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\nmosaic::favstats(~ recov.score, data = hydrate) |&gt; kable(digits = 1)\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n44\n61.5\n71.5\n80\n100\n71.6\n12.9\n36\n0\n\n\n\n\nI see no serious problems with assuming Normality for these recovery scores. Our outcome variable doesn’t in any way need to follow a Normal distribution, but it’s nice when it does, because summaries involving means and standard deviations make sense.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dehydration Recovery</span>"
    ]
  },
  {
    "objectID": "16-hydrate.html#simple-regression-using-dose-to-predict-recovery",
    "href": "16-hydrate.html#simple-regression-using-dose-to-predict-recovery",
    "title": "16  Dehydration Recovery",
    "section": "\n16.5 Simple Regression: Using Dose to predict Recovery",
    "text": "16.5 Simple Regression: Using Dose to predict Recovery\nTo start, consider a simple (one predictor) regression model using dose alone to predict the % Recovery (recov.score). Ignoring the age and weight covariates, what can we conclude about this relationship?",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dehydration Recovery</span>"
    ]
  },
  {
    "objectID": "16-hydrate.html#the-scatterplot-with-fitted-linear-model",
    "href": "16-hydrate.html#the-scatterplot-with-fitted-linear-model",
    "title": "16  Dehydration Recovery",
    "section": "\n16.6 The Scatterplot, with fitted Linear Model",
    "text": "16.6 The Scatterplot, with fitted Linear Model\n\nggplot(hydrate, aes(x = dose, y = recov.score)) +\n    geom_point(size = 2) +\n    geom_smooth(method = \"lm\", formula = y ~ x, col = \"red\") +\n    labs(title = \"Simple Regression model for the hydrate data\",\n         x = \"Dose (mEq/l)\", y = \"Recovery Score (points)\")",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dehydration Recovery</span>"
    ]
  },
  {
    "objectID": "16-hydrate.html#the-fitted-linear-model",
    "href": "16-hydrate.html#the-fitted-linear-model",
    "title": "16  Dehydration Recovery",
    "section": "\n16.7 The Fitted Linear Model",
    "text": "16.7 The Fitted Linear Model\nTo obtain the fitted linear regression model, we use the lm function:\n\nm1 &lt;- lm(recov.score ~ dose, data = hydrate)\n\ntidy(m1) |&gt; kbl(digits = 2)\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n63.90\n3.97\n16.09\n0.00\n\n\ndose\n4.88\n2.17\n2.25\n0.03\n\n\n\n\n\nSo, our fitted regression model (prediction model) is recov.score = 63.9 + 4.88 dose.\n\n16.7.1 Confidence Intervals\nWe can obtain confidence intervals around the coefficients of our fitted model with tidy, too.\n\ntidy(m1, conf.int = TRUE, conf.level = 0.90) |&gt; \n  kbl(digits = 2)\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n63.90\n3.97\n16.09\n0.00\n57.18\n70.61\n\n\ndose\n4.88\n2.17\n2.25\n0.03\n1.21\n8.55\n\n\n\n\n\nSo, our 90% confidence interval for the slope of dose ranges from 1.21 to 8.55.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dehydration Recovery</span>"
    ]
  },
  {
    "objectID": "16-hydrate.html#coefficient-plots-with-modelplot",
    "href": "16-hydrate.html#coefficient-plots-with-modelplot",
    "title": "16  Dehydration Recovery",
    "section": "\n16.8 Coefficient Plots with modelplot\n",
    "text": "16.8 Coefficient Plots with modelplot\n\nThe modelplot() function from the modelsummary package can provide us with one potential graph.\n\nmodelplot(m1, conf_level = 0.90)",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dehydration Recovery</span>"
    ]
  },
  {
    "objectID": "16-hydrate.html#coefficient-plots-with-ggstance",
    "href": "16-hydrate.html#coefficient-plots-with-ggstance",
    "title": "16  Dehydration Recovery",
    "section": "\n16.9 Coefficient Plots with ggstance\n",
    "text": "16.9 Coefficient Plots with ggstance\n\nThe tidy method makes it easy to construct coefficient plots using ggplot2, and we’ll also make use of the geom_crossbarh function from the ggstance package.\n\ntd &lt;- tidy(m1, conf.int = TRUE, conf.level = 0.90)\n\nggplot(td, aes(x = estimate, y = term, col = term)) +\n  geom_point() +\n  geom_crossbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_vline(xintercept = 0) +\n  guides(col = \"none\") +\n  labs(title = \"Estimates with 90% confidence intervals from m1 in hydrate\")\n\nWarning: Using the `size` aesthetic with geom_polygon was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\nWarning: Using the `size` aesthetic with geom_segment was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\n\nAnother option would be to use geom_errorbarh in this setting, perhaps with a different color scheme…\n\ntd &lt;- tidy(m1, conf.int = TRUE, conf.level = 0.90)\nggplot(td, aes(x = estimate, y = term, col = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_vline(xintercept = 0) +\n  scale_color_viridis_d(end = 0.5) +\n  guides(col = \"none\") +\n  labs(title = \"Estimates with 90% confidence intervals from m1 in hydrate\")\n\nWarning: Using the `size` aesthetic with geom_path was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dehydration Recovery</span>"
    ]
  },
  {
    "objectID": "16-hydrate.html#the-summary-output",
    "href": "16-hydrate.html#the-summary-output",
    "title": "16  Dehydration Recovery",
    "section": "\n16.10 The Summary Output",
    "text": "16.10 The Summary Output\nTo get a more complete understanding of the fitted model, we’ll summarize it.\n\nsummary(lm(recov.score ~ dose, data = hydrate))\n\n\nCall:\nlm(formula = recov.score ~ dose, data = hydrate)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3360  -7.2763   0.0632   8.4233  23.9028 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   63.896      3.970  16.093   &lt;2e-16 ***\ndose           4.881      2.172   2.247   0.0313 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.21 on 34 degrees of freedom\nMultiple R-squared:  0.1293,    Adjusted R-squared:  0.1037 \nF-statistic: 5.047 on 1 and 34 DF,  p-value: 0.03127\n\n\n\n16.10.1 Model Specification\n\nThe first part of the output specifies the model that has been fit.\n\nHere, we have a simple regression model that predicts recov.score on the basis of dose.\nNotice that we’re treating dose here as a quantitative variable. If we wanted dose to be treated as a factor, we’d have specified that in the model.\n\n\n\n16.10.2 Residual Summary\n\nThe second part of the output summarizes the regression residuals across the subjects involved in fitting the model.\n\nThe residual is defined as the Actual value of our outcome minus the predicted value of that outcome fitted by the model.\nIn our case, the residual for a given child is their actual recov.score minus the predicted recov.score according to our model, for that child.\nThe residual summary gives us a sense of how “incorrect” our predictions are for the hydrate observations.\n\nA positive residual means that the observed value was higher than the predicted value from the linear regression model, so the prediction was too low.\nA negative residual means that the observed value was lower than the predicted value from the linear regression model, so the prediction was too high.\nThe residuals will center near 0 (the ordinary least squares model fitting process is designed so the mean of the residuals will always be zero)\nWe hope to see the median of the residuals also be near zero, generally. In this case, the median prediction is 0.06 point too low.\nThe minimum and maximum show us the largest prediction errors, made in the subjects used to fit this model.\nHere, we predicted a recovery score that was 22.3 points too high for one patient, and another of our predicted recovery scores was 23.9 points too low.\nThe middle half of our predictions were between 8.4 points too low and 7.3 points too high.\n\n\n\n\n\n16.10.3 Coefficients Output\n\nThe Coefficients output begins with a table of the estimated coefficients from the regression equation.\n\nGenerally, we write a simple regression model as \\(y = \\beta_0 + \\beta_1 x\\).\nIn the hydrate model, we have recov.score = \\(\\beta_0\\) + \\(\\beta_1\\) dose.\nThe first column of the table gives the estimated \\(\\beta\\) coefficients for our model\n\nHere the estimated intercept \\(\\hat{\\beta_0} = 63.9\\)\n\nThe estimated slope of dose \\(\\hat{\\beta_1} = 4.88\\)\n\nThus, our model is recov.score= 63.9 + 4.88 dose\n\n\n\n\n\n\nWe interpret these coefficients as follows:\n\nThe intercept (63.9) is the predicted recov.score for a patient receiving a dose of 0 mEq/l of the electrolytic solution.\nThe slope (4.88) of the dose is the predicted change in recov.score associated with a 1 mEq/l increase in the dose of electrolytic solution.\n\nEssentially, if we have two children like the ones studied here, and we give Roger a popsicle with dose X and Sarah a popsicle with dose X + 1, then this model predicts that Sarah will have a recovery score that is 4.88 points higher than will Roger.\nFrom the confidence interval output we saw previously with the function confint(lm(recov.score ~ dose)), we are 95% confident that the true slope for dose is between (0.47, 9.30) mEq/l. We are also 95% confident that the true intercept is between (55.8, 72.0).\n\n\n\n16.10.4 Correlation and Slope\nIf we like, we can use the cor function to specify the Pearson correlation of recov.score and dose, which turns out to be 0.36. - Note that the slope in a simple regression model will follow the sign of the Pearson correlation coefficient, in this case, both will be positive.\n\nhydrate |&gt; select(recov.score, dose) |&gt; cor()\n\n            recov.score     dose\nrecov.score    1.000000 0.359528\ndose           0.359528 1.000000\n\n\n\n16.10.5 Coefficient Testing\n\nsummary(lm(recov.score ~ dose, data = hydrate))\n\n\nCall:\nlm(formula = recov.score ~ dose, data = hydrate)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.3360  -7.2763   0.0632   8.4233  23.9028 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   63.896      3.970  16.093   &lt;2e-16 ***\ndose           4.881      2.172   2.247   0.0313 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.21 on 34 degrees of freedom\nMultiple R-squared:  0.1293,    Adjusted R-squared:  0.1037 \nF-statistic: 5.047 on 1 and 34 DF,  p-value: 0.03127\n\n\nNext to each coefficient in the summary regression table is its estimated standard error, followed by the coefficient’s t value (the coefficient value divided by the standard error), and the associated two-tailed p value for the test of:\n\n\n\\(H_0\\): This coefficient’s \\(\\beta\\) value = 0 vs. \n\n\\(H_A\\): This coefficient’s \\(\\beta\\) value \\(\\neq\\) 0.\n\nFor the slope coefficient, we can interpret this choice as:\n\n\n\\(H_0\\): This predictor adds minimal detectable predictive value to the model vs. \n\n\\(H_A\\): This predictor adds some predictive value to the model.\n\nIn the hydrate simple regression model, by running either tidy with or just the confint function shown below, we can establish a confidence interval for each of the estimated regression coefficients.\n\ntidy(m1, conf.int = TRUE, conf.level = 0.95) |&gt; kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n63.90\n3.97\n16.09\n0.00\n55.83\n71.96\n\n\ndose\n4.88\n2.17\n2.25\n0.03\n0.47\n9.30\n\n\n\n\n\n\nconfint(m1, level = .95)\n\n                2.5 %    97.5 %\n(Intercept) 55.826922 71.964589\ndose         0.465695  9.295466\n\n\nIf the slope of dose was in fact zero, then this would mean that knowing the dose information would be of no additional value in predicting the outcome over just guessing the mean of recov.score for every subject.\nSo, since the confidence interval for the slope of dose does not include zero, it appears that there is at least some evidence that the model m1 is more effective than a model that ignores the dose information (and simply predicts the mean of recov.score for each subject.) That’s not saying much, actually.\n\n16.10.6 Summarizing the Quality of Fit\n\nThe next part of the regression summary output is a summary of fit quality.\n\nThe residual standard error estimates the standard deviation of the prediction errors made by the model.\n\nIf assumptions hold, the model will produce residuals that follow a Normal distribution with mean 0 and standard deviation equal to this residual standard error.\n\nSo we’d expect roughly 95% of our residuals to fall between -2(12.21) and +2(12.21), or roughly -24.4 to +24.4 and that we’d see virtually no residuals outside the range of -3(12.21) to +3(12.21), or roughly -36.6 to +36.6.\nThe output at the top of the summary tells us about the observed regression residuals, and that they actually range from -22 to +24.\nIn context, it’s hard to know whether or not we should be happy about this. On a scale from 0 to 100, rarely missing by more than 24 seems OK to me, but not terrific.\n\n\nThe degrees of freedom here are the same as the denominator degrees of freedom in the ANOVA to follow. The calculation is \\(n - k\\), where \\(n\\) = the number of observations and \\(k\\) is the number of coefficients estimated by the regression (including the intercept and any slopes).\n\nHere, there are 36 observations in the model, and we fit k = 2 coefficients; the slope and the intercept, as in any simple regression model, so df = 36 - 2 = 34.\n\n\n\nThe multiple R-squared value is usually just referred to as R-squared.\n\nThis is interpreted as the proportion of variation in the outcome variable that has been accounted for by our regression model.\n\nHere, we’ve accounted for just under 13% of the variation in % Recovery using Dose.\n\n\nThe R in multiple R-squared is the Pearson correlation of recov.score and dose, which in this case is 0.3595.\n\nSquaring this value gives the R-squared for this simple regression.\n(0.3595)^2 = 0.129\n\n\n\nR-squared is greedy.\n\nR-squared will always suggest that we make our models as big as possible, often including variables of dubious predictive value.\nAs a result, there are various methods for adjusting or penalizing R-squared so that we wind up with smaller models.\nThe adjusted R-squared is often a useful way to compare multiple models for the same response.\n\n\n\\(R^2_{adj} = 1 - \\frac{(1-R^2)(n - 1)}{n - k}\\), where \\(n\\) = the number of observations and \\(k\\) is the number of coefficients estimated by the regression (including the intercept and any slopes).\nSo, in this case, \\(R^2_{adj} = 1 - \\frac{(1 - 0.1293)(35)}{34} = 0.1037\\)\n\nThe adjusted R-squared value is not, technically, a proportion of anything, but it is comparable across models for the same outcome.\nThe adjusted R-squared will always be less than the (unadjusted) R-squared.\n\n\n\n16.10.7 ANOVA F test\n\nThe last part of the standard summary of a regression model is the overall ANOVA F test.\n\nThe hypotheses for this test are:\n\nH0: Each of the coefficients in the model (other than the intercept) has \\(\\beta\\) = 0 vs.\nHA: At least one regression slope has \\(\\beta \\neq\\) 0\n\nSince we are doing a simple regression with just one predictor, the ANOVA F test hypotheses are exactly the same as the t test for dose:\n\nH0: The slope for dose has \\(\\beta\\) = 0 vs.\nHA: The slope for dose has \\(\\beta \\neq\\) 0\n\nIn this case, we have an F statistic of 5.05 on 1 and 34 degrees of freedom, yielding p = 0.03\nThis provides some evidence that “something” in our model (here, dose is the only predictor) predicts the outcome to a degree beyond that easily attributed to chance alone. This is not actually surprising, nor is it especially interesting. The confidence interval for the slope is definitely more interesting than this.\n\nIn simple regression (regression with only one predictor), the t test for the slope (dose) always provides the same p value as the ANOVA F test.\n\nThe F test statistic in a simple regression is always by definition just the square of the slope’s t test statistic.\nHere, F = 5.047, and this is the square of t = 2.247 from the Coefficients output\n\n\n\nThis test is basically just a combination of the R-squared value (13%) and the sample size. We don’t learn much from it that’s practically interesting or useful.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dehydration Recovery</span>"
    ]
  },
  {
    "objectID": "16-hydrate.html#viewing-the-complete-anova-table",
    "href": "16-hydrate.html#viewing-the-complete-anova-table",
    "title": "16  Dehydration Recovery",
    "section": "\n16.11 Viewing the complete ANOVA table",
    "text": "16.11 Viewing the complete ANOVA table\nWe can obtain the complete ANOVA table associated with this particular model, and the details behind this F test using the anova function:\n\nanova(lm(recov.score ~ dose, data = hydrate))\n\nAnalysis of Variance Table\n\nResponse: recov.score\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)  \ndose       1  752.2  752.15  5.0473 0.03127 *\nResiduals 34 5066.7  149.02                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe R-squared for our regression model is equal to the \\(\\eta^2\\) for this ANOVA model.\n\nIf we divide SS(dose) = 752.2 by the total sum of squares (752.2 + 5066.7), we’ll get the multiple R-squared [0.1293]\n\n\nNote that this is not the same ANOVA model we would get if we treated dose as a factor with seven levels, rather than as a quantitative variable.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dehydration Recovery</span>"
    ]
  },
  {
    "objectID": "16-hydrate.html#using-glance-to-summarize-the-models-fit",
    "href": "16-hydrate.html#using-glance-to-summarize-the-models-fit",
    "title": "16  Dehydration Recovery",
    "section": "\n16.12 Using glance to summarize the model’s fit",
    "text": "16.12 Using glance to summarize the model’s fit\nWhen applied to a linear model, the glance function from the broom package summarizes 12 characteristics of the model’s fit.\nLet’s look at the eight of these that we’ve already addressed.\n\nglance(m1) |&gt; select(r.squared:df, df.residual, nobs) |&gt;\n  kbl(digits = c(3, 3, 1, 2, 3, 0, 0, 0))\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\ndf.residual\nnobs\n\n\n0.129\n0.104\n12.2\n5.05\n0.031\n1\n34\n36\n\n\n\n\n\nWe’ve discussed the R-square value, shown in r.squared.\nWe’ve also discussed the adjusted R-square value, in adj.r.squared\n\n\nsigma is the residual standard error.\n\nstatistic is the ANOVA F statistic.\n\np.value is the p value associated with the ANOVA F statistic.\n\ndf is the numerator degrees of freedom (here, the df associated with dose) for the ANOVA test associated with this model.\n\ndf.residual is the denominator degrees of freedom (here the df associated with residual) for that same ANOVA test.\nRemember that the F-statistic at the bottom of the summary output provides these last four statistics, as well.\n\nnobs is the number of observations (rows) used to fit the model.\n\nNow, let’s look at the remaining four summaries:\n\nglance(m1) |&gt; select(logLik:deviance) |&gt;\n  kbl(digits = 1)\n\n\n\nlogLik\nAIC\nBIC\ndeviance\n\n\n-140.1\n286.3\n291\n5066.7\n\n\n\n\n\n\nlogLik is the log-likelihood value for the model, and is most commonly used for a model (like the ordinary least squares model fit by lm that is fit using the method of maximum likelihood). Thus, the log-likelihood value will be maximized in this fit.\n\nAIC is the Akaike Information Criterion for the model. When comparing models fitted by maximum likelihood to the same outcome variable (using the same transformation, for example), the smaller the AIC, the better the fit.\n\nBIC is the Bayes Information Criterion for the model. When comparing models fitted by maximum likelihood to the same outcome variable (using the same transformation, for example), the smaller the BIC, the better the fit. BIC often prefers models with fewer coefficients to estimate than does AIC.\n\n\nAIC and BIC can be estimated using several different approaches in R, but we’ll need to use the same one across multiple models if we’re comparing the results, because the concepts are only defined up to a constant.\n\n\n\ndeviance is the fitted model’s deviance, a measure of lack of fit. It is a generalization of the residual sum of squares seen in the ANOVA table, and takes the same value in the case of a simple linear regression model fit with lm as we have here. For some generalized linear models, we’ll use this for hypothesis testing, just as the ANOVA table does in the linear model case.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dehydration Recovery</span>"
    ]
  },
  {
    "objectID": "16-hydrate.html#plotting-residuals-vs.-fitted-values",
    "href": "16-hydrate.html#plotting-residuals-vs.-fitted-values",
    "title": "16  Dehydration Recovery",
    "section": "\n16.13 Plotting Residuals vs. Fitted Values",
    "text": "16.13 Plotting Residuals vs. Fitted Values\nTo save the residuals and predicted (fitted) values from this simple regression model, we can use the resid and fitted commands, respectively, or we can use the augment function in the broom package to obtain a tidy data set containing these objects and others.\n\naug_m1 &lt;- augment(m1)\n\nggplot(aug_m1, aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x, se = F) +\n    geom_smooth(method = \"lm\", formula = y ~ x, se = F, col = \"red\") +\n    labs(title = \"Residuals vs. Fitted values for Model m1\")\n\n\n\n\n\n\n\nWe can also obtain a plot of residuals vs. fitted values for m1 using the following code from base R.\n\nplot(m1, which = 1)\n\n\n\n\n\n\n\nWe hope in this plot to see a generally random scatter of points, perhaps looking like a “fuzzy football”. Since we only have seven possible dose values, we obtain only seven distinct predicted values, which explains the seven vertical lines in the plot. Here, the smooth red line indicates a gentle curve, but no evidence of a strong curve, or any other regular pattern in this residual plot.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Dehydration Recovery</span>"
    ]
  },
  {
    "objectID": "17-wcgs.html",
    "href": "17-wcgs.html",
    "title": "17  The WCGS",
    "section": "",
    "text": "17.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(broom)\nlibrary(GGally)\nlibrary(ggridges)\nlibrary(janitor)\nlibrary(kableExtra)\nlibrary(patchwork)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\nWe will also use the favstats function from the mosaic package, even though I won’t load mosaic here.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The WCGS</span>"
    ]
  },
  {
    "objectID": "17-wcgs.html#the-western-collaborative-group-study-wcgs",
    "href": "17-wcgs.html#the-western-collaborative-group-study-wcgs",
    "title": "17  The WCGS",
    "section": "\n17.2 The Western Collaborative Group Study (wcgs)",
    "text": "17.2 The Western Collaborative Group Study (wcgs)\nVittinghoff et al. (2012) explore data from the Western Collaborative Group Study (WCGS) in some detail1. We’ll touch lightly on some key issues in this Chapter.\n\nThe Western Collaborative Group Study (WCGS) was designed to test the hypothesis that the so-called Type A behavior pattern (TABP) - “characterized particularly by excessive drive, aggressiveness, and ambition, frequently in association with a relatively greater preoccupation with competitive activity, vocational deadlines, and similar pressures” - is a cause of coronary heart disease (CHD). Two additional goals, developed later in the study, were (1) to investigate the comparability of formulas developed in WCGS and in the Framingham Study (FS) for prediction of CHD risk, and (2) to determine how addition of TABP to an existing multivariate prediction formula affects ability to select subjects for intervention programs.\n\nThe study enrolled over 3,000 men ages 39-59 who were employed in San Francisco or Los Angeles, during 1960 and 1961.\nIn the code chunk below, after importing the data and creating a tibble with read_csv, I used mutate(across(where(is.character), as_factor) to convert all variables containing character data into factors.\n\nwcgs &lt;- read_csv(\"data/wcgs.csv\") |&gt;\n    mutate(across(where(is.character), as_factor))\n\nwcgs\n\n# A tibble: 3,154 × 22\n      id   age agec  height weight lnwght wghtcat   bmi   sbp lnsbp   dbp  chol\n   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  2343    50 46-50     67    200   5.30 170-200  31.3   132  4.88    90   249\n 2  3656    51 51-55     73    192   5.26 170-200  25.3   120  4.79    74   194\n 3  3526    59 56-60     70    200   5.30 170-200  28.7   158  5.06    94   258\n 4 22057    51 51-55     69    150   5.01 140-170  22.1   126  4.84    80   173\n 5 12927    44 41-45     71    160   5.08 140-170  22.3   126  4.84    80   214\n 6 16029    47 46-50     64    158   5.06 140-170  27.1   116  4.75    76   206\n 7  3894    40 35-40     70    162   5.09 140-170  23.2   122  4.80    78   190\n 8 11389    41 41-45     70    160   5.08 140-170  23.0   130  4.87    84   212\n 9 12681    50 46-50     71    195   5.27 170-200  27.2   112  4.72    70   130\n10 10005    43 41-45     68    187   5.23 170-200  28.4   120  4.79    80   233\n# ℹ 3,144 more rows\n# ℹ 10 more variables: behpat &lt;fct&gt;, dibpat &lt;fct&gt;, smoke &lt;fct&gt;, ncigs &lt;dbl&gt;,\n#   arcus &lt;dbl&gt;, chd69 &lt;fct&gt;, typchd69 &lt;dbl&gt;, time169 &lt;dbl&gt;, t1 &lt;dbl&gt;,\n#   uni &lt;dbl&gt;\n\n\nHere, we have 3154 rows (subjects) and 22 columns (variables).\n\n17.2.1 Structure of wcgs\n\nWe can specify the (sometimes terrible) variable names, through the names function, or we can add other elements of the structure, so that we can identify items of particular interest.\n\nstr(wcgs)\n\ntibble [3,154 × 22] (S3: tbl_df/tbl/data.frame)\n $ id      : num [1:3154] 2343 3656 3526 22057 12927 ...\n $ age     : num [1:3154] 50 51 59 51 44 47 40 41 50 43 ...\n $ agec    : Factor w/ 5 levels \"46-50\",\"51-55\",..: 1 2 3 2 4 1 5 4 1 4 ...\n $ height  : num [1:3154] 67 73 70 69 71 64 70 70 71 68 ...\n $ weight  : num [1:3154] 200 192 200 150 160 158 162 160 195 187 ...\n $ lnwght  : num [1:3154] 5.3 5.26 5.3 5.01 5.08 ...\n $ wghtcat : Factor w/ 4 levels \"170-200\",\"140-170\",..: 1 1 1 2 2 2 2 2 1 1 ...\n $ bmi     : num [1:3154] 31.3 25.3 28.7 22.1 22.3 ...\n $ sbp     : num [1:3154] 132 120 158 126 126 116 122 130 112 120 ...\n $ lnsbp   : num [1:3154] 4.88 4.79 5.06 4.84 4.84 ...\n $ dbp     : num [1:3154] 90 74 94 80 80 76 78 84 70 80 ...\n $ chol    : num [1:3154] 249 194 258 173 214 206 190 212 130 233 ...\n $ behpat  : Factor w/ 4 levels \"A1\",\"A2\",\"B3\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ dibpat  : Factor w/ 2 levels \"Type A\",\"Type B\": 1 1 1 1 1 1 1 1 1 1 ...\n $ smoke   : Factor w/ 2 levels \"Yes\",\"No\": 1 1 2 2 2 1 2 1 2 1 ...\n $ ncigs   : num [1:3154] 25 25 0 0 0 80 0 25 0 25 ...\n $ arcus   : num [1:3154] 1 0 1 1 0 0 0 0 1 0 ...\n $ chd69   : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ typchd69: num [1:3154] 0 0 0 0 0 0 0 0 0 0 ...\n $ time169 : num [1:3154] 1367 2991 2960 3069 3081 ...\n $ t1      : num [1:3154] -1.63 -4.06 0.64 1.12 2.43 ...\n $ uni     : num [1:3154] 0.486 0.186 0.728 0.624 0.379 ...\n\n\n\n17.2.2 Codebook for wcgs\n\nThis table was lovingly hand-crafted, and involved a lot of typing. We’ll look for better ways in 432.\n\n\n\n\n\n\n\n\nName\nStored As\nType\nDetails (units, levels, etc.)\n\n\n\nid\ninteger\n(nominal)\nID #, nominal and uninteresting\n\n\nage\ninteger\nquantitative\nage, in years - no decimal places\n\n\nagec\nfactor (5)\n(ordinal)\nage: 35-40, 41-45, 46-50, 51-55, 56-60\n\n\nheight\ninteger\nquantitative\nheight, in inches\n\n\nweight\ninteger\nquantitative\nweight, in pounds\n\n\nlnwght\nnumber\nquantitative\nnatural logarithm of weight\n\n\nwghtcat\nfactor (4)\n(ordinal)\nwt: &lt; 140, 140-170, 170-200, &gt; 200\n\n\nbmi\nnumber\nquantitative\nbody-mass index:  703 * weight in lb / (height in in)2\n\n\n\nsbp\ninteger\nquantitative\nsystolic blood pressure, in mm Hg\n\n\nlnsbp\nnumber\nquantitative\nnatural logarithm of sbp\n\n\ndbp\ninteger\nquantitative\ndiastolic blood pressure, mm Hg\n\n\nchol\ninteger\nquantitative\ntotal cholesterol, mg/dL\n\n\nbehpat\nfactor (4)\n(nominal)\nbehavioral pattern: A1, A2, B3 or B4\n\n\ndibpat\nfactor (2)\n(binary)\nbehavioral pattern: A or B\n\n\nsmoke\nfactor (2)\n(binary)\ncigarette smoker: Yes or No\n\n\nncigs\ninteger\nquantitative\nnumber of cigarettes smoked per day\n\n\narcus\ninteger\n(nominal)\narcus senilis present (1) or absent (0)\n\n\nchd69\nfactor (2)\n(binary)\nCHD event: Yes or No\n\n\ntypchd69\ninteger\n(4 levels)\nevent: 0 = no CHD, 1 = MI or SD,  2 = silent MI, 3 = angina\n\n\ntime169\ninteger\nquantitative\nfollow-up time in days\n\n\nt1\nnumber\nquantitative\nheavy-tailed (random draws)\n\n\nuni\nnumber\nquantitative\nlight-tailed (random draws)\n\n\n\n17.2.3 Quick Summary\n\nsummary(wcgs)\n\n       id             age           agec          height          weight   \n Min.   : 2001   Min.   :39.00   46-50: 750   Min.   :60.00   Min.   : 78  \n 1st Qu.: 3741   1st Qu.:42.00   51-55: 528   1st Qu.:68.00   1st Qu.:155  \n Median :11406   Median :45.00   56-60: 242   Median :70.00   Median :170  \n Mean   :10478   Mean   :46.28   41-45:1091   Mean   :69.78   Mean   :170  \n 3rd Qu.:13115   3rd Qu.:50.00   35-40: 543   3rd Qu.:72.00   3rd Qu.:182  \n Max.   :22101   Max.   :59.00                Max.   :78.00   Max.   :320  \n                                                                           \n     lnwght         wghtcat          bmi             sbp            lnsbp      \n Min.   :4.357   170-200:1171   Min.   :11.19   Min.   : 98.0   Min.   :4.585  \n 1st Qu.:5.043   140-170:1538   1st Qu.:22.96   1st Qu.:120.0   1st Qu.:4.787  \n Median :5.136   &gt; 200  : 213   Median :24.39   Median :126.0   Median :4.836  \n Mean   :5.128   &lt; 140  : 232   Mean   :24.52   Mean   :128.6   Mean   :4.850  \n 3rd Qu.:5.204                  3rd Qu.:25.84   3rd Qu.:136.0   3rd Qu.:4.913  \n Max.   :5.768                  Max.   :38.95   Max.   :230.0   Max.   :5.438  \n                                                                               \n      dbp              chol       behpat       dibpat     smoke     \n Min.   : 58.00   Min.   :103.0   A1: 264   Type A:1589   Yes:1502  \n 1st Qu.: 76.00   1st Qu.:197.2   A2:1325   Type B:1565   No :1652  \n Median : 80.00   Median :223.0   B3:1216                           \n Mean   : 82.02   Mean   :226.4   B4: 349                           \n 3rd Qu.: 86.00   3rd Qu.:253.0                                     \n Max.   :150.00   Max.   :645.0                                     \n                  NA's   :12                                        \n     ncigs          arcus        chd69         typchd69         time169    \n Min.   : 0.0   Min.   :0.0000   No :2897   Min.   :0.0000   Min.   :  18  \n 1st Qu.: 0.0   1st Qu.:0.0000   Yes: 257   1st Qu.:0.0000   1st Qu.:2842  \n Median : 0.0   Median :0.0000              Median :0.0000   Median :2942  \n Mean   :11.6   Mean   :0.2985              Mean   :0.1363   Mean   :2684  \n 3rd Qu.:20.0   3rd Qu.:1.0000              3rd Qu.:0.0000   3rd Qu.:3037  \n Max.   :99.0   Max.   :1.0000              Max.   :3.0000   Max.   :3430  \n                NA's   :2                                                  \n       t1                 uni           \n Min.   :-47.43147   Min.   :0.0007097  \n 1st Qu.: -1.00337   1st Qu.:0.2573755  \n Median :  0.00748   Median :0.5157779  \n Mean   : -0.03336   Mean   :0.5052159  \n 3rd Qu.:  0.97575   3rd Qu.:0.7559902  \n Max.   : 47.01623   Max.   :0.9994496  \n NA's   :39                             \n\n\nFor a more detailed description, we might consider Hmisc::describe, psych::describe, mosaic::inspect, etc., as we’ve done (for instance) in Chapter 3 and Chapter 7.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The WCGS</span>"
    ]
  },
  {
    "objectID": "17-wcgs.html#are-the-sbps-normally-distributed",
    "href": "17-wcgs.html#are-the-sbps-normally-distributed",
    "title": "17  The WCGS",
    "section": "\n17.3 Are the SBPs Normally Distributed?",
    "text": "17.3 Are the SBPs Normally Distributed?\nConsider the question of whether the distribution of the systolic blood pressure results is well-approximated by the Normal, where we’ll make use of tools based on our discussion in Chapter 11.\n\nres &lt;- mosaic::favstats(~ sbp, data = wcgs)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nbin_w &lt;- 5 # specify binwidth\n\nggplot(wcgs, aes(x = sbp)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"orchid\", \n                   col = \"blue\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"navy\") +\n    labs(title = \"Systolic BP for `wcgs` subjects\",\n     x = \"Systolic BP (mm Hg)\", y = \"\",\n     caption = \"Superimposed Normal model\")\n\n\n\n\n\n\n\nSince the data contain both sbp and lnsbp (its natural logarithm), let’s compare them. Note that in preparing the graph, we’ll need to change the location for the text annotation.\n\nres &lt;- mosaic::favstats(~ lnsbp, data = wcgs)\nbin_w &lt;- 0.05 # specify binwidth\n\nggplot(wcgs, aes(x = lnsbp)) +\n    geom_histogram(binwidth = bin_w, \n                   fill = \"orange\", \n                   col = \"blue\") +\n    stat_function(\n        fun = function(x) dnorm(x, mean = res$mean, \n                                sd = res$sd) * \n            res$n * bin_w,\n        col = \"navy\") +\n    labs(title = \"ln(Systolic BP) for `wcgs` subjects\",\n     x = \"ln(Systolic BP)\", y = \"\",\n     caption = \"Superimposed Normal model\")\n\n\n\n\n\n\n\nWe can also look at Normal Q-Q plots, for instance…\n\np1 &lt;- ggplot(wcgs, aes(sample = sbp)) +\n    geom_qq(color = \"orchid\") + \n    geom_qq_line(color = \"red\") +\n    labs(y = \"Ordered SBP\", title = \"sbp in wcgs\")\n\np2 &lt;- ggplot(wcgs, aes(sample = lnsbp)) +\n    geom_qq(color = \"orange\") + \n    geom_qq_line(color = \"red\") +\n    labs(y = \"Ordered ln(SBP)\", title = \"ln(sbp) in wcgs\")\n\n## next step requires library(patchwork)\n\np1 + p2 + \n    plot_annotation(title = \"Normal Q-Q plots of SBP and ln(SBP) in wcgs\")\n\n\n\n\n\n\n\nThere’s at best a small improvement from sbp to lnsbp in terms of approximation by a Normal distribution.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The WCGS</span>"
    ]
  },
  {
    "objectID": "17-wcgs.html#identifying-and-describing-sbp-outliers",
    "href": "17-wcgs.html#identifying-and-describing-sbp-outliers",
    "title": "17  The WCGS",
    "section": "\n17.4 Identifying and Describing SBP outliers",
    "text": "17.4 Identifying and Describing SBP outliers\nIt looks like there’s an outlier (or a series of them) in the SBP data.\n\nggplot(wcgs, aes(x = \"\", y = sbp)) +\n    geom_violin() +\n    geom_boxplot(width = 0.3, fill = \"royalblue\", \n                 outlier.color = \"royalblue\") +\n    labs(title = \"Boxplot with Violin of SBP in `wcgs` data\",\n         y = \"Systolic Blood Pressure (mm Hg)\", \n         x = \"\") +\n    coord_flip() \n\n\n\n\n\n\n\n\nmosaic::favstats(wcgs$sbp)\n\n min  Q1 median  Q3 max     mean       sd    n missing\n  98 120    126 136 230 128.6328 15.11773 3154       0\n\nHmisc::describe(wcgs$sbp)\n\nwcgs$sbp \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3154        0       62    0.996    128.6    16.25      110      112 \n     .25      .50      .75      .90      .95 \n     120      126      136      148      156 \n\nlowest :  98 100 102 104 106, highest: 200 208 210 212 230\n\n\nThe maximum value here is 230, and is clearly the most extreme value in the data set. One way to gauge this is to describe that observation’s Z score, the number of standard deviations away from the mean that the observation falls. Here, the maximum value, 230 is 6.71 standard deviations above the mean, and thus has a Z score of 6.7.\nA negative Z score would indicate a point below the mean, while a positive Z score indicates, as we’ve seen, a point above the mean. The minimum systolic blood pressure, 98 is 2.03 standard deviations below the mean, so it has a Z score of -2.\nRecall that the Empirical Rule (described in Chapter 11) suggests that if a variable follows a Normal distribution, it would have approximately 95% of its observations falling inside a Z score of (-2, 2), and 99.74% falling inside a Z score range of (-3, 3). Do the systolic blood pressures appear Normally distributed?",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The WCGS</span>"
    ]
  },
  {
    "objectID": "17-wcgs.html#does-weight-category-relate-to-sbp",
    "href": "17-wcgs.html#does-weight-category-relate-to-sbp",
    "title": "17  The WCGS",
    "section": "\n17.5 Does Weight Category Relate to SBP?",
    "text": "17.5 Does Weight Category Relate to SBP?\nThe data are collected into four groups based on the subject’s weight (in pounds).\n\nggplot(wcgs, aes(x = wghtcat, y = sbp)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = wghtcat), width = 0.3, notch = TRUE) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") + \n    labs(title = \"Boxplot of Systolic BP by Weight Category in WCGS\", \n         x = \"Weight Category\", y = \"Systolic Blood Pressure\")",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The WCGS</span>"
    ]
  },
  {
    "objectID": "17-wcgs.html#re-leveling-a-factor",
    "href": "17-wcgs.html#re-leveling-a-factor",
    "title": "17  The WCGS",
    "section": "\n17.6 Re-Leveling a Factor",
    "text": "17.6 Re-Leveling a Factor\nWell, that’s not so good. We really want those weight categories (the levels) to be ordered more sensibly.\n\nwcgs |&gt; tabyl(wghtcat)\n\n wghtcat    n    percent\n 170-200 1171 0.37127457\n 140-170 1538 0.48763475\n   &gt; 200  213 0.06753329\n   &lt; 140  232 0.07355739\n\n\nLike all factor variables in R, the categories are specified as levels. We want to change the order of the levels in a new version of this factor variable so they make sense. There are multiple ways to do this, but I prefer the fct_relevel function from the forcats package (part of the tidyverse.) Which order is more appropriate?\nI’ll add a new variable to the wcgs data called weight_f that relevels the wghtcat data.\n\nwcgs &lt;- wcgs |&gt;\n    mutate(weight_f = fct_relevel(wghtcat, \"&lt; 140\", \"140-170\", \"170-200\", \"&gt; 200\"))\n\nwcgs |&gt; tabyl(weight_f)\n\n weight_f    n    percent\n    &lt; 140  232 0.07355739\n  140-170 1538 0.48763475\n  170-200 1171 0.37127457\n    &gt; 200  213 0.06753329\n\n\nFor more on the forcats package, check out Hadley Wickham and Grolemund (2023), especially its section on Factors.\n\n17.6.1 SBP by Weight Category\n\nggplot(wcgs, aes(x = weight_f, y = sbp, fill = weight_f)) +\n    geom_boxplot(notch = TRUE) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"Systolic Blood Pressure by Reordered Weight Category in WCGS\", \n         x = \"Weight Category\", y = \"Systolic Blood Pressure\")\n\n\n\n\n\n\n\nWe might see some details well with a ridgeline plot, too.\n\nggplot(wcgs, aes(x = sbp, y = weight_f, fill = weight_f, height = ..density..)) +\n    ggridges::geom_density_ridges(scale = 2) +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") +\n    labs(title = \"SBP by Weight Category (wcgs)\",\n         x = \"Systolic Blood Pressure\",\n         y = \"Weight Category\") \n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\nPicking joint bandwidth of 3.74\n\n\n\n\n\n\n\n\nAs the plots suggest, patients in the heavier groups generally had higher systolic blood pressures.\n\nmosaic::favstats(sbp ~ weight_f, data = wcgs)\n\n  weight_f min  Q1 median  Q3 max     mean       sd    n missing\n1    &lt; 140  98 112    120 130 196 123.1379 14.73394  232       0\n2  140-170 100 118    124 134 192 126.2939 13.65294 1538       0\n3  170-200 100 120    130 140 230 131.1136 15.57024 1171       0\n4    &gt; 200 110 126    132 150 212 137.8685 16.75522  213       0",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The WCGS</span>"
    ]
  },
  {
    "objectID": "17-wcgs.html#are-weight-and-sbp-linked",
    "href": "17-wcgs.html#are-weight-and-sbp-linked",
    "title": "17  The WCGS",
    "section": "\n17.7 Are Weight and SBP Linked?",
    "text": "17.7 Are Weight and SBP Linked?\nLet’s build a scatter plot of SBP (Outcome) by Weight (Predictor), rather than breaking down into categories.\n\nggplot(wcgs, aes(x = weight, y = sbp)) +\n    geom_point(size=3, shape=1, color=\"forestgreen\") + ## default size = 2\n    geom_smooth(method = \"lm\", se = FALSE, col = \"red\", formula = y ~ x) +\n    geom_smooth(method = \"loess\", col = \"blue\", formula = y ~ x) +\n    ggtitle(\"SBP vs. Weight in 3,154 WCGS Subjects\") \n\n\n\n\n\n\n\n\nThe mass of the data is hidden from us - showing 3154 points in one plot can produce little more than a blur where there are lots of points on top of each other.\nHere the least squares regression line (in red), and loess scatterplot smoother, (in blue) can help.\n\nThe relationship between systolic blood pressure and weight appears to be very close to linear, but of course there is considerable scatter around that generally linear relationship. It turns out that the Pearson correlation of these two variables is 0.253.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The WCGS</span>"
    ]
  },
  {
    "objectID": "17-wcgs.html#sbp-and-weight-by-arcus-senilis-groups",
    "href": "17-wcgs.html#sbp-and-weight-by-arcus-senilis-groups",
    "title": "17  The WCGS",
    "section": "\n17.8 SBP and Weight by Arcus Senilis groups?",
    "text": "17.8 SBP and Weight by Arcus Senilis groups?\nAn issue of interest to us will be to assess whether the SBP-Weight relationship we see above is similar among subjects who have been diagnosed with arcus senilis and those who have not.\n\nArcus senilis is an old age syndrome where there is a white, grey, or blue opaque ring in the corneal margin (peripheral corneal opacity), or white ring in front of the periphery of the iris. It is present at birth but then fades; however, it is quite commonly present in the elderly. It can also appear earlier in life as a result of hypercholesterolemia.\nWikipedia article on Arcus Senilis, retrieved 2017-08-15\n\nLet’s start with a quick look at the arcus data.\n\nwcgs |&gt; tabyl(arcus)\n\n arcus    n      percent valid_percent\n     0 2211 0.7010145847     0.7014594\n     1  941 0.2983512999     0.2985406\n    NA    2 0.0006341154            NA\n\n\nWe have 2 missing values, so we probably want to do something about that before plotting the data, and we may also want to create a factor variable with more meaningful labels than 1 (which means yes, arcus senilis is present) and 0 (which means no, it isn’t.)\n\nwcgs &lt;- wcgs |&gt;\n    mutate(arcus_f = fct_recode(factor(arcus),\n                                \"Arcus senilis\" = \"1\",\n                                \"No arcus senilis\" = \"0\"),\n           arcus_f = fct_relevel(arcus_f, \"Arcus senilis\"))\n\nwcgs |&gt; tabyl(arcus_f, arcus)\n\n          arcus_f    0   1 NA_\n    Arcus senilis    0 941   0\n No arcus senilis 2211   0   0\n             &lt;NA&gt;    0   0   2\n\n\nLet’s build a version of the wcgs data that eliminates all missing data in the variables of immediate interest, and then plot the SBP-weight relationship in groups of patients with and without arcus senilis.\n\nwcgs_temp &lt;- wcgs |&gt;\n    filter(complete.cases(arcus_f, sbp, weight)) \n\nggplot(wcgs_temp, aes(x = weight, y = sbp, group = arcus_f)) +\n    geom_point(shape = 1) + \n    geom_smooth(method = \"lm\", col = \"red\", formula = y ~ x) +\n    geom_smooth(method = \"loess\", se = FALSE, col = \"blue\", formula = y ~ x) +\n    labs(title = \"SBP vs. Weight by Arcus Senilis status\",\n         caption = \"3,152 WCGS with known arcus senilis status\") + \n    facet_wrap(~ arcus_f)",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The WCGS</span>"
    ]
  },
  {
    "objectID": "17-wcgs.html#linear-model-for-sbp-weight-relationship-subjects-without-arcus-senilis",
    "href": "17-wcgs.html#linear-model-for-sbp-weight-relationship-subjects-without-arcus-senilis",
    "title": "17  The WCGS",
    "section": "\n17.9 Linear Model for SBP-Weight Relationship: subjects without Arcus Senilis",
    "text": "17.9 Linear Model for SBP-Weight Relationship: subjects without Arcus Senilis\n\nmodel.noarcus &lt;- \n    lm(sbp ~ weight, data = filter(wcgs, arcus == 0))\n\ntidy(model.noarcus) |&gt; \n    kbl(digits = 2) |&gt;\n    kable_styling(full_width = FALSE)\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n95.92\n2.56\n37.54\n0\n\n\nweight\n0.19\n0.01\n12.77\n0\n\n\n\n\nglance(model.noarcus) |&gt; \n    select(r.squared:p.value, AIC) |&gt; \n    kbl(digits = c(3, 3, 1, 1, 3, 0)) |&gt;\n    kable_styling(full_width = FALSE)\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\nAIC\n\n\n0.069\n0.068\n14.8\n163\n0\n18194\n\n\n\nsummary(model.noarcus)\n\n\nCall:\nlm(formula = sbp ~ weight, data = filter(wcgs, arcus == 0))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.011 -10.251  -2.447   7.553  99.848 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  95.9219     2.5552   37.54   &lt;2e-16 ***\nweight        0.1902     0.0149   12.77   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.8 on 2209 degrees of freedom\nMultiple R-squared:  0.0687,    Adjusted R-squared:  0.06828 \nF-statistic:   163 on 1 and 2209 DF,  p-value: &lt; 2.2e-16\n\n\nThe linear model for the 2211 patients without Arcus Senilis has R-squared = 6.87%.\n\nThe regression equation is 95.92 - 0.19 weight, for those patients without Arcus Senilis.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The WCGS</span>"
    ]
  },
  {
    "objectID": "17-wcgs.html#linear-model-for-sbp-weight-relationship-subjects-with-arcus-senilis",
    "href": "17-wcgs.html#linear-model-for-sbp-weight-relationship-subjects-with-arcus-senilis",
    "title": "17  The WCGS",
    "section": "\n17.10 Linear Model for SBP-Weight Relationship: subjects with Arcus Senilis",
    "text": "17.10 Linear Model for SBP-Weight Relationship: subjects with Arcus Senilis\n\nmodel.witharcus &lt;- \n    lm(sbp ~ weight, data = filter(wcgs, arcus == 1))\n\ntidy(model.witharcus) |&gt; \n    kbl(digits = 2) |&gt;\n    kable_styling(full_width = FALSE)\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n101.88\n3.76\n27.13\n0\n\n\nweight\n0.16\n0.02\n7.39\n0\n\n\n\n\nglance(model.witharcus) |&gt; \n    select(r.squared:p.value, AIC) |&gt; \n    kbl(digits = c(3, 3, 1, 1, 3, 0)) |&gt;\n    kable_styling(full_width = FALSE)\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\nAIC\n\n\n0.055\n0.054\n14.2\n54.6\n0\n7667\n\n\n\nsummary(model.witharcus)\n\n\nCall:\nlm(formula = sbp ~ weight, data = filter(wcgs, arcus == 1))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-30.335  -9.636  -1.961   7.973  76.738 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 101.87847    3.75572  27.126  &lt; 2e-16 ***\nweight        0.16261    0.02201   7.388 3.29e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.19 on 939 degrees of freedom\nMultiple R-squared:  0.05494,   Adjusted R-squared:  0.05393 \nF-statistic: 54.58 on 1 and 939 DF,  p-value: 3.29e-13\n\n\nThe linear model for the 941 patients with Arcus Senilis has R-squared = 5.49%.\n\nThe regression equation is 101.88 - 0.163 weight, for those patients with Arcus Senilis.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The WCGS</span>"
    ]
  },
  {
    "objectID": "17-wcgs.html#including-arcus-status-in-the-model",
    "href": "17-wcgs.html#including-arcus-status-in-the-model",
    "title": "17  The WCGS",
    "section": "\n17.11 Including Arcus Status in the model",
    "text": "17.11 Including Arcus Status in the model\n\nmodel3 &lt;- lm(sbp ~ weight * arcus, data = filter(wcgs, !is.na(arcus)))\n\ntidy(model3) |&gt; \n    kbl(digits = 2) |&gt;\n    kable_styling(full_width = FALSE)\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n95.92\n2.52\n38.00\n0.00\n\n\nweight\n0.19\n0.01\n12.92\n0.00\n\n\narcus\n5.96\n4.62\n1.29\n0.20\n\n\nweight:arcus\n-0.03\n0.03\n-1.02\n0.31\n\n\n\n\nglance(model3) |&gt; \n    select(r.squared:p.value, AIC) |&gt; \n    kbl(digits = c(3, 3, 1, 1, 3, 0)) |&gt;\n    kable_styling(full_width = FALSE)\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\nAIC\n\n\n0.066\n0.065\n14.6\n74.1\n0\n25861\n\n\n\nsummary(model3)\n\n\nCall:\nlm(formula = sbp ~ weight * arcus, data = filter(wcgs, !is.na(arcus)))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-30.335 -10.152  -2.349   7.669  99.848 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  95.92190    2.52440  37.998   &lt;2e-16 ***\nweight        0.19017    0.01472  12.921   &lt;2e-16 ***\narcus         5.95657    4.61972   1.289    0.197    \nweight:arcus -0.02756    0.02703  -1.019    0.308    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.62 on 3148 degrees of freedom\nMultiple R-squared:  0.06595,   Adjusted R-squared:  0.06506 \nF-statistic: 74.09 on 3 and 3148 DF,  p-value: &lt; 2.2e-16\n\n\nThe actual regression equation in this setting includes both weight, and an indicator variable (1 = yes, 0 = no) for arcus senilis status, and the product term combining weight and that 1/0 indicator. In 432, we’ll spend substantial time and energy discussing these product terms, but we’ll not do much of that in 431.\n\nNote the use of the product term weight*arcus in the setup of the model to allow both the slope of weight and the intercept term in the model to change depending on arcus senilis status.\n\nFor a patient who has arcus, the regression equation is SBP = 95.92 + 0.19 weight + 5.96 (1) - 0.028 weight (1) = 101.88 + 0.162 weight.\nFor a patient without arcus senilis, the regression equation is SBP = 95.92 + 0.19 weight + 5.96 (0) - 0.028 weight (0) = 95.92 + 0.19 weight.\n\n\n\nThe linear model including the interaction of weight and arcus to predict sbp for the 3152 patients with known Arcus Senilis status has R-squared = 6.6%. Again, we’ll discuss interaction more substantially in 432.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The WCGS</span>"
    ]
  },
  {
    "objectID": "17-wcgs.html#predictions-from-these-linear-models",
    "href": "17-wcgs.html#predictions-from-these-linear-models",
    "title": "17  The WCGS",
    "section": "\n17.12 Predictions from these Linear Models",
    "text": "17.12 Predictions from these Linear Models\nWhat is our predicted SBP for a subject weighing 175 pounds?\nHow does that change if our subject weighs 200 pounds?\nRecall that\n\n\nWithout Arcus Senilis, linear model for SBP = 95.9 + 0.19 x weight\n\nWith Arcus Senilis, linear model for SBP = 101.9 + 0.16 x weight\n\nSo the predictions for a 175 pound subject are:\n\n95.9 + 0.19 x 175 = 129 mm Hg without Arcus Senilis, and\n101.9 + 0.16 x 175 = 130 mm Hg with Arcus Senilis.\n\nAnd thus, the predictions for a 200 pound subject are:\n\n95.9 + 0.19 x 200 = 134 mm Hg without Arcus Senilis, and\n101.9 + 0.16 x 200 = 134.4 mm Hg with Arcus Senilis.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The WCGS</span>"
    ]
  },
  {
    "objectID": "17-wcgs.html#scatterplots-with-facets-across-a-categorical-variable",
    "href": "17-wcgs.html#scatterplots-with-facets-across-a-categorical-variable",
    "title": "17  The WCGS",
    "section": "\n17.13 Scatterplots with Facets Across a Categorical Variable",
    "text": "17.13 Scatterplots with Facets Across a Categorical Variable\nWe can use facets in ggplot2 to show scatterplots across the levels of a categorical variable, like behpat.\n\nggplot(wcgs, aes(x = weight, y = sbp, col = behpat)) +\n    geom_point() +\n    facet_wrap(~ behpat) +\n    geom_smooth(method = \"lm\", se = FALSE, \n                formula = y ~ x, col = \"black\") +\n    guides(color = \"none\") +\n    theme(strip.text = element_text(face=\"bold\", size=rel(1.25), color=\"white\"),\n          strip.background = element_rect(fill=\"royalblue\")) +\n    labs(title = \"Scatterplots of SBP vs. Weight within Behavior Pattern\")",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The WCGS</span>"
    ]
  },
  {
    "objectID": "17-wcgs.html#scatterplot-and-correlation-matrices",
    "href": "17-wcgs.html#scatterplot-and-correlation-matrices",
    "title": "17  The WCGS",
    "section": "\n17.14 Scatterplot and Correlation Matrices",
    "text": "17.14 Scatterplot and Correlation Matrices\nA scatterplot matrix can be very helpful in understanding relationships between multiple variables simultaneously. There are several ways to build such a thing, including the pairs function…\n\npairs (~ sbp + age + weight + height, data=wcgs, \n       main=\"Simple Scatterplot Matrix\")\n\n\n\n\n\n\n\n\n17.14.1 Displaying a Correlation Matrix\n\nwcgs |&gt;\n    select(sbp, age, weight, height) |&gt;\n    cor() |&gt;\n    kbl(digits = 3) |&gt;\n    kable_styling(full_width = FALSE)\n\n\n\n\nsbp\nage\nweight\nheight\n\n\n\nsbp\n1.000\n0.166\n0.253\n0.018\n\n\nage\n0.166\n1.000\n-0.034\n-0.095\n\n\nweight\n0.253\n-0.034\n1.000\n0.533\n\n\nheight\n0.018\n-0.095\n0.533\n1.000\n\n\n\n\n\n\n17.14.2 Using the GGally package\nThe ggplot2 system doesn’t have a built-in scatterplot system. There are some nice add-ins in the world, though. One option I sort of like is in the GGally package, which can produce both correlation matrices and scatterplot matrices.\nThe ggpairs function provides a density plot on each diagonal, Pearson correlations on the upper right and scatterplots on the lower left of the matrix.\n\nggpairs(wcgs |&gt; \n          select(sbp, age, weight, height),\n        title = \"Scatterplot Matrix via ggpairs\")\n\n\n\n\n\n\n\n\n\n\n\nHadley Wickham, Mine Çetinyaka-Rundel, and Garrett Grolemund. 2023. R for Data Science. Second. O’Reilly. https://r4ds.hadley.nz/.\n\n\nVittinghoff, Eric, David V. Glidden, Stephen C. Shiboski, and Charles E. McCulloch. 2012. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. Second. Springer-Verlag, Inc. http://www.biostat.ucsf.edu/vgsm/.",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The WCGS</span>"
    ]
  },
  {
    "objectID": "17-wcgs.html#footnotes",
    "href": "17-wcgs.html#footnotes",
    "title": "17  The WCGS",
    "section": "",
    "text": "For more on the WCGS, you might look at http://www.epi.umn.edu/cvdepi/study-synopsis/western-collaborative-group-study/↩︎",
    "crumbs": [
      "Part A. Exploring Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The WCGS</span>"
    ]
  },
  {
    "objectID": "18-testing_intro.html",
    "href": "18-testing_intro.html",
    "title": "18  Hypothesis Testing: What is it good for?",
    "section": "",
    "text": "18.1 Introduction\nHypothesis testing uses sample data to attempt to reject the hypothesis that nothing interesting is happening – that is, to reject the notion that chance alone can explain the sample results. We can, in many settings, use confidence intervals to summarize the results, as well, and confidence intervals and hypothesis tests are closely connected.\nIn particular, it’s worth stressing that:",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis Testing: What is it good for?</span>"
    ]
  },
  {
    "objectID": "18-testing_intro.html#introduction",
    "href": "18-testing_intro.html#introduction",
    "title": "18  Hypothesis Testing: What is it good for?",
    "section": "",
    "text": "A significant effect is not necessarily the same thing as an interesting effect. For example, results calculated from large samples are nearly always “significant” even when the effects are quite small in magnitude. Before doing a test, always ask if the effect is large enough to be of any practical interest. If not, why do the test?\nA non-significant effect is not necessarily the same thing as no difference. A large effect of real practical interest may still produce a non-significant result simply because the sample is too small.\nThere are assumptions behind all statistical inferences. Checking assumptions is crucial to validating the inference made by any test or confidence interval.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis Testing: What is it good for?</span>"
    ]
  },
  {
    "objectID": "18-testing_intro.html#five-steps-in-any-hypothesis-test",
    "href": "18-testing_intro.html#five-steps-in-any-hypothesis-test",
    "title": "18  Hypothesis Testing: What is it good for?",
    "section": "18.2 Five Steps in any Hypothesis Test",
    "text": "18.2 Five Steps in any Hypothesis Test\n\nSpecify the null hypothesis, \\(H_0\\) (which usually indicates that there is no difference or no association between the results in various groups of subjects)\nSpecify the research or alternative hypothesis, \\(H_A\\), sometimes called \\(H_1\\) (which usually indicates that there is some difference or some association between the results in those same groups of subjects).\nSpecify the test procedure or test statistic to be used to make inferences to the population based on sample data. Here is where we usually specify \\(\\alpha\\), the probability of incorrectly rejecting \\(H_0\\) that we are willing to accept. In the absence of other information, we often use \\(\\alpha = 0.05\\)\nObtain the data, and summarize it to obtain the relevant test statistic, which gets summarized as a \\(p\\) value.\nUse the \\(p\\) value to either\n\nreject \\(H_0\\) in favor of the alternative \\(H_A\\) (concluding that there is a statistically significant difference/association at the \\(\\alpha\\) significance level) or\nretain \\(H_0\\) (and conclude that there is no statistically significant difference/association at the \\(\\alpha\\) significance level)",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis Testing: What is it good for?</span>"
    ]
  },
  {
    "objectID": "18-testing_intro.html#type-i-and-type-ii-error",
    "href": "18-testing_intro.html#type-i-and-type-ii-error",
    "title": "18  Hypothesis Testing: What is it good for?",
    "section": "18.3 Type I and Type II Error",
    "text": "18.3 Type I and Type II Error\nOnce we know how unlikely the results would have been if the null hypothesis were true, we must make one of two choices:\n\nThe p value is not small enough to convincingly rule out chance. Therefore, we cannot reject the null hypothesis as an explanation for the results.\nThe p value was small enough to convincingly rule out chance. We reject the null hypothesis and accept the alternative hypothesis.\n\nHow small must the p value be in order to rule out the null hypothesis? The standard choice is 5%. This standardization has some substantial disadvantages. It is simply a convention that has become accepted over the years, and there are many situations for which a 5% cutoff is unwise. While it does give a specific level to keep in mind, it suggests a rather mindless cutpoint having nothing to do with the importance of the decision nor the costs or losses associated with outcomes.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis Testing: What is it good for?</span>"
    ]
  },
  {
    "objectID": "18-testing_intro.html#the-courtroom-analogy",
    "href": "18-testing_intro.html#the-courtroom-analogy",
    "title": "18  Hypothesis Testing: What is it good for?",
    "section": "18.4 The Courtroom Analogy",
    "text": "18.4 The Courtroom Analogy\nConsider the analogy of the jury in a courtroom.\n\nThe evidence is not strong enough to convincingly rule out that the defendant is innocent. Therefore, we cannot reject the null hypothesis, or innocence of the defendant.\nThe evidence was strong enough that we are willing to rule out the possibility that an innocent person (as stated in the null hypothesis) produced the observed data. We reject the null hypothesis, that the defendant is innocent, and assert the alternative hypothesis.\n\nConsistent with our thinking in hypothesis testing, in many cases we would not accept the hypothesis that the defendant is innocent. We would simply conclude that the evidence was not strong enough to rule out the possibility of innocence.\nThe p value is the probability of getting a result as extreme or more extreme than the one observed if the proposed null hypothesis is true. Notice that it is not valid to actually accept that the null hypothesis is true. To do so would be to say that we are essentially convinced that chance alone produced the observed results – a common mistake.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis Testing: What is it good for?</span>"
    ]
  },
  {
    "objectID": "18-testing_intro.html#significance-vs.-importance",
    "href": "18-testing_intro.html#significance-vs.-importance",
    "title": "18  Hypothesis Testing: What is it good for?",
    "section": "18.5 Significance vs. Importance",
    "text": "18.5 Significance vs. Importance\nRemember that a statistically significant relationship or difference does not necessarily mean an important one. A result that is significant in the statistical meaning of the word may not be significant clinically. Statistical significance is a technical term. Findings can be both statistically significant and practically significant or either or neither.\nWhen we have large samples, we will regularly find small differences that have a small p value even though they have no practical importance. At the other extreme, with small samples, even large differences will often not be large enough to create a small p value. The notion of statistical significance has not helped science, and we won’t perpetuate it any further.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis Testing: What is it good for?</span>"
    ]
  },
  {
    "objectID": "18-testing_intro.html#what-does-dr.-love-dislike-about-p-values-and-statistical-significance",
    "href": "18-testing_intro.html#what-does-dr.-love-dislike-about-p-values-and-statistical-significance",
    "title": "18  Hypothesis Testing: What is it good for?",
    "section": "18.6 What does Dr. Love dislike about p values and “statistical significance”?",
    "text": "18.6 What does Dr. Love dislike about p values and “statistical significance”?\nA lot of things. A major issue is that I believe that p values are impossible to explain in a way that is both [a] technically correct and [b] straightforward at the same time. As evidence of this, you might want to look at this article and associated video by Christie Aschwanden at 538.com\nThe notion of a p value was an incredibly impressive achievement back when Wald and others were doing the work they were doing in the 1940s, and might still have been useful as recently as 10 years ago. But the notion of a p value relies on a lot of flawed assumptions, and null hypothesis significance testing is fraught with difficulties. Nonetheless, researchers use p values every day.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis Testing: What is it good for?</span>"
    ]
  },
  {
    "objectID": "18-testing_intro.html#the-asa-articles-in-2016-and-2019-on-statistical-significance-and-p-values",
    "href": "18-testing_intro.html#the-asa-articles-in-2016-and-2019-on-statistical-significance-and-p-values",
    "title": "18  Hypothesis Testing: What is it good for?",
    "section": "18.7 The ASA Articles in 2016 and 2019 on Statistical Significance and P-Values",
    "text": "18.7 The ASA Articles in 2016 and 2019 on Statistical Significance and P-Values\nHowever, my primary motivation for taking the approach I’m taking comes from the pieces in two key reference collections we’ll read and discuss more thoroughly in 431 and 432.\n\nThe American Statistical Association’s 2016 Statement on p-Values: Context, Process and Purpose.\n\n\nThe ASA Statement on p-Values and Statistical Significance (Wasserstein and Lazar 2016) was developed primarily because after decades, warnings about the don’ts had gone mostly unheeded. The statement was about what not to do, because there is widespread agreement about the don’ts.\n\n\nStatistical Inference in the 21st Century: A World Beyond p &lt; 0.05 from 2019 in The American Statistician\n\n\nThis is a world where researchers are free to treat “p = 0.051” and “p = 0.049” as not being categorically different, where authors no longer find themselves constrained to selectively publish their results based on a single magic number. In this world, where studies with “p &lt; 0.05” and studies with “p &gt; 0.05” are not automatically in conflict, researchers will see their results more easily replicated-and, even when not, they will better understand why. As we venture down this path, we will begin to see fewer false alarms, fewer overlooked discoveries, and the development of more customized statistical strategies. Researchers will be free to communicate all their findings in all their glorious uncertainty, knowing their work is to be judged by the quality and effective communication of their science, and not by their p-values. As “statistical significance” is used less, statistical thinking will be used more. The ASA Statement on P-Values and Statistical Significance started moving us toward this world…. Now we must go further.\n\n\nThe ASA Statement on P-Values and Statistical Significance stopped just short of recommending that declarations of “statistical significance” be abandoned. We take that step here. We conclude, based on our review of the articles in this special issue and the broader literature, that it is time to stop using the term “statistically significant” entirely. Nor should variants such as “significantly different,” “p &lt; 0.05,” and “nonsignificant” survive, whether expressed in words, by asterisks in a table, or in some other way…. Regardless of whether it was ever useful, a declaration of “statistical significance” has today become meaningless.\n\nFor the moment, I will say this. I emphasize confidence intervals over p values, which is at best a partial solution. But …\n\nVery rarely does a situation emerge in which a p value can be available in which looking at the associated confidence interval isn’t far more helpful for making a comparison of interest.\nThe use of a p value requires making at least as many assumptions about the population, sample, individuals and data as does a confidence interval.\nMost null hypotheses are clearly not exactly true prior to data collection, and so the test summarized by a p value is of questionable value most of the time.\nNo one has a truly adequate definition of a p value, in terms of both precision and parsimony. Brief, understandable definitions always fail to be technically accurate.\nBayesian approaches avoid some of these pitfalls, but come with their own issues.\nMany smart people agree with me, and have sworn off of p values whenever they can.\n\nAgain, we’ll look at these issues in greater depth later in the course.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis Testing: What is it good for?</span>"
    ]
  },
  {
    "objectID": "18-testing_intro.html#errors-in-hypothesis-testing",
    "href": "18-testing_intro.html#errors-in-hypothesis-testing",
    "title": "18  Hypothesis Testing: What is it good for?",
    "section": "18.8 Errors in Hypothesis Testing",
    "text": "18.8 Errors in Hypothesis Testing\nIn testing hypotheses, there are two potential decisions and each one brings with it the possibility that a mistake has been made.\nLet’s use the courtroom analogy. Here are the potential choices and associated potential errors. Although the seriousness of errors depends on the seriousness of the crime and punishment, the potential error for choice 2 is usually more serious.\n\nWe cannot rule out that the defendant is innocent, so (s)he is set free without penalty.\n\nPotential Error: A criminal has been erroneously freed.\n\nWe believe that there is enough evidence to conclude that the defendant is guilty.\n\nPotential Error: An innocent person is convicted / penalized and a guilty person remains free.\n\n\nAs another example, consider being tested for disease. Most tests for diseases are not 100% accurate. The lab technician or physician must make a choice:\n\nIn the opinion of the medical practitioner, you are healthy. The test result was weak enough to be called “negative” for the disease.\n\nPotential Error: You actually have the disease but have been told that you do not. This is called a false negative.\n\nIn the opinion of the medical practitioner, you have the disease. The test results were strong enough to be called “positive” for the disease.\n\nPotential Error: You are actually healthy but have been told you have the disease. This is called a false positive.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis Testing: What is it good for?</span>"
    ]
  },
  {
    "objectID": "18-testing_intro.html#the-two-types-of-hypothesis-testing-errors",
    "href": "18-testing_intro.html#the-two-types-of-hypothesis-testing-errors",
    "title": "18  Hypothesis Testing: What is it good for?",
    "section": "18.9 The Two Types of Hypothesis Testing Errors",
    "text": "18.9 The Two Types of Hypothesis Testing Errors\n\n\n\n\n\n\n\n\n–\n\\(H_A\\) is true\n\\(H_0\\) is true\n\n\n\n\nTest Rejects \\(H_0\\)\nCorrect Decision\nType I Error (False Positive)\n\n\nTest Retains \\(H_0\\)\nType II Error (False Negative)\nCorrect Decision\n\n\n\n\nA Type I error can only be made if the null hypothesis is actually true.\nA Type II error can only be made if the alternative hypothesis is actually true.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis Testing: What is it good for?</span>"
    ]
  },
  {
    "objectID": "18-testing_intro.html#the-significance-level-is-the-probability-of-a-type-i-error",
    "href": "18-testing_intro.html#the-significance-level-is-the-probability-of-a-type-i-error",
    "title": "18  Hypothesis Testing: What is it good for?",
    "section": "18.10 The Significance Level is the Probability of a Type I Error",
    "text": "18.10 The Significance Level is the Probability of a Type I Error\nIf the null hypothesis is true, the p value is the probability of making an error by choosing the alternative hypothesis instead. Alpha (\\(\\alpha\\)) is defined as the probability of rejecting the null hypothesis when the null hypothesis is actually true, creating a Type I error. This is also called the significance level, so that 100(1-\\(\\alpha\\)) is the confidence level – the probability of correctly concluding that there is no difference (retaining \\(H_0\\)) when the null hypothesis is true.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis Testing: What is it good for?</span>"
    ]
  },
  {
    "objectID": "18-testing_intro.html#the-probability-of-avoiding-a-type-ii-error-is-called-power",
    "href": "18-testing_intro.html#the-probability-of-avoiding-a-type-ii-error-is-called-power",
    "title": "18  Hypothesis Testing: What is it good for?",
    "section": "18.11 The Probability of avoiding a Type II Error is called Power",
    "text": "18.11 The Probability of avoiding a Type II Error is called Power\nA Type II error is made if the alternative hypothesis is true, but you fail to choose it. The probability depends on exactly which part of the alternative hypothesis is true, so that computing the probability of making a Type II error is not feasible. The power of a test is the probability of making the correct decision when the alternative hypothesis is true. Beta (\\(\\beta\\)) is defined as the probability of concluding that there was no difference, when in fact there was one (a Type II error). Power is then just 1 - \\(\\beta\\), the probability of concluding that there was a difference, when, in fact, there was one.\nTraditionally, people like the power of a test to be at least 80%, meaning that \\(\\beta\\) is at most 0.20. Often, I’ll be arguing for 90% as a minimum power requirement, or we’ll be presenting a range of power calculations for a variety of sample size choices.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis Testing: What is it good for?</span>"
    ]
  },
  {
    "objectID": "18-testing_intro.html#incorporating-the-costs-of-various-types-of-errors",
    "href": "18-testing_intro.html#incorporating-the-costs-of-various-types-of-errors",
    "title": "18  Hypothesis Testing: What is it good for?",
    "section": "18.12 Incorporating the Costs of Various Types of Errors",
    "text": "18.12 Incorporating the Costs of Various Types of Errors\nWhich error is more serious in medical testing, where we think of our \\(H_0\\): patient is healthy vs. \\(H_A\\): disease is present?\nIt depends on the disease and on the consequences of a negative or positive test result. A false negative in a screening test for cancer could lead to a fatal delay in treatment, whereas a false positive would probably lead to a retest. A more troublesome example occurs in testing for an infectious disease. Inevitably, there is a trade-off between the two types of errors. It all depends on the consequences.\nIt would be nice if we could specify the probability that we were making an error with each potential decision. We could then weigh the consequence of the error against its probability. Unfortunately, in most cases, we can only specify the conditional probability of making a Type I error, given that the null hypothesis is true.\nIn deciding whether to reject a null hypothesis, we will need to consider the consequences of the two potential types of errors. If a Type I error is very serious, then you should reject the null hypothesis only if the p value is very small. Conversely, if a Type II error is more serious, you should be willing to reject the null hypothesis with a larger p value, perhaps 0.10 or 0.20, instead of 0.05.\n\n\\(\\alpha\\) is the probability of rejecting \\(H_0\\) when \\(H_0\\) is true.\n\nSo 1 - \\(\\alpha\\), the confidence level, is the probability of retaining \\(H_0\\) when that’s the right thing to do.\n\n\\(\\beta\\) is the probability of retaining \\(H_0\\) when \\(H_A\\) is true.\n\nSo 1 - \\(\\beta\\), the power, is the probability of rejecting \\(H_0\\) when that’s the right thing to do.\n\n\n\n\n\n\n\n\n\n\n–\n\\(H_A\\) is True\n\\(H_0\\) is True\n\n\n\n\nTest Rejects \\(H_0\\)\nCorrect Decision (1 - \\(\\beta\\))\nType I Error (\\(\\alpha\\))\n\n\nTest Retains \\(H_0\\)\nType II Error (\\(\\beta\\))\nCorrect Decision (1 - \\(\\alpha\\))",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis Testing: What is it good for?</span>"
    ]
  },
  {
    "objectID": "18-testing_intro.html#power-and-sample-size-considerations",
    "href": "18-testing_intro.html#power-and-sample-size-considerations",
    "title": "18  Hypothesis Testing: What is it good for?",
    "section": "18.13 Power and Sample Size Considerations",
    "text": "18.13 Power and Sample Size Considerations\nFor most statistical tests, it is theoretically possible to estimate the power of the test in the design stage, (before any data are collected) for various sample sizes, so we can hone in on a sample size choice which will enable us to collect data only on as many subjects as are truly necessary.\nA power calculation is likely the most common element of an scientific grant proposal on which a statistician is consulted. This is a fine idea in theory, but in practice…\n\nThe tests that have power calculations worked out in intensive detail using R are mostly those with more substantial assumptions. Examples include t tests that assume population normality, common population variance and balanced designs in the independent samples setting, or paired t tests that assume population normality in the paired samples setting.\nThese power calculations are also usually based on tests rather than confidence intervals, which would be much more useful in most settings. Simulation is your friend here.\nEven more unfortunately, this process of doing power and related calculations is far more of an art than a science.\nAs a result, the value of many power calculations is negligible, since the assumptions being made are so arbitrary and poorly connected to real data.\nOn several occasions, I have stood in front of a large audience of medical statisticians actively engaged in clinical trials and other studies that require power calculations for funding. When I ask for a show of hands of people who have had power calculations prior to such a study whose assumptions matched the eventual data perfectly, I get lots of laughs. It doesn’t happen.\nEven the underlying framework that assumes a power of 80% with a significance level of 5% is sufficient for most studies is pretty silly.\n\nAll that said, I feel obliged to show you some examples of power calculations done using R, and provide some insight on how to make some of the key assumptions in a way that won’t alert reviewers too much to the silliness of the enterprise, and so I will do so in Chapter 22.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Hypothesis Testing: What is it good for?</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html",
    "href": "19-ci_for_mean.html",
    "title": "19  Confidence Intervals for a Mean",
    "section": "",
    "text": "19.1 Setup: Packages Used Here\nIn this part of the course, we’ll make use of a few scripts I’ve gathered for you, in the Love-boost R script.\nknitr::opts_chunk$set(comment = NA)\n\nsource(\"data/Love-boost.R\")\nlibrary(boot)\nlibrary(broom)\nlibrary(Hmisc)\nlibrary(janitor)\nlibrary(kableExtra)\nlibrary(patchwork)\nlibrary(psych)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\nWe will also use the favstats function from the mosaic package.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#introduction",
    "href": "19-ci_for_mean.html#introduction",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.2 Introduction",
    "text": "19.2 Introduction\nThe basic theory of estimation can be used to indicate the probable accuracy and potential for bias in estimating based on limited samples. A point estimate provides a single best guess as to the value of a population or process parameter.\nA confidence interval is a particularly useful way to convey to people just how much error one must allow for in a given estimate. In particular, a confidence interval allows us to quantify just how close we expect, for instance, the sample mean to be to the population or process mean. The computer will do the calculations; we need to interpret the results.\nThe key things that we will need to trade off are cost vs. precision, and precision vs. confidence in the correctness of the statement. Often, if we are dissatisfied with the width of the confidence interval and want to make it smaller, we have little choice but to reconsider the sample – larger samples produce shorter intervals.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#this-chapters-goals",
    "href": "19-ci_for_mean.html#this-chapters-goals",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.3 This Chapter’s Goals",
    "text": "19.3 This Chapter’s Goals\nSuppose that we are interested in learning something about a population or process, from which we can obtain a sample that consists of a subset of potential results from that population or process. The main goal for many of the parametric models that are a large part of statistics is to estimate population parameters, like a population mean, or regression coefficient, on the basis of a sample. When we do this, we want to describe not only our best guess at the parameter (referred to as a point estimate) but also say something useful about the uncertainty in our estimate, to let us more completely assess what the data have to tell us. A key tool for doing this is a confidence interval.\nEssentially every textbook on introductory statistics describes the development of a confidence interval, at least for a mean. Good supplemental resources are highlighted in the references I’ve provided in the course syllabus.\nWe’ll develop confidence intervals to compare parameters about two populations (either through matched pairs or independent samples) with confidence intervals soon. Here, we’ll consider the problem of estimating a confidence interval to describe the mean (or median) of the population represented by a single sample of quantitative data.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#serum-zinc-levels-in-462-teenage-males-serzinc",
    "href": "19-ci_for_mean.html#serum-zinc-levels-in-462-teenage-males-serzinc",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.4 Serum Zinc Levels in 462 Teenage Males (serzinc)",
    "text": "19.4 Serum Zinc Levels in 462 Teenage Males (serzinc)\nThe serzinc data include serum zinc levels in micrograms per deciliter that have been gathered for a sample of 462 males aged 15-17, My source for these data is Appendix B1 of Pagano and Gauvreau (2000). Serum zinc deficiency has been associated with anemia, loss of strength and endurance, and it is thought that 25% of the world’s population is at risk of zinc deficiency. Such a deficiency can indicate poor nutrition, and can affect growth and vision, for instance. “Typical” values1 are said to be 0.66-1.10 mcg/ml, which is 66 - 110 micrograms per deciliter.\n\nserzinc &lt;- read_csv(\"data/serzinc.csv\", show_col_types = FALSE)\n\nnames(serzinc)\n\n[1] \"ID\"   \"zinc\"\n\nmosaic::favstats(~ zinc, data = serzinc) |&gt;\n  kbl(digits = 2) |&gt;\n  kable_styling()\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n50\n76\n86\n98\n153\n87.94\n16\n462\n0",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#our-goal-a-confidence-interval-for-the-population-mean",
    "href": "19-ci_for_mean.html#our-goal-a-confidence-interval-for-the-population-mean",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.5 Our Goal: A Confidence Interval for the Population Mean",
    "text": "19.5 Our Goal: A Confidence Interval for the Population Mean\nAfter we assess the data a bit, and are satisfied that we understand it, our first inferential goal will be to produce a confidence interval for the true (population) mean of males age 15-17 based on this sample, assuming that these 462 males are a random sample from the population of interest, that each serum zinc level is drawn independently from an identical distribution describing that population.\nTo do this, we will have several different procedures available, including:\n\nA confidence interval for the population mean based on a t distribution, when we assume that the data are drawn from an approximately Normal distribution, using the sample standard deviation. (Interval corresponding to a t test, and it will be a good choice when the data really are approximately Normally distributed.)\nA resampling approach to generate a bootstrap confidence interval for the population mean, which does not require that we assume either that the population standard deviation is known, nor that the data are drawn from an approximately Normal distribution, but which has some other weaknesses.\nA rank-based procedure called the Wilcoxon signed rank test can also be used to yield a confidence interval statement about the population pseudo-median, a measure of the population distribution’s center (but not the population’s mean).",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#exploratory-data-analysis-for-serum-zinc",
    "href": "19-ci_for_mean.html#exploratory-data-analysis-for-serum-zinc",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.6 Exploratory Data Analysis for Serum Zinc",
    "text": "19.6 Exploratory Data Analysis for Serum Zinc\n\n19.6.1 Graphical Summaries\nThe code presented below builds:\n\na histogram (with Normal model superimposed),\na boxplot (with median notch) and\na Normal Q-Q plot (with guiding straight line through the quartiles)\n\nfor the zinc results from the serzinc tibble.\n\np1 &lt;- ggplot(serzinc, aes(sample = zinc)) +\n  geom_qq(col = \"dodgerblue\") + geom_qq_line(col = \"navy\") + \n  theme(aspect.ratio = 1) + \n  labs(title = \"Normal Q-Q plot\")\n\np2 &lt;- ggplot(serzinc, aes(x = zinc)) +\n  geom_histogram(aes(y = stat(density)), \n                 bins = 10, fill = \"dodgerblue\", col = \"white\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(serzinc$zinc), \n                            sd = sd(serzinc$zinc)),\n                col = \"navy\", lwd = 1.5) +\n  labs(title = \"Histogram and Normal Density\")\n\np3 &lt;- ggplot(serzinc, aes(x = zinc, y = \"\")) +\n  geom_boxplot(fill = \"dodgerblue\", outlier.color = \"dodgerblue\") + \n  stat_summary(fun = \"mean\", geom = \"point\", \n               shape = 23, size = 3, fill = \"white\") +\n  labs(title = \"Boxplot\", y = \"\")\n\np1 + (p2 / p3 + plot_layout(heights = c(4,1))) +\n  plot_annotation(title = \"Serum Zinc (micrograms per deciliter) for 462 Teenage Males\")\n\nWarning: `stat(density)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\nThese results include some of the more useful plots and numerical summaries when assessing shape, center and spread. The zinc data in the serzinc data frame appear to be slightly right skewed, with five outlier values on the high end of the scale, in particular.\n\n19.6.2 Numerical Summaries\nThis section describes some numerical summaries of interest to augment the plots in summarizing the center, spread and shape of the distribution of serum zinc among these 462 teenage males. Note that I use summarise here to be sure that the dplyr package’s summarise/summarize function is used, rather than the one contained in Hmisc. I could also have used dplyr::summarize() to accomplish the same goal.\n\ndescribe(serzinc$zinc) |&gt;  # from psych package\n  kbl(digits = 2) |&gt; kable_styling()\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\nX1\n1\n462\n87.94\n16\n86\n87.17\n16.31\n50\n153\n103\n0.62\n0.87\n0.74\n\n\n\nserzinc |&gt;\n    summarise(mean(zinc), median(zinc), sd(zinc),\n              skew1 = (mean(zinc) - median(zinc))/sd(zinc)) |&gt;\n    kbl(digits = 2) |&gt; kable_styling()\n\n\n\nmean(zinc)\nmedian(zinc)\nsd(zinc)\nskew1\n\n\n87.94\n86\n16\n0.12\n\n\n\n\nThe skew1 value here (mean - median divided by the standard deviation) backs up our graphical assessment, that the data are slightly right skewed.\nRounded to two decimal places, the standard deviation of the serum zinc data turns out to be 16, and so the standard error of the mean, shown as se in the psych::describe output, is 16 divided by the square root of the sample size, n = 462. This standard error is about to become quite important to us in building statistical inferences about the mean of the entire population of teenage males based on this sample.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#defining-a-confidence-interval",
    "href": "19-ci_for_mean.html#defining-a-confidence-interval",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.7 Defining a Confidence Interval",
    "text": "19.7 Defining a Confidence Interval\nA confidence interval for a population or process mean uses data from a sample (and perhaps some additional information) to identify a range of potential values for the population mean, which, if certain assumptions hold, can be assumed to provide a reasonable estimate for the true population mean. A confidence interval consists of:\n\nAn interval estimate describing the population parameter of interest (here the population mean), and\nA probability statement, expressed in terms of a confidence level.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#estimating-the-population-mean-from-the-serum-zinc-data",
    "href": "19-ci_for_mean.html#estimating-the-population-mean-from-the-serum-zinc-data",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.8 Estimating the Population Mean from the Serum Zinc data",
    "text": "19.8 Estimating the Population Mean from the Serum Zinc data\nAs an example, suppose that we are willing to assume that the mean serum zinc level across the entire population of teenage males, \\(\\mu\\), follows a Normal distribution (and so, summarizing it with a mean is a rational thing to do.) Suppose that we are also willing to assume that the 462 teenage males contained in the serzinc tibble are a random sample from that complete population. While we know the mean of the sample of 462 boys, we don’t know \\(\\mu\\), the mean across all teenage males. So we need to estimate it.\nEarlier we estimated that a 90% confidence interval for the mean serum zinc level (\\(\\mu\\)) across the entire population of teenage males was (86.71, 89.16) micrograms per deciliter. How should we interpret this result?\n\nSome people think this means that there is a 90% chance that the true mean of the population, \\(\\mu\\), falls between 86.71 and 89.16 micrograms per deciliter. That’s not correct.\nThe population mean is a constant parameter of the population of interest. That constant is not a random variable, and does not change. So the actual probability of the population mean falling inside that range is either 0 or 1.\nOur confidence is in our process.\n\nIt’s in the sampling method (random sampling) used to generate the data, and in the assumption that the population follows a Normal distribution.\nIt’s captured in our accounting for one particular type of error (called sampling error) in developing our interval estimate, while assuming all other potential sources of error are negligible.\n\n\n\nSo, what’s closer to the truth is:\n\nIf we used this same method to sample data from the true population of teenage males, and built 100 such 90% confidence intervals, then about 90 of them would contain the true population mean.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#confidence-vs.-significance-level",
    "href": "19-ci_for_mean.html#confidence-vs.-significance-level",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.9 Confidence vs. Significance Level",
    "text": "19.9 Confidence vs. Significance Level\nWe’ve estimated a 90% confidence interval for the population mean serum zinc level among teenage boys using the serzinc data.\n\nWe call 100(1-\\(\\alpha\\))%, here, 90%, or 0.90, the confidence level, and\n\n\\(\\alpha\\) = 10%, or 0.10 is called the significance level.\n\nIf we had instead built a series of 100 different 95% confidence intervals, then about 95 of them would contain the true value of \\(\\mu\\).\nLet’s look more closely at the issue of estimating a population mean based on a sample of observations. We will need three critical pieces - the sample, the confidence level, and the margin of error, which is based on the standard error of a sample mean, when we are estimating a population mean.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#the-standard-error-of-a-sample-mean",
    "href": "19-ci_for_mean.html#the-standard-error-of-a-sample-mean",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.10 The Standard Error of a Sample Mean",
    "text": "19.10 The Standard Error of a Sample Mean\nThe standard error, generally, is the name we give to the standard deviation associated with any particular parameter estimate.\n\nIf we are using a sample mean based on a sample of size \\(n\\) to estimate a population mean, the standard error of that sample mean is the standard deviation of the measurements in the population, divided by the square root of the sample size.\nWe often estimate this particular standard error with \\(s\\) (the sample standard deviation) divided by the square root of the sample size.\n\nOther statistics have different standard errors.\n\n\n\\(\\sqrt{p (1-p) / n}\\) is the standard error of the sample proportion \\(p\\) estimated using a sample of size \\(n\\).\n\n\\(\\sqrt{\\frac{1-r^2}{n-2}}\\) is the standard error of the sample Pearson correlation \\(r\\) estimated using \\(n\\) pairs of observations.\n\n\\(\\sqrt{\\frac{SD_1^2}{n_1} + \\frac{SD_2^2}{n_2}}\\) is the standard error of the difference between two means \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\), estimated using samples of sizes \\(n_1\\) and \\(n_2\\) with sample standard devistions \\(SD_1\\) and \\(SD_2\\), respectively.\n\n\n\nIn developing a confidence interval for a population mean, we may be willing to assume that the data in our sample are drawn from a Normally distributed population. If so, the most common and useful means of building a confidence interval makes use of the t distribution (sometimes called Student’s t) and the notion of a standard error.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#the-t-distribution-and-cis-for-a-mean",
    "href": "19-ci_for_mean.html#the-t-distribution-and-cis-for-a-mean",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.11 The t distribution and CIs for a Mean",
    "text": "19.11 The t distribution and CIs for a Mean\nIn practical settings, we will use the t distribution to estimate a confidence interval from a population mean whenever we:\n\nare willing to assume that the sample is drawn at random from a population or process with a Normal distribution,\nare using our sample to estimate both the mean and standard deviation, and\nhave a small sample size.\n\n\n19.11.1 The Formula\nThe two-sided \\(100(1-\\alpha)\\)% confidence interval (based on a \\(t\\) test) is:\n\\[\\bar{x} \\pm t_{\\alpha/2, n-1}(s / \\sqrt{n})\\]\nwhere \\(t_{\\alpha/2, n-1}\\) is the value that cuts off the top \\(\\alpha/2\\) percent of the \\(t\\) distribution, with \\(n - 1\\) degrees of freedom.\nWe obtain the relevant cutoff value in R by substituting in values for alphaover2 and n-1 into the following line of R code:\nqt(alphaover2, df = n-1, lower.tail=FALSE)\n\n19.11.2 Student’s t distribution\nStudent’s t distribution looks a lot like a Normal distribution, when the sample size is large. Unlike the normal distribution, which is specified by two parameters, the mean and the standard deviation, the t distribution is specified by one parameter, the degrees of freedom.\n\nt distributions with large numbers of degrees of freedom are more or less indistinguishable from the standard Normal distribution.\nt distributions with smaller degrees of freedom (say, with df &lt; 30, in particular) are still symmetric, but are more outlier-prone than a Normal distribution\n\n\np1 &lt;- ggplot(data.frame(x = c(-3, 3)), aes(x)) + \n  stat_function(fun = dt, args = list(df = 1)) + \n  stat_function(fun = dnorm, col = \"red\") +\n  labs(title = \"t with 1 df\", y = \"Density\", x = \"\")\n\np2 &lt;- ggplot(data.frame(x = c(-3, 3)), aes(x)) + \n  stat_function(fun = dt, args = list(df = 3)) + \n  stat_function(fun = dnorm, col = \"red\") +\n  labs(title = \"t with 3 df\", y = \"Density\", x = \"\")\n\np3 &lt;- ggplot(data.frame(x = c(-3, 3)), aes(x)) + \n  stat_function(fun = dt, args = list(df = 5)) + \n  stat_function(fun = dnorm, col = \"red\") +\n  labs(title = \"t with 5 df\", y = \"Density\", x = \"\")\n\np4 &lt;- ggplot(data.frame(x = c(-3, 3)), aes(x)) + \n  stat_function(fun = dt, args = list(df = 10)) + \n  stat_function(fun = dnorm, col = \"red\") +\n  labs(title = \"t with 10 df\", y = \"Density\", x = \"\")\n\np5 &lt;- ggplot(data.frame(x = c(-3, 3)), aes(x)) + \n  stat_function(fun = dt, args = list(df = 20)) + \n  stat_function(fun = dnorm, col = \"red\") +\n  labs(title = \"t with 20 df\", y = \"Density\", x = \"\")\n\np6 &lt;- ggplot(data.frame(x = c(-3, 3)), aes(x)) + \n  stat_function(fun = dt, args = list(df = 30)) + \n  stat_function(fun = dnorm, col = \"red\") +\n  labs(title = \"t with 30 df\", y = \"Density\", x = \"\")\n\n(p1 + p2 + p3) / (p4 + p5 + p6) + \n    plot_annotation(\n        title = \"Various t distributions and the Standard Normal\",\n        caption = \"In each plot, the Standard Normal distribution is in red\")",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#building-the-ci-in-r",
    "href": "19-ci_for_mean.html#building-the-ci-in-r",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.12 Building the CI in R",
    "text": "19.12 Building the CI in R\nSuppose we wish to build a 90% confidence interval for the true mean serum zinc level across the entire population of teenage males. The confidence level will be 90%, or 0.90, and so the \\(\\alpha\\) value, which is 1 - confidence = 0.10.\nSo what we know going in is that:\n\nWe want \\(\\alpha\\) = 0.10, because we’re creating a 90% confidence interval.\nThe sample size n = 462 serum zinc measurements.\nThe sample mean of those measurements, \\(\\bar{x}\\) = 87.937 micrograms per deciliter.\nThe sample standard deviation of those measurements, s = 16.005 micrograms per deciliter.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#using-an-intercept-only-regression-model",
    "href": "19-ci_for_mean.html#using-an-intercept-only-regression-model",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.13 Using an intercept-only regression model",
    "text": "19.13 Using an intercept-only regression model\nin the context of fitting an intercept-only linear regression model. An intercept-only model is fitted by putting the number 1 on the right hand side of our linear model. The resulting model simply fits the overall mean of the data as a prediction for all subjects.\n\nmodel_zinc &lt;- lm(zinc ~ 1, data = serzinc)\n\n\nsummary(model_zinc)\n\n\nCall:\nlm(formula = zinc ~ 1, data = serzinc)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.937 -11.937  -1.937  10.063  65.063 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  87.9372     0.7446   118.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16 on 461 degrees of freedom\n\nconfint(model_zinc, level = 0.90)\n\n              5 %     95 %\n(Intercept) 86.71 89.16446\n\n\nGenerally, though, I’ll use the tidy() function in the broom package to obtain the key information from a model like this:\n\ntidy(model_zinc, conf.int = TRUE, conf = 0.90) |&gt;\n    kbl(digits = 2) |&gt; kable_styling()\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n(Intercept)\n87.94\n0.74\n118.1\n0\n86.71\n89.16\n\n\n\n\nAs an alternative, we could also use the t.test function, which can build (in this case) a two-sided confidence interval for the zinc levels like this:\n\ntt &lt;- t.test(serzinc$zinc, \n             conf.level = 0.90, \n             alternative = \"two.sided\")\n\ntt\n\n\n    One Sample t-test\n\ndata:  serzinc$zinc\nt = 118.1, df = 461, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 86.71000 89.16446\nsample estimates:\nmean of x \n 87.93723 \n\n\nand the tidy() function from the broom package works here, too.\n\ntidy(tt, conf.int = TRUE, conf = 0.90) |&gt; \n  kbl(digits = 2) |&gt; kable_styling()\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n87.94\n118.1\n0\n461\n86.71\n89.16\nOne Sample t-test\ntwo.sided\n\n\n\n\nAnd again, our 90% confidence interval for the true population mean serum zinc level, based on our sample of 462 patients, is (86.71, 89.16) micrograms per deciliter2.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#interpreting-the-result",
    "href": "19-ci_for_mean.html#interpreting-the-result",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.14 Interpreting the Result",
    "text": "19.14 Interpreting the Result\nAn appropriate interpretation of the 90% two-sided confidence interval above follows:\n\n(86.71, 89.16) micrograms per deciliter is a 90% two-sided confidence interval for the population mean serum zinc level among teenage males.\nOur point estimate for the true population mean serum zinc level is 87.94. The values in the interval (86.71, 89.16) represent a reasonable range of estimates for the true population mean serum zinc level, and we are 90% confident that this method of creating a confidence interval will produce a result containing the true population mean serum zinc level.\nWere we to draw 100 samples of size 462 from the population described by this sample, and use each such sample to produce a confidence interval in this manner, approximately 90 of those confidence intervals would cover the true population mean serum zinc level.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#what-if-we-want-a-95-or-99-confidence-interval-instead",
    "href": "19-ci_for_mean.html#what-if-we-want-a-95-or-99-confidence-interval-instead",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.15 What if we want a 95% or 99% confidence interval instead?",
    "text": "19.15 What if we want a 95% or 99% confidence interval instead?\nWe can obtain them using tidy and the same modeling approach.\n\ntidy(model_zinc, conf.int = TRUE, conf.level = 0.95) |&gt; \n  kbl(digits = 2) |&gt; kable_styling()\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n(Intercept)\n87.94\n0.74\n118.1\n0\n86.47\n89.4\n\n\n\n\n\ntidy(model_zinc, conf.int = TRUE, conf.level = 0.99) |&gt; \n  kbl(digits = 2) |&gt; kable_styling()\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n(Intercept)\n87.94\n0.74\n118.1\n0\n86.01\n89.86",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#using-the-broom-package-with-the-t-test",
    "href": "19-ci_for_mean.html#using-the-broom-package-with-the-t-test",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.16 Using the broom package with the t test",
    "text": "19.16 Using the broom package with the t test\nThe broom package takes the messy output of built-in functions in R, such as lm, t.test or wilcox.test, and turns them into tidy data frames. A detailed description of the package and three of its key functions is found at https://github.com/tidyverse/broom.\nFor example, we can use the tidy function within broom to create a single-row tibble of the key results from a t test.\n\ntt &lt;- t.test(serzinc$zinc, conf.level = 0.95, alternative = \"two.sided\")\ntidy(tt) |&gt; kbl(digits = 2) |&gt; kable_styling()\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n87.94\n118.1\n0\n461\n86.47\n89.4\nOne Sample t-test\ntwo.sided\n\n\n\n\nWe can thus pull the endpoints of a 95% confidence interval directly from this output. broom also has a glance function, which returns the same information as tidy in the case of a t-test.\n\n19.16.1 Effect of Changing the Confidence Level\nBelow, we see two-sided confidence intervals for various levels of \\(\\alpha\\).\n\n\n\n\n\n\n\n\nConfidence Level\n\\(\\alpha\\)\nTwo-Sided Interval Estimate for Zinc Level Population Mean, \\(\\mu\\)\n\nPoint Estimate of \\(\\mu\\)\n\n\n\n\n80% or 0.80\n0.20\n(87, 88.9)\n87.9\n\n\n90% or 0.90\n0.10\n(86.7, 89.2)\n87.9\n\n\n95% or 0.95\n0.05\n(86.5, 89.4)\n87.9\n\n\n99% or 0.99\n0.01\n(86, 89.9)\n87.9\n\n\n\nWhat happens to the width of the confidence interval in this table as the confidence level changes?",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#one-sided-vs.-two-sided-confidence-intervals",
    "href": "19-ci_for_mean.html#one-sided-vs.-two-sided-confidence-intervals",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.17 One-sided vs. Two-sided Confidence Intervals",
    "text": "19.17 One-sided vs. Two-sided Confidence Intervals\nOccasionally, we want to estimate either an upper limit for the population mean \\(\\mu\\), or a lower limit for \\(\\mu\\), but not both.\n\nt.test(serzinc$zinc, conf.level = 0.90, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  serzinc$zinc\nt = 118.1, df = 461, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is greater than 0\n90 percent confidence interval:\n 86.98161      Inf\nsample estimates:\nmean of x \n 87.93723 \n\nt.test(serzinc$zinc, conf.level = 0.90, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  serzinc$zinc\nt = 118.1, df = 461, p-value = 1\nalternative hypothesis: true mean is less than 0\n90 percent confidence interval:\n     -Inf 88.89285\nsample estimates:\nmean of x \n 87.93723 \n\n\nNote the relationship between the two-sided 80% confidence interval, and the one-sided 90% confidence intervals.\n\n\n\n\n\n\n\n\nConfidence\n\\(\\alpha\\)\nType of Interval\nInterval Estimate for Zinc Level Population Mean, \\(\\mu\\)\n\n\n\n\n80% (.80)\n0.20\nTwo-Sided\n(86.98, 88.89)\n\n\n90% (.90)\n0.10\nOne-Sided (Less Than)\n\n\\(\\mu\\) &lt; 88.89.\n\n\n90% (.90)\n0.10\nOne-Sided (Greater Than)\n\n\\(\\mu\\) &gt; 86.98.\n\n\n\nWhy does this happen? The 80% two-sided interval is placed so as to cut off the top 10% of the distribution with its upper bound, and the bottom 10% of the distribution with its lower bound. The 90% “less than” one-sided interval is placed so as to have its lower bound cut off the top 10% of the distribution.\nThe same issue appears when we consider two-sided 90% and one-sided 95% confidence intervals.\n\n\n\n\n\n\n\n\nConfidence\n\\(\\alpha\\)\nType of Interval\nInterval Estimate for Zinc Level Population Mean, \\(\\mu\\)\n\n\n\n\n90% (.90)\n0.10\nTwo-Sided\n(86.71, 89.16)\n\n\n95% (.95)\n0.05\nOne-Sided (Less Than)\n\n\\(\\mu\\) &lt; 89.16.\n\n\n95% (.95)\n0.05\nOne-Sided (Greater Than)\n\n\\(\\mu\\) &gt; 86.71.\n\n\n\nAgain, the 90% two-sided interval cuts off the top 5% and bottom 5% of the distribution with its bounds. The 95% “less than” one-sided interval also has its lower bound cut off the top 5% of the distribution.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#bootstrap-confidence-intervals",
    "href": "19-ci_for_mean.html#bootstrap-confidence-intervals",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.18 Bootstrap Confidence Intervals",
    "text": "19.18 Bootstrap Confidence Intervals\nThe bootstrap (and in particular, what’s known as bootstrap resampling) is a really good idea that you should know a little bit about.\nIf we want to know how accurately a sample mean estimates the population mean, we would ideally like to take a very, very large sample, because if we did so, we could conclude with something that would eventually approach mathematical certainty that the sample mean would be very close to the population mean.\nBut we can rarely draw enormous samples. So what can we do?",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#resampling-is-a-big-idea",
    "href": "19-ci_for_mean.html#resampling-is-a-big-idea",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.19 Resampling is A Big Idea",
    "text": "19.19 Resampling is A Big Idea\nOne way to find out how precise our estimates are is to run them on multiple samples of the same size. This resampling approach was codified originally by Brad Efron in 1979.\nOversimplifying a lot, the idea is that if we sample (with replacement) from our current sample, we can draw a new sample of the same size as our original.\n\nAnd if we repeat this many times, we can generate as many samples of, say, 462 zinc levels, as we like.\nThen we take these thousands of samples and calculate (for instance) the sample mean for each, and plot a histogram of those means.\nIf we then cut off the top and bottom 5% of these sample means, we obtain a reasonable 90% confidence interval for the population mean.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#when-is-a-bootstrap-confidence-interval-reasonable",
    "href": "19-ci_for_mean.html#when-is-a-bootstrap-confidence-interval-reasonable",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.20 When is a Bootstrap Confidence Interval Reasonable?",
    "text": "19.20 When is a Bootstrap Confidence Interval Reasonable?\nA bootstrapped interval estimate for the population mean, \\(\\mu\\), will be reasonable as long as we’re willing to believe that:\n\nthe original sample was a random sample (or at least a completely representative sample) from a population,\nand that the samples are independent of each other,\n\neven if the population of interest doesn’t follow a Normal, or even a symmetric distribution.\nA downside of the bootstrap is that you and I will get (somewhat) different answers if we resample from the same data without setting the same random seed.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#bootstrap-confidence-interval-for-the-mean-process",
    "href": "19-ci_for_mean.html#bootstrap-confidence-interval-for-the-mean-process",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.21 Bootstrap confidence interval for the mean: Process",
    "text": "19.21 Bootstrap confidence interval for the mean: Process\nTo avoid the Normality assumption, and take advantage of modern computing power, we use R to obtain a bootstrap confidence interval for the population mean based on a sample.\nWhat the computer does:\n\nRe-sample the data with replacement, until it obtains a new sample that is equal in size to the original data set.\nCalculates the statistic of interest (here, a sample mean.)\nRepeat the steps above many times (the default is 1,000 using our approach) to obtain a set of 1,000 sample means.\nSort those 1,000 sample means in order, and estimate the 95% confidence interval for the population mean based on the middle 95% of the 1,000 bootstrap samples.\nSend us a result, containing the sample mean, and a 95% confidence interval for the population mean",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#using-r-to-estimate-a-bootstrap-ci",
    "href": "19-ci_for_mean.html#using-r-to-estimate-a-bootstrap-ci",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.22 Using R to estimate a bootstrap CI",
    "text": "19.22 Using R to estimate a bootstrap CI\nThe command that we use to obtain a Confidence Interval for \\(\\mu\\) using the basic nonparametric bootstrap and without assuming a Normally distributed population, is smean.cl.boot, a part of the Hmisc package in R.\n\nset.seed(431)\nsmean.cl.boot(serzinc$zinc, B = 1000, conf.int = 0.90) ## from Hmisc\n\n    Mean    Lower    Upper \n87.93723 86.76775 89.20617 \n\n\n\nRemember that the t-based 90% CI for \\(\\mu\\) was (86.71, 89.16), according to the following output…\n\n\ntidy(lm(zinc ~ 1, data = serzinc), conf.int = TRUE, conf.level = 0.90)\n\n# A tibble: 1 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     87.9     0.745      118.       0     86.7      89.2",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#comparing-bootstrap-and-t-based-confidence-intervals",
    "href": "19-ci_for_mean.html#comparing-bootstrap-and-t-based-confidence-intervals",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.23 Comparing Bootstrap and T-Based Confidence Intervals",
    "text": "19.23 Comparing Bootstrap and T-Based Confidence Intervals\n\nThe smean.cl.boot function (unlike most R functions) deletes missing data automatically, as does the smean.cl.normal function, which can also be used to produce the t-based confidence interval.\n\n\nset.seed(431)\nsmean.cl.boot(serzinc$zinc, B = 1000, conf.int = 0.90)\n\n    Mean    Lower    Upper \n87.93723 86.76775 89.20617 \n\nsmean.cl.normal(serzinc$zinc, conf.int = 0.90)\n\n    Mean    Lower    Upper \n87.93723 86.71000 89.16446 \n\n\nBootstrap resampling confidence intervals do not follow the general confidence interval strategy using a point estimate plus or minus a margin for error.\n\nA bootstrap interval is often asymmetric, and while it will generally have the point estimate (the sample mean) near its center, for highly skewed data, this will not necessarily be the case.\nWe will usually use either 1,000 (the default) or 10,000 bootstrap replications for building confidence intervals – practically, it makes little difference.\n\n\n19.23.1 Bootstrap Resampling: Advantages and Caveats\nThe bootstrap may seem like the solution to all estimation problems. In theory, we could use the same approach to find a confidence interval for any other parameter – it’s not perfect, but it is very useful. Bootstrap procedures exist for virtually any statistical comparison - the t-test analog is just one of many possibilities, and bootstrap methods are rapidly gaining on more traditional approaches in the literature thanks mostly to faster computers.\nThe great advantage of the bootstrap is its relative simplicity, but don’t forget that many of the original assumptions of the t-based confidence interval still hold.\n\nUsing a bootstrap does eliminate the need to worry about the Normality assumption in small sample size settings, but it still requires independent and identically distributed samples from the population of interest.\n\nThe bootstrap produces clean and robust inferences (such as confidence intervals) in many tricky situations. It is still possible that the results can be both:\n\n\ninaccurate (i.e. they can include the true value of the unknown population mean less often than the stated confidence probability) and\n\nimprecise (i.e., they can include more extraneous values of the unknown population mean than is desirable).",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#using-the-bootstrap-to-develop-other-cis",
    "href": "19-ci_for_mean.html#using-the-bootstrap-to-develop-other-cis",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.24 Using the Bootstrap to develop other CIs",
    "text": "19.24 Using the Bootstrap to develop other CIs\n\n19.24.1 Changing the Confidence Level\nWhat if we wanted to change the confidence level?\n\nset.seed(431654)\nsmean.cl.boot(serzinc$zinc, B = 1000, conf.int = 0.95)\n\n    Mean    Lower    Upper \n87.93723 86.51066 89.42002 \n\nset.seed(431321)\nsmean.cl.boot(serzinc$zinc, B = 1000, conf.int = 0.99)\n\n    Mean    Lower    Upper \n87.93723 86.20657 89.68619",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#one-tailed-bootstrap-confidence-intervals",
    "href": "19-ci_for_mean.html#one-tailed-bootstrap-confidence-intervals",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.25 One-Tailed Bootstrap Confidence Intervals",
    "text": "19.25 One-Tailed Bootstrap Confidence Intervals\nIf you want to estimate a one tailed confidence interval for the population mean using the bootstrap, then the procedure is as follows:\n\nDetermine \\(\\alpha\\), the significance level you want to use in your one-sided confidence interval. Remember that \\(\\alpha\\) is 1 minus the confidence level. Let’s assume we want a 90% one-sided interval, so \\(\\alpha\\) = 0.10.\nDouble \\(\\alpha\\) to determine the significance level we will use in the next step to fit a two-sided confidence interval.\nFit a two-sided confidence interval with confidence level \\(100(1 - 2*\\alpha)\\). Let the bounds of this interval be (a, b).\nThe one-sided (greater than) confidence interval will have a as its lower bound.\nThe one-sided (less than) confidence interval will have b as its upper bound.\n\nSuppose that we want to find a 95% one-sided upper bound for the population mean serum zinc level among teenage males, \\(\\mu\\), using the bootstrap.\nSince we want a 95% confidence interval, we have \\(\\alpha\\) = 0.05. We double that to get \\(\\alpha\\) = 0.10, which implies we need to instead fit a two-sided 90% confidence interval.\n\nset.seed(43101)\nsmean.cl.boot(serzinc$zinc, B = 1000, conf.int = 0.90)  ## from Hmisc package\n\n    Mean    Lower    Upper \n87.93723 86.70509 89.11266 \n\n\nThe upper bound of this two-sided 90% CI will also be the upper bound for a 95% one-sided CI.\n\n19.25.1 Bootstrap CI for the Population Median\nIf we are willing to do a small amount of programming work in R, we can obtain bootstrap confidence intervals for other population parameters besides the mean. One statistic of common interest is the median. How do we find a confidence interval for the population median using a bootstrap approach? The easiest way I know of makes use of the boot package, as follows.\nIn step 1, we specify a new function to capture the medians from our sample.\n\nf.median &lt;- function(y, id) \n{    median ( y[id])  }\n\nIn step 2, we call the boot.ci function from the boot package.\n\nset.seed(431787)\nboot.ci(boot(serzinc$zinc, f.median, 1000), conf=0.90, type=\"basic\")\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot(serzinc$zinc, f.median, 1000), conf = 0.9, \n    type = \"basic\")\n\nIntervals : \nLevel      Basic         \n90%   (84, 87 )  \nCalculations and Intervals on Original Scale\n\n\nThis yields a 90% confidence interval for the population median serum zinc level. Recall that the sample median for the serum zinc levels in our sample of 462 teenage males was 86 micrograms per deciliter.\n\nmosaic::favstats(~ zinc, data = serzinc) |&gt;\n  kbl(digits = 2) |&gt; kable_styling()\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n50\n76\n86\n98\n153\n87.94\n16\n462\n0\n\n\n\n\nActually, the boot.ci function can provide up to five different types of confidence interval (see the help file) if we change to type=\"all\", and some of those other versions have attractive properties. However, we’ll stick with the basic approach in 431.\n\n19.25.2 Bootstrap CI for the IQR\nIf for some reason, we want to find a 95% confidence interval for the population value of the inter-quartile range via the bootstrap, we can do it.\n\nIQR(serzinc$zinc)\n\n[1] 22\n\nf.IQR &lt;- function(y, id) \n{    IQR (y[id]) }\n\nset.seed(431207)\nboot.ci(boot(serzinc$zinc, f.IQR, 1000), \n        conf=0.95, type=\"basic\")\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot(serzinc$zinc, f.IQR, 1000), conf = 0.95, \n    type = \"basic\")\n\nIntervals : \nLevel      Basic         \n95%   (20.00, 24.24 )  \nCalculations and Intervals on Original Scale",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#wilcoxon-signed-rank-procedure-for-cis",
    "href": "19-ci_for_mean.html#wilcoxon-signed-rank-procedure-for-cis",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.26 Wilcoxon Signed Rank Procedure for CIs",
    "text": "19.26 Wilcoxon Signed Rank Procedure for CIs\nIt turns out to be difficult, without the bootstrap, to estimate an appropriate confidence interval for the median of a population, which might be an appealing thing to do, particularly if the sample data are clearly not Normally distributed, so that a median seems like a better summary of the center of the data. Bootstrap procedures are available to perform the task.\nThe Wilcoxon signed rank approach can be used as an alternative to t-based procedures to build interval estimates for the population pseudo-median when the population cannot be assumed to follow a Normal distribution.\nAs it turns out, if you’re willing to assume the population is symmetric (but not necessarily Normally distributed) then the pseudo-median is actually equal to the population median.\n\n19.26.1 What is a Pseudo-Median?\nThe pseudo-median of a particular distribution G is the median of the distribution of (u + v)/2, where both u and v have the same distribution (G).\n\nIf the distribution G is symmetric, then the pseudomedian is equal to the median.\nIf the distribution is skewed, then the pseudomedian is not the same as the median.\nFor any sample, the pseudomedian is defined as the median of all of the midpoints of pairs of observations in the sample.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#wilcoxon-signed-rank-based-ci-in-r",
    "href": "19-ci_for_mean.html#wilcoxon-signed-rank-based-ci-in-r",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.27 Wilcoxon Signed Rank-based CI in R",
    "text": "19.27 Wilcoxon Signed Rank-based CI in R\n\nwilcox.test(serzinc$zinc, conf.int = TRUE, conf.level = 0.95)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  serzinc$zinc\nV = 106953, p-value &lt; 2.2e-16\nalternative hypothesis: true location is not equal to 0\n95 percent confidence interval:\n 85.99997 88.50002\nsample estimates:\n(pseudo)median \n      87.49996 \n\n\n\n19.27.1 Interpreting the Wilcoxon CI for the Population Median\nIf we’re willing to believe the zinc levels come from a population with a symmetric distribution, the 95% Confidence Interval for the population median would be (86, 88.5)\nFor a non-symmetric population, this only applies to the pseudo-median.\nNote that the pseudo-median (87.5) is actually closer here to the sample mean (87.9) than it is to the sample median (86).\n\n19.27.2 Using the broom package with the Wilcoxon test\nWe can also use the tidy function within broom to create a single-row tibble of the key results from a Wilcoxon test, so long as we run wilcox.test specifying that we want a confidence interval.\n\nwt &lt;- wilcox.test(serzinc$zinc, conf.int = TRUE, conf.level = 0.95)\ntidy(wt) |&gt; kbl(digits = 2) |&gt; kable_styling()\n\n\n\nestimate\nstatistic\np.value\nconf.low\nconf.high\nmethod\nalternative\n\n\n87.5\n106953\n0\n86\n88.5\nWilcoxon signed rank test with continuity correction\ntwo.sided",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#general-advice",
    "href": "19-ci_for_mean.html#general-advice",
    "title": "19  Confidence Intervals for a Mean",
    "section": "\n19.28 General Advice",
    "text": "19.28 General Advice\nWe have described several approaches to estimating a confidence interval for the center of a distribution of quantitative data.\n\nThe most commonly used approach uses the t distribution to estimate a confidence interval for a population/process mean. This requires some extra assumptions, most particularly that the underlying distribution of the population values is at least approximately Normally distributed. This is identical to the result we get from an intercept-only linear regression model.\nA more modern and very general approach uses the idea of the bootstrap to estimate a confidence for a population/process parameter, which could be a mean, median or other summary statistic. The bootstrap, and the underlying notion of resampling is an important idea that lets us avoid some of the assumptions (in particular Normality) that are required by other methods. Bootstrap confidence intervals involve random sampling, so that the actual values obtained will differ a bit across replications.\nFinally, the Wilcoxon signed-rank method is one of a number of inferential tools which transform the data to their ranks before estimating a confidence interval. This avoids some assumptions, but yields inferences about a less-familiar parameter - the pseudo-median.\n\nMost of the time, the bootstrap provides a reasonably adequate confidence interval estimate of the population value of a parameter (mean or median, most commonly) from a distribution when our data consists of a single sample of quantitative information.\n\n\n\n\nPagano, Marcello, and Kimberlee Gauvreau. 2000. Principles of Biostatistics. Second. Duxbury Press.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "19-ci_for_mean.html#footnotes",
    "href": "19-ci_for_mean.html#footnotes",
    "title": "19  Confidence Intervals for a Mean",
    "section": "",
    "text": "Reference values for those over the age of 10 years at http://www.mayomedicallaboratories.com/test-catalog/Clinical+and+Interpretive/8620 , visited 2019-09-17.↩︎\nSince the measured zinc levels appear as integers, we should probably be rounding even further in our confidence interval, down to perhaps one decimal place.↩︎",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confidence Intervals for a Mean</span>"
    ]
  },
  {
    "objectID": "20-ibuprofen_sepsis.html",
    "href": "20-ibuprofen_sepsis.html",
    "title": "20  Ibuprofen in Sepsis",
    "section": "",
    "text": "20.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nsource(\"data/Love-boost.R\")\nlibrary(broom)\nlibrary(Epi)\nlibrary(ggridges)\nlibrary(Hmisc)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\nIn addition to the Love-boost.R script, we will also use the favstats function from the mosaic package.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ibuprofen in Sepsis</span>"
    ]
  },
  {
    "objectID": "20-ibuprofen_sepsis.html#the-trial",
    "href": "20-ibuprofen_sepsis.html#the-trial",
    "title": "20  Ibuprofen in Sepsis",
    "section": "\n20.2 The Trial",
    "text": "20.2 The Trial\nOur next study is a randomized controlled trial comparing ibuprofen vs. placebo in patients with sepsis, which uses an independent samples design to compare two samples of quantitative data. We will be working with a sample from the Ibuprofen in Sepsis study, which is also studied in Dupont (2002). Quoting the abstract from Bernard et al. (1997):\n\nIbuprofen has been shown to have effects on sepsis in humans, but because of their small samples (fewer than 30 patients), previous studies have been inadequate to assess effects on mortality. We sought to determine whether ibuprofen can alter rates of organ failure and mortality in patients with the sepsis syndrome, how the drug affects the increased metabolic demand in sepsis (e.g., fever, tachypnea, tachycardia, hypoxemia, and lactic acidosis), and what potential adverse effects the drug has in the sepsis syndrome.\n\nIn this study, patients meeting specific criteria (including elevated temperature) for a diagnosis of sepsis were recruited if they fulfilled an additional set of study criteria in the intensive care unit at one of seven participating centers.\nThe full trial involved 455 patients, of which our sample includes 300. 150 of our patients were randomly assigned to the Ibuprofen group and 150 to the Placebo group1. I picked the sepsis sample we will work with excluding patients with missing values for our outcome of interest, and then selected a random sample of 150 Ibuprofen and 150 Placebo patients from the rest of the group, and converted the temperatures and changes from Fahrenheit to Celsius. The data are gathered in the sepsis data file.\n\nsepsis &lt;- read_csv(\"data/sepsis.csv\", show_col_types = FALSE)\n\nFor the moment, we focus on two variables:\n\n\ntreat, which specifies the treatment group (intravenous Ibuprofen or intravenous Placebo), which was assigned via randomization to each patient, and\n\ntemp_drop, the outcome of interest, measured as the change from baseline to 2 hours later in degrees Celsius. Positive values indicate improvement, that is, a drop in temperature over the 2 hours following the baseline measurement.\n\nThe sepsis.csv file also contains each subject’s\n\n\nid, which is just a code\n\nrace (three levels: White, AfricanA or Other)\n\napache = baseline APACHE II score, a severity of disease score ranging from 0 to 71 with higher scores indicating more severe disease and a higher mortality risk\n\ntemp_0 = baseline temperature, degrees Celsius.\n\nbut for the moment, we won’t worry about those.\n\nsepsis &lt;- sepsis |&gt;\n    mutate(treat = factor(treat),\n           race = factor(race))\n\nsummary(sepsis)\n\n      id                  treat           race         apache    \n Length:300         Ibuprofen:150   AfricanA: 80   Min.   : 0.0  \n Class :character   Placebo  :150   Other   : 23   1st Qu.:10.0  \n Mode  :character                   White   :197   Median :14.0  \n                                                   Mean   :15.4  \n                                                   3rd Qu.:20.0  \n                                                   Max.   :35.0  \n     temp_0        temp_drop      \n Min.   :33.10   Min.   :-2.7000  \n 1st Qu.:37.48   1st Qu.:-0.1000  \n Median :38.20   Median : 0.3000  \n Mean   :38.00   Mean   : 0.3083  \n 3rd Qu.:38.70   3rd Qu.: 0.7000  \n Max.   :41.70   Max.   : 3.1000",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ibuprofen in Sepsis</span>"
    ]
  },
  {
    "objectID": "20-ibuprofen_sepsis.html#comparing-two-groups",
    "href": "20-ibuprofen_sepsis.html#comparing-two-groups",
    "title": "20  Ibuprofen in Sepsis",
    "section": "\n20.3 Comparing Two Groups",
    "text": "20.3 Comparing Two Groups\nIn making a choice between two alternatives, questions such as the following become paramount.\n\nIs there a status quo?\nIs there a standard approach?\nWhat are the costs of incorrect decisions?\nAre such costs balanced?\n\nThe process of comparing the means/medians/proportions/rates of the populations represented by two independently obtained samples can be challenging, and such an approach is not always the best choice. Often, specially designed experiments can be more informative at lower cost (i.e. smaller sample size). As one might expect, using these more sophisticated procedures introduces trade-offs, but the costs are typically small relative to the gain in information.\nWhen faced with such a comparison of two alternatives, a test based on paired data is often much better than a test based on two distinct, independent samples. Why? If we have done our experiment properly, the pairing lets us eliminate background variation that otherwise hides meaningful differences.\n\n20.3.1 Model-Based Comparisons and ANOVA/Regression\nComparisons based on independent samples of quantitative variables are also frequently accomplished through other equivalent methods, including the analysis of variance approach and dummy variable regression, both of which produce identical confidence intervals to the pooled variance t test for the same comparison.\nWe will also discuss some of the main ideas in developing, designing and analyzing statistical experiments, specifically in terms of making comparisons. The ideas we will present in this section allow for the comparison of more than two populations in terms of their population means. The statistical techniques employed analyze the sample variance in order to test and estimate the population means and for this reason the method is called the analysis of variance (ANOVA), and we will discuss this approach alone, and within the context of a linear regression model using dummy or indicator variables.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ibuprofen in Sepsis</span>"
    ]
  },
  {
    "objectID": "20-ibuprofen_sepsis.html#key-questions-for-comparing-with-independent-samples",
    "href": "20-ibuprofen_sepsis.html#key-questions-for-comparing-with-independent-samples",
    "title": "20  Ibuprofen in Sepsis",
    "section": "\n20.4 Key Questions for Comparing with Independent Samples",
    "text": "20.4 Key Questions for Comparing with Independent Samples\n\n20.4.1 What is the population under study?\n\nAll patients in the intensive care unit with sepsis who meet the inclusion and exclusion criteria of the study, at the entire population of health centers like the ones included in the trial.\n\n20.4.2 What is the sample? Is it representative of the population?\n\nThe sample consists of 300 patients. It is a convenient sample from the population under study.\nThis is a randomized clinical trial. 150 of the patients were assigned to Ibuprofen, and the rest to Placebo. It is this treatment assignment that is randomized, not the selection of the sample as a whole.\nIn expectation, randomization of individuals to treatments, as in this study, should be expected to eliminate treatment selection bias.\n\n20.4.3 Who are the subjects / individuals within the sample?\n\n150 patients who received Ibuprofen and a completely different set of 150 patients who received Placebo.\nThere is no match or link between the patients. They are best thought of as independent samples.\n\n20.4.4 What data are available on each individual?\n\nThe key variables are the treatment indicator (Ibuprofen or Placebo) and the outcome (drop in temperature in the 2 hours following administration of the randomly assigned treatment.)\n\n20.4.5 RCT Caveats\nThe placebo-controlled, double-blind randomized clinical trial, especially if pre-registered, is often considered the best feasible study for assessing the effectiveness of a treatment. While that’s not always true, it is a very solid design. The primary caveat is that the patients who are included in such trials are rarely excellent representations of the population of potentially affected patients as a whole.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ibuprofen in Sepsis</span>"
    ]
  },
  {
    "objectID": "20-ibuprofen_sepsis.html#exploratory-data-analysis",
    "href": "20-ibuprofen_sepsis.html#exploratory-data-analysis",
    "title": "20  Ibuprofen in Sepsis",
    "section": "\n20.5 Exploratory Data Analysis",
    "text": "20.5 Exploratory Data Analysis\nConsider the following boxplot with violin of the temp_drop data within each treat group.\n\nggplot(sepsis, aes(x = treat, y = temp_drop, fill = treat)) +\n    geom_violin() +\n    geom_boxplot(width = 0.3, fill = \"white\") +\n    scale_fill_viridis_d() +\n    guides(fill = \"none\") + \n    labs(title = \"Boxplot of Temperature Drop in Sepsis Patients\",\n         x = \"\", y = \"Drop in Temperature (degrees C)\") + \n    coord_flip() \n\n\n\n\n\n\n\nNext, we’ll consider faceted histograms of the data.\n\nggplot(sepsis, aes(x = temp_drop, fill = treat, color = treat)) +\n    geom_histogram(bins = 20) +\n    scale_fill_viridis_d() +\n    scale_color_viridis_d(direction = -1) +\n    guides(fill = \"none\", color = \"none\") + \n    labs(title = \"Histograms of Temperature Drop in Sepsis Patients\",\n         x = \"Drop in Temperature (degrees Celsius\") +\n    facet_wrap(~ treat)\n\n\n\n\n\n\n\nHere’s a pair of Normal Q-Q plots. It’s not hard to use a Normal model to approximate the Ibuprofen data, but such a model is probably not a good choice for the Placebo results.\n\nggplot(sepsis, aes(sample = temp_drop)) +\n    geom_qq() + geom_qq_line(col = \"red\") +\n    facet_wrap(~ treat) + \n    labs(y = \"Temperature Drop Values (in degrees C)\")\n\n\n\n\n\n\n\nWe’ll could perhaps also look at a ridgeline plot, with some extra functions and theming from the ggridges package.\n\nggplot(sepsis, aes(x = temp_drop, y = treat, fill = treat)) +\n    geom_density_ridges(scale = 0.9) +\n    guides(fill = \"none\") + \n    labs(title = \"Temperature Drop in Sepsis Patients\",\n         x = \"Drop in Temperature (degrees Celsius)\", y = \"\") +\n    theme_ridges()\n\nPicking joint bandwidth of 0.182\n\n\n\n\n\n\n\n\nThe center of the ibuprofen distribution is shifted a bit towards the more positive (greater improvement) direction, it seems, than is the distribution for the placebo patients. This conclusion matches what we see in some key numerical summaries, within the treatment groups.\n\nmosaic::favstats(temp_drop ~ treat, data = sepsis)\n\n      treat  min     Q1 median  Q3 max      mean        sd   n missing\n1 Ibuprofen -1.5  0.000    0.5 0.9 3.1 0.4640000 0.6877919 150       0\n2   Placebo -2.7 -0.175    0.1 0.4 1.9 0.1526667 0.5709637 150       0",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ibuprofen in Sepsis</span>"
    ]
  },
  {
    "objectID": "20-ibuprofen_sepsis.html#estimating-the-difference-in-population-means",
    "href": "20-ibuprofen_sepsis.html#estimating-the-difference-in-population-means",
    "title": "20  Ibuprofen in Sepsis",
    "section": "\n20.6 Estimating the Difference in Population Means",
    "text": "20.6 Estimating the Difference in Population Means\nNext, we will build a point estimate and 90% confidence interval for the difference between the mean temp_drop if treated with Ibuprofen and the mean temp_drop if treated with Placebo. We’ll use a regression model with a single predictor (the treat group) to do this.\n\nmodel_sep &lt;- lm(temp_drop ~ treat == \"Ibuprofen\", data = sepsis)\n\ntidy(model_sep, conf.int = TRUE, conf.level = 0.90) |&gt;\n    kbl(digits = 3) |&gt; kable_styling()\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n0.153\n0.052\n2.958\n0.003\n0.068\n0.238\n\n\ntreat == \"Ibuprofen\"TRUE\n0.311\n0.073\n4.266\n0.000\n0.191\n0.432\n\n\n\n\n\nThe point estimate for the “Ibuprofen - Placebo” difference in population means is 0.311 degrees C, and the 90% confidence interval is (0.191, 0.432) degrees C.\nWe could also have run the model like this:\n\nmodel_sep2 &lt;- lm(temp_drop ~ treat, data = sepsis)\n\ntidy(model_sep2, conf.int = TRUE, conf.level = 0.90) |&gt;\n    kbl(digits = 3) |&gt; kable_styling()\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n0.464\n0.052\n8.991\n0\n0.379\n0.549\n\n\ntreatPlacebo\n-0.311\n0.073\n-4.266\n0\n-0.432\n-0.191\n\n\n\n\n\nand would therefore conclude that the Placebo - Ibuprofen difference was estimated as -0.311, with 90% confidence interval (-0.432, -0.191), which is of course equivalent to our previous estimate.\nFundamentally, this regression model approach is identical to a two-sample t test, assuming equal population variances, also called a pooled t test. This is just one possible way for us to estimate the difference between population means, as it turns out.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ibuprofen in Sepsis</span>"
    ]
  },
  {
    "objectID": "20-ibuprofen_sepsis.html#t-based-ci-for-population-mean1---mean2-difference",
    "href": "20-ibuprofen_sepsis.html#t-based-ci-for-population-mean1---mean2-difference",
    "title": "20  Ibuprofen in Sepsis",
    "section": "\n20.7 t-based CI for population mean1 - mean2 difference",
    "text": "20.7 t-based CI for population mean1 - mean2 difference\n\n20.7.1 The Pooled t procedure\nThe most commonly used t-procedure for building a confidence interval assumes not only that each of the two populations being compared follows a Normal distribution, but also that they have the same population variance. This is the pooled t-test, and it is what people usually mean when they describe a two-sample t test.\n\nt.test(temp_drop ~ treat,\n       data = sepsis,\n       conf.level = 0.90,\n       alt = \"two.sided\",\n       var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  temp_drop by treat\nt = 4.2656, df = 298, p-value = 2.68e-05\nalternative hypothesis: true difference in means between group Ibuprofen and group Placebo is not equal to 0\n90 percent confidence interval:\n 0.1909066 0.4317600\nsample estimates:\nmean in group Ibuprofen   mean in group Placebo \n              0.4640000               0.1526667 \n\n\nOr, we can use tidy on this object:\n\ntt1 &lt;- t.test(temp_drop ~ treat,\n              data = sepsis, \n              conf.level = 0.90,\n              alt = \"two.sided\",\n              var.equal = TRUE)\ntidy(tt1) |&gt; kbl(digits = 2) |&gt; kable_styling()\n\n\n\nestimate\nestimate1\nestimate2\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.31\n0.46\n0.15\n4.27\n0\n298\n0.19\n0.43\nTwo Sample t-test\ntwo.sided\n\n\n\n\n\n20.7.2 Using linear regression to obtain a pooled t confidence interval\nAs we’ve seen, and will demonstrate again below, a linear regression model, using the same outcome and predictor (group) as the pooled t procedure, produces the same confidence interval, again, under the assumption that the two populations we are comparing follow a Normal distribution with the same (population) variance.\n\nmodel1 &lt;- lm(temp_drop ~ treat, data = sepsis)\n\ntidy(model1, conf.int = TRUE, conf.level = 0.90)\n\n# A tibble: 2 × 7\n  term         estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     0.464    0.0516      8.99 2.91e-17    0.379     0.549\n2 treatPlacebo   -0.311    0.0730     -4.27 2.68e- 5   -0.432    -0.191\n\n\nWe see that our point estimate from the linear regression model is that the difference in temp_drop is -0.3113333, where Ibuprofen subjects have higher temp_drop values than do Placebo subjects, and that the 90% confidence interval for this difference ranges from -0.43176 to -0.1909066.\nWe can obtain a t-based confidence interval for each of the parameter estimates in a linear model directly using tidy from the broom package. Linear models usually summarize only the estimate and standard error. Remember that a reasonable approximation in large samples to a 95% confidence interval for a regression estimate (slope or intercept) can be obtained from estimate plus or minus two times the standard error.\n\ntidy(model1, conf.int = TRUE, conf.level = 0.95) |&gt; \n  kbl(digits = 3) |&gt; kable_styling()\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n0.464\n0.052\n8.991\n0\n0.362\n0.566\n\n\ntreatPlacebo\n-0.311\n0.073\n-4.266\n0\n-0.455\n-0.168\n\n\n\n\n\nSo, in the case of the treatPlacebo estimate, we can obtain an approximate 95% confidence interval with (-0.457, -0.165). Compare this to the 95% confidence interval available from the model directly, shown in the tidied output above, or with the confint command below, and you’ll see only a small difference.\nNote that we can also use summary and confint to build our estimates.\n\nsummary(model1)\n\n\nCall:\nlm(formula = temp_drop ~ treat, data = sepsis)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.85267 -0.36400 -0.05267  0.34733  2.63600 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.46400    0.05161   8.991  &lt; 2e-16 ***\ntreatPlacebo -0.31133    0.07299  -4.266 2.68e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6321 on 298 degrees of freedom\nMultiple R-squared:  0.05755,   Adjusted R-squared:  0.05438 \nF-statistic:  18.2 on 1 and 298 DF,  p-value: 2.68e-05\n\nconfint(model1, level = 0.95)\n\n                  2.5 %     97.5 %\n(Intercept)   0.3624351  0.5655649\ntreatPlacebo -0.4549679 -0.1676988\n\n\n\n20.7.3 The Welch t procedure\nThe default confidence interval based on the t test for independent samples in R uses something called the Welch test, in which the two populations being compared are not assumed to have the same variance. Each population is assumed to follow a Normal distribution.\n\nt.test(temp_drop ~ treat, data = sepsis, conf.level = 0.90, alt = \"two.sided\")\n\n\n    Welch Two Sample t-test\n\ndata:  temp_drop by treat\nt = 4.2656, df = 288.24, p-value = 2.706e-05\nalternative hypothesis: true difference in means between group Ibuprofen and group Placebo is not equal to 0\n90 percent confidence interval:\n 0.1908939 0.4317728\nsample estimates:\nmean in group Ibuprofen   mean in group Placebo \n              0.4640000               0.1526667 \n\n\nTidying works in this situation, too.\n\ntt0 &lt;- t.test(temp_drop ~ treat, \n              data = sepsis, conf.level = 0.90, alt = \"two.sided\")\n\ntidy(tt0) |&gt; kbl(digits = 2) |&gt; kable_styling()\n\n\n\nestimate\nestimate1\nestimate2\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.31\n0.46\n0.15\n4.27\n0\n288.24\n0.19\n0.43\nWelch Two Sample t-test\ntwo.sided\n\n\n\n\nWhen there is a balanced design, that is, when the same number of observations appear in each of the two samples, then the Welch t test and the Pooled t test produce the same confidence interval. Differences appear if the sample sizes in the two groups being compared are different.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ibuprofen in Sepsis</span>"
    ]
  },
  {
    "objectID": "20-ibuprofen_sepsis.html#wilcoxon-mann-whitney-rank-sum-ci",
    "href": "20-ibuprofen_sepsis.html#wilcoxon-mann-whitney-rank-sum-ci",
    "title": "20  Ibuprofen in Sepsis",
    "section": "\n20.8 Wilcoxon-Mann-Whitney “Rank Sum” CI",
    "text": "20.8 Wilcoxon-Mann-Whitney “Rank Sum” CI\nAs in the one-sample case, a rank-based alternative attributed to Wilcoxon (and sometimes to Mann and Whitney) provides a two-sample comparison of the pseudomedians in the two treat groups in terms of temp_drop. This is called a rank sum test, rather than the Wilcoxon signed rank test that is used for inference about a single sample. Here’s the resulting 90% confidence interval for the difference in pseudomedians.\n\nwt &lt;- wilcox.test(temp_drop ~ treat, data = sepsis,\n                  conf.int = TRUE, conf.level = 0.90,\n                  alt = \"two.sided\") \n\nwt\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  temp_drop by treat\nW = 14614, p-value = 7.281e-06\nalternative hypothesis: true location shift is not equal to 0\n90 percent confidence interval:\n 0.1999699 0.4000330\nsample estimates:\ndifference in location \n             0.3000368 \n\ntidy(wt) |&gt; kbl(digits = 2) |&gt; kable_styling()\n\n\n\nestimate\nstatistic\np.value\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.3\n14614.5\n0\n0.2\n0.4\nWilcoxon rank sum test with continuity correction\ntwo.sided",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ibuprofen in Sepsis</span>"
    ]
  },
  {
    "objectID": "20-ibuprofen_sepsis.html#bootstrapping-a-more-robust-approach",
    "href": "20-ibuprofen_sepsis.html#bootstrapping-a-more-robust-approach",
    "title": "20  Ibuprofen in Sepsis",
    "section": "\n20.9 Bootstrapping: A More Robust Approach",
    "text": "20.9 Bootstrapping: A More Robust Approach\nWithin a script called Love-boost.R, I have provided the following R code to create a function called bootdif.\n\nbootdif &lt;-\n  function(y, g, conf.level=0.95, B.reps = 2000) {\n    lowq = (1 - conf.level)/2\n    g &lt;- as.factor(g)\n    a &lt;- attr(Hmisc::smean.cl.boot(y[g==levels(g)[1]], B=B.reps, reps=TRUE),'reps')\n    b &lt;- attr(Hmisc::smean.cl.boot(y[g==levels(g)[2]], B=B.reps, reps=TRUE),'reps')\n    meandif &lt;- diff(tapply(y, g, mean, na.rm=TRUE))\n    a.b &lt;- quantile(b-a, c(lowq,1-lowq))\n    res &lt;- c(meandif, a.b)\n    names(res) &lt;- c('Mean Difference',lowq, 1-lowq)\n    res\n  }\n\nRunning this code will place a new function called bootdif in your environment, which will help us calculate an appropriate confidence interval using a bootstrap procedure. The bootdif function contained in the Love-boost.R script is a slightly edited version of the function at http://biostat.mc.vanderbilt.edu/wiki/Main/BootstrapMeansSoftware.\n\n20.9.1 Bootstrap CI for the Sepsis study\nNote that this approach uses a comma to separate the outcome variable (here, temp_drop) from the variable identifying the exposure groups (here, treat).\n\nset.seed(431212)\n\nbootdif(sepsis$temp_drop, sepsis$treat, conf.level = 0.90)\n\nMean Difference            0.05            0.95 \n     -0.3113333      -0.4313667      -0.1833000 \n\n\nThis approach calculates a 90% confidence interval for the difference in means between the two treatment groups. Note that the sign is in the opposite direction from what we’ve seen in our previous work. We can tell from the mean difference (and the summarized means from the data in each group) that this approach is finding a confidence interval using a bootstrap procedure for the Placebo - Ibuprofen difference, specifically (-0.431, -0.183).\n\nmosaic::favstats(temp_drop ~ treat, data = sepsis)\n\n      treat  min     Q1 median  Q3 max      mean        sd   n missing\n1 Ibuprofen -1.5  0.000    0.5 0.9 3.1 0.4640000 0.6877919 150       0\n2   Placebo -2.7 -0.175    0.1 0.4 1.9 0.1526667 0.5709637 150       0\n\n\nTo find a confidence interval using this bootstrap approach for the Ibuprofen - Placebo difference, we just need to switch the signs, and conclude that the 90% bootstrap confidence interval for that difference would be (0.183, 0.431).",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ibuprofen in Sepsis</span>"
    ]
  },
  {
    "objectID": "20-ibuprofen_sepsis.html#summary-specifying-a-two-sample-study-design",
    "href": "20-ibuprofen_sepsis.html#summary-specifying-a-two-sample-study-design",
    "title": "20  Ibuprofen in Sepsis",
    "section": "\n20.10 Summary: Specifying A Two-Sample Study Design",
    "text": "20.10 Summary: Specifying A Two-Sample Study Design\nThese questions will help specify the details of the study design involved in any comparison of two populations on a quantitative outcome, perhaps with means.\n\nWhat is the outcome under study?\nWhat are the (in this case, two) treatment/exposure groups?\nWere the data collected using matched / paired samples or independent samples?\nAre the data a random sample from the population(s) of interest? Or is there at least a reasonable argument for generalizing from the sample to the population(s)?\nWhat is the significance level (or, the confidence level) we require here?\nAre we doing one-sided or two-sided testing/confidence interval generation?\nIf we have paired samples, did pairing help reduce nuisance variation?\nIf we have paired samples, what does the distribution of sample paired differences tell us about which inferential procedure to use?\nIf we have independent samples, what does the distribution of each individual sample tell us about which inferential procedure to use?",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ibuprofen in Sepsis</span>"
    ]
  },
  {
    "objectID": "20-ibuprofen_sepsis.html#results-for-the-sepsis-study",
    "href": "20-ibuprofen_sepsis.html#results-for-the-sepsis-study",
    "title": "20  Ibuprofen in Sepsis",
    "section": "\n20.11 Results for the sepsis study",
    "text": "20.11 Results for the sepsis study\n\nThe outcome is temp_drop, the change in body temperature (in \\(^{\\circ}\\)C) from baseline to 2 hours later, so that positive numbers indicate drops in temperature (a good outcome.)\nThe groups are Ibuprofen and Placebo as contained in the treat variable in the sepsis tibble.\nThe data were collected using independent samples. The Ibuprofen subjects are not matched or linked to individual Placebo subjects - they are separate groups.\nThe subjects of the study aren’t drawn from a random sample of the population of interest, but they are randomly assigned to their respective treatments (Ibuprofen and Placebo) which will provide the reasoned basis for our inferences.\nWe’ll use a 10% significance level (or 90% confidence level) in this setting, as we did in our previous work on these data.\nWe’ll use a two-sided testing and confidence interval approach.\n\nQuestions 7 and 8 don’t apply, because these are independent samples of data, rather than paired samples.\nTo address question 9, we’ll need to look at the data in each sample, as we did previously to allow us to assess the Normality of the distributions of (separately) the temp_drop results in the Ibuprofen and Placebo groups. We’ll repeat those below.\n\nggplot(sepsis, aes(x = treat, y = temp_drop, fill = treat)) +\n    geom_violin() +\n    geom_boxplot(width = 0.3, fill = \"white\") +\n    stat_summary(fun = \"mean\", geom = \"point\", \n                 shape = 23, size = 3, fill = \"red\") +\n    scale_fill_viridis_d(alpha = 0.3) +\n    guides(fill = \"none\") + \n    labs(title = \"Boxplot of Temperature Drop in Sepsis Patients\",\n         x = \"\", y = \"Drop in Temperature (degrees C)\") + \n    coord_flip() \n\n\n\n\n\n\n\n\nggplot(sepsis, aes(sample = temp_drop)) +\n    geom_qq() + geom_qq_line(col = \"red\") +\n    facet_wrap(~ treat) + \n    labs(y = \"Temperature Drop Values (in degrees C)\")\n\n\n\n\n\n\n\nFrom these plots we conclude that the data in the Ibuprofen sample follow a reasonably Normal distribution, but this isn’t quite as true for the Placebo sample. It’s hard to know whether the apparent Placebo group outliers will affect whether the Normal distribution assumption is reasonable, so we can see if the confidence intervals change much when we don’t assume Normality (for instance, comparing the bootstrap to the t-based approaches), as a way of understanding whether a Normal model has a large impact on our conclusions.\n\n20.11.1 Sepsis Estimation Results\nHere’s a set of confidence interval estimates (we’ll use 90% confidence here) using the methods discussed in this Chapter.\n\nmosaic::favstats(temp_drop ~ treat, data = sepsis)\n\n      treat  min     Q1 median  Q3 max      mean        sd   n missing\n1 Ibuprofen -1.5  0.000    0.5 0.9 3.1 0.4640000 0.6877919 150       0\n2   Placebo -2.7 -0.175    0.1 0.4 1.9 0.1526667 0.5709637 150       0\n\n\n\ns_pooled_t_test &lt;- t.test(temp_drop ~ treat, data = sepsis, conf.level = 0.90, \n                          alt = \"two.sided\", var.equal = TRUE)\n\ntidy(s_pooled_t_test) |&gt; \n    select(conf.low, conf.high)\n\n# A tibble: 1 × 2\n  conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;\n1    0.191     0.432\n\n\n\ns_welch_t_test &lt;- t.test(temp_drop ~ treat, data = sepsis, conf.level = 0.90,\n                         alt = \"two.sided\", var.equal = FALSE)\n\ntidy(s_welch_t_test) |&gt; \n    select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.311    0.191     0.432\n\n\n\ns_wilcoxon_test &lt;- wilcox.test(temp_drop ~ treat, data = sepsis, \n                               conf.int = TRUE, conf.level = 0.90,\n                               alt = \"two.sided\") \n\ntidy(s_wilcoxon_test) |&gt; \n    select(estimate, conf.low, conf.high)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.300    0.200     0.400\n\n\n\nset.seed(431212)\ns_bootstrap &lt;- bootdif(sepsis$temp_drop, sepsis$treat, conf.level = 0.90)\n\ns_bootstrap\n\nMean Difference            0.05            0.95 \n     -0.3113333      -0.4313667      -0.1833000 \n\n\n\n\nProcedure\nCompares…\nPoint Estimate\n90% CI\n\n\n\nPooled t\nMeans\n0.311\n(0.191, 0.432)\n\n\nWelch t\nMeans\n0.311\n(0.191, 0.432)\n\n\nBootstrap\nMeans\n0.311\n(0.183, 0.431)\n\n\nWilcoxon rank sum\nPseudo-Medians\n0.3\n(0.2, 0.4)\n\n\n\nWhat conclusions can we draw in this setting?",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ibuprofen in Sepsis</span>"
    ]
  },
  {
    "objectID": "20-ibuprofen_sepsis.html#categorizing-the-outcome-and-comparing-rates",
    "href": "20-ibuprofen_sepsis.html#categorizing-the-outcome-and-comparing-rates",
    "title": "20  Ibuprofen in Sepsis",
    "section": "\n20.12 Categorizing the Outcome and Comparing Rates",
    "text": "20.12 Categorizing the Outcome and Comparing Rates\nSuppose we were interested in comparing the percentage of patients in each arm of the trial (Ibuprofen vs. Placebo) that showed an improvement in their temperature (temp_drop &gt; 0). To build the cross-tabulation of interest, we could create a new variable, called dropped which indicates whether the subject’s temperature dropped, and then use tabyl.\n\nsepsis &lt;- sepsis |&gt;\n    mutate(dropped = ifelse(temp_drop &gt; 0, \"Drop\", \"No Drop\"))\n\nsepsis |&gt; tabyl(treat, dropped)\n\n     treat Drop No Drop\n Ibuprofen  107      43\n   Placebo   80      70\n\n\nOur primary interest is in comparing the percentage of Ibuprofen patients whose temperature dropped to the percentage of Placebo patients whose temperature dropped.\n\nsepsis |&gt; tabyl(treat, dropped) |&gt; \n    adorn_totals() |&gt;\n    adorn_percentages(denom = \"row\") |&gt;\n    adorn_pct_formatting(digits = 1) |&gt;\n    adorn_ns(position = \"front\")\n\n     treat        Drop     No Drop\n Ibuprofen 107 (71.3%)  43 (28.7%)\n   Placebo  80 (53.3%)  70 (46.7%)\n     Total 187 (62.3%) 113 (37.7%)",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ibuprofen in Sepsis</span>"
    ]
  },
  {
    "objectID": "20-ibuprofen_sepsis.html#estimating-the-difference-in-proportions",
    "href": "20-ibuprofen_sepsis.html#estimating-the-difference-in-proportions",
    "title": "20  Ibuprofen in Sepsis",
    "section": "\n20.13 Estimating the Difference in Proportions",
    "text": "20.13 Estimating the Difference in Proportions\nIn our sample, 71.3% of the Ibuprofen subjects, and 53.3% of the Placebo subjects, experienced a drop in temperature. So our point estimate of the difference in percentages would be 18.0 percentage points, but we will usually set this instead in terms of proportions, so that the difference is 0.180.\nNow, we’ll find a confidence interval for that difference, which we can do in several ways, including the twoby2 function in the Epi package.\n\ntable(sepsis$treat, sepsis$dropped) |&gt; twoby2(alpha = 0.10)\n\n2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : Drop \nComparing : Ibuprofen vs. Placebo \n\n          Drop No Drop    P(Drop) 90% conf. interval\nIbuprofen  107      43     0.7133    0.6490   0.7701\nPlacebo     80      70     0.5333    0.4661   0.5993\n\n                                   90% conf. interval\n             Relative Risk: 1.3375    1.1492   1.5567\n         Sample Odds Ratio: 2.1773    1.4583   3.2509\nConditional MLE Odds Ratio: 2.1716    1.4177   3.3437\n    Probability difference: 0.1800    0.0881   0.2677\n\n             Exact P-value: 0.0019 \n        Asymptotic P-value: 0.0014 \n------------------------------------------------------\n\n\nWhile there is a lot of additional output here, we’ll look for now just at the Probability difference row, where we see the point estimate (0.180) and the 90% confidence interval estimate for the difference in proportions (0.088, 0.268) comparing Ibuprofen vs. Placebo for the outcome of Dropping in Temperature.\nMore on estimation of the difference in population proportions will be found later.\n\n\n\n\nBernard, Gordon R., Arthur P. Wheeler, James A. Russell, Roland Schein, Warren R. Summer, Kenneth P. Steinberg, William J. Fulkerson, et al. 1997. “The Effects of Ibuprofen on the Physiology and Survival of Patients with Sepsis.” New England Journal of Medicine 336: 912–18. http://www.nejm.org/doi/full/10.1056/NEJM199703273361303#t=article.\n\n\nDupont, William D. 2002. Statistical Modeling for Biomedical Researchers. New York: Cambridge University Press.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ibuprofen in Sepsis</span>"
    ]
  },
  {
    "objectID": "20-ibuprofen_sepsis.html#footnotes",
    "href": "20-ibuprofen_sepsis.html#footnotes",
    "title": "20  Ibuprofen in Sepsis",
    "section": "",
    "text": "This was a double-blind study, where neither the patients nor their care providers know, during the execution of the trial, what intervention group was assigned to each patient.↩︎",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ibuprofen in Sepsis</span>"
    ]
  },
  {
    "objectID": "21-pairedsamples.html",
    "href": "21-pairedsamples.html",
    "title": "\n21  Comparing Means with Paired Samples\n",
    "section": "",
    "text": "21.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nsource(\"data/Love-boost.R\")\nlibrary(kableExtra)\nlibrary(broom)\nlibrary(ggridges)\nlibrary(Hmisc)\nlibrary(patchwork)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\nIn addition to the Love-boost.R script, we will also use the favstats function from the mosaic package.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Comparing Means with Paired Samples</span>"
    ]
  },
  {
    "objectID": "21-pairedsamples.html#lead-in-the-blood-of-children",
    "href": "21-pairedsamples.html#lead-in-the-blood-of-children",
    "title": "\n21  Comparing Means with Paired Samples\n",
    "section": "\n21.2 Lead in the Blood of Children",
    "text": "21.2 Lead in the Blood of Children\n\nOne of the best ways to eliminate a source of variation and the errors of interpretation associated with it is through the use of matched pairs. Each subject in one group is matched as closely as possible by a subject in the other group. If a 45-year-old African-American male with hypertension is given a [treatment designed to lower their blood pressure], then we give a second, similarly built 45-year old African-American male with hypertension a placebo.\n\n\n\nGood (2005), section 5.2.4",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Comparing Means with Paired Samples</span>"
    ]
  },
  {
    "objectID": "21-pairedsamples.html#the-lead-in-the-blood-of-children-study",
    "href": "21-pairedsamples.html#the-lead-in-the-blood-of-children-study",
    "title": "\n21  Comparing Means with Paired Samples\n",
    "section": "\n21.3 The Lead in the Blood of Children Study",
    "text": "21.3 The Lead in the Blood of Children Study\nMorton et al. (1982) studied the absorption of lead into the blood of children. This was a matched-sample study, where the exposed group of interest contained 33 children of parents who worked in a battery manufacturing factory (where lead was used) in the state of Oklahoma. Specifically, each child with a lead-exposed parent was matched to another child of the same age, exposure to traffic, and living in the same neighborhood whose parents did not work in lead-related industries. So the complete study had 66 children, arranged in 33 matched pairs. The outcome of interest, gathered from a sample of whole blood from each of the children, was lead content, measured in mg/dl.\nOne motivation for doing this study is captured in the Abstract from Morton et al. (1982).\n\nIt has been repeatedly reported that children of employees in a lead-related industry are at increased risk of lead absorption because of the high levels of lead found in the household dust of these workers.\n\nThe data are available in several places, including Table 5 of Pruzek and Helmreich (2009), in the BloodLead data set within the PairedData package in R, but we also make them available in the bloodlead.csv file. A table of the first few pairs of observations (blood lead levels for one child exposed to lead and the matched control) is shown below.\n\nbloodlead &lt;- read_csv(\"data/bloodlead.csv\", show_col_types = FALSE)\n\nbloodlead\n\n# A tibble: 33 × 3\n   pair  exposed control\n   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 P01        38      16\n 2 P02        23      18\n 3 P03        41      18\n 4 P04        18      24\n 5 P05        37      19\n 6 P06        36      11\n 7 P07        23      10\n 8 P08        62      15\n 9 P09        31      16\n10 P10        34      18\n# ℹ 23 more rows\n\n\n\nIn each pair, one child was exposed (to having a parent working in the factory) and the other was not.\nOtherwise, though, each child was very similar to its matched partner.\nThe data under exposed and control are the blood lead content, in mg/dl.\n\nOur primary goal will be to estimate the difference in lead content between the exposed and control children, and then use that sample estimate to make inferences about the difference in lead content between the population of all children like those in the exposed group and the population of all children like those in the control group.\n\n21.3.1 Our Key Questions for a Paired Samples Comparison\n\nWhat is the population under study?\n\n\nAll pairs of children living in Oklahoma near the factory in question, in which one had a parent working in a factory that exposed them to lead, and the other did not.\n\n\nWhat is the sample? Is it representative of the population?\n\n\nThe sample consists of 33 pairs of one exposed and one control child.\nThis is a case-control study, where the children were carefully enrolled to meet the design criteria. Absent any other information, we’re likely to assume that there is no serious bias associated with these pairs, and that assuming they represent the population effectively (and perhaps the broader population of kids whose parents work in lead-based industries more generally) may well be at least as reasonable as assuming they don’t.\n\n\nWho are the subjects / individuals within the sample?\n\n\nEach of our 33 pairs of children includes one exposed child and one unexposed (control) child.\n\n\nWhat data are available on each individual?\n\n\nThe blood lead content, as measured in mg/dl of whole blood.\n\n21.3.2 Lead Study Caveats\nNote that the children were not randomly selected from general populations of kids whose parents did and did not work in lead-based industries.\n\nTo make inferences to those populations, we must make strong assumptions to believe, for instance, that the sample of exposed children is as representative as a random sample of children with similar exposures across the world would be.\nThe researchers did have a detailed theory about how the exposed children might be at increased risk of lead absorption, and in fact as part of the study gathered additional information about whether a possible explanation might be related to the quality of hygiene of the parents (all of them were fathers, actually) who worked in the factory.\nThis is an observational study, so that the estimation of a causal effect between parental work in a lead-based industry and children’s blood lead content can be made, without substantial (and perhaps heroic) assumptions.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Comparing Means with Paired Samples</span>"
    ]
  },
  {
    "objectID": "21-pairedsamples.html#exploratory-data-analysis-for-paired-samples",
    "href": "21-pairedsamples.html#exploratory-data-analysis-for-paired-samples",
    "title": "\n21  Comparing Means with Paired Samples\n",
    "section": "\n21.4 Exploratory Data Analysis for Paired Samples",
    "text": "21.4 Exploratory Data Analysis for Paired Samples\nWe’ll begin by adjusting the data in two ways.\n\nWe’d like that first variable (pair) to be a factor rather than a character type in R, because we want to be able to summarize it more effectively. So we’ll make that change.\nAlso, we’d like to calculate the difference in lead content between the exposed and the control children in each pair, and we’ll save that within-pair difference in a variable called lead_diff. We’ll take lead_diff = exposed - control so that positive values indicate increased lead in the exposed child.\n\n\nbloodlead_original &lt;- bloodlead\n\nbloodlead &lt;- bloodlead_original |&gt;\n    mutate(pair = factor(pair),\n           lead_diff = exposed - control)\n\nbloodlead\n\n# A tibble: 33 × 4\n   pair  exposed control lead_diff\n   &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 P01        38      16        22\n 2 P02        23      18         5\n 3 P03        41      18        23\n 4 P04        18      24        -6\n 5 P05        37      19        18\n 6 P06        36      11        25\n 7 P07        23      10        13\n 8 P08        62      15        47\n 9 P09        31      16        15\n10 P10        34      18        16\n# ℹ 23 more rows\n\n\n\n21.4.1 The Paired Differences\nTo begin, we focus on lead_diff for our exploratory work, which is the exposed - control difference in lead content within each of the 33 pairs. So, we’ll have 33 observations, as compared to the 462 in the serum zinc data, but most of the same tools are still helpful.\n\np1 &lt;- ggplot(bloodlead, aes(sample = lead_diff)) +\n  geom_qq(col = \"steelblue\") + geom_qq_line(col = \"navy\") + \n  theme(aspect.ratio = 1) + \n  labs(title = \"Normal Q-Q plot\")\n\np2 &lt;- ggplot(bloodlead, aes(x = lead_diff)) +\n  geom_histogram(aes(y = stat(density)), \n                 binwidth = 5, fill = \"steelblue\", col = \"white\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(bloodlead$lead_diff), \n                            sd = sd(bloodlead$lead_diff)),\n                col = \"navy\", lwd = 1.5) +\n  labs(title = \"Histogram with Normal Density\")\n\np3 &lt;- ggplot(bloodlead, aes(x = lead_diff, y = \"\")) +\n  geom_boxplot(fill = \"steelblue\", outlier.color = \"steelblue\") + \n  stat_summary(fun = \"mean\", geom = \"point\", \n               shape = 23, size = 3, fill = \"white\") +\n  labs(title = \"Boxplot\", y = \"\")\n\np1 + (p2 / p3 + plot_layout(heights = c(4,1))) + \n    plot_annotation(title = \"Difference in Blood Lead Content (mg/dl) for 33 Pairs of Children\")\n\nWarning: `stat(density)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\nNote that in all of this work, I plotted the paired differences. One obvious way to tell if you have paired samples is that you can pair every single subject from one exposure group to a unique subject in the other exposure group. Everyone has to be paired, so the sample sizes will always be the same in the two groups.\nHere’s a summary of the paired differences.\n\nmosaic::favstats(~ lead_diff, data = bloodlead) |&gt;\n    kbl(digits = 2) |&gt; kable_styling()\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n-9\n4\n15\n25\n60\n15.97\n15.86\n33\n0\n\n\n\nbloodlead |&gt; summarise(skew1 = \n                            (mean(lead_diff) - median(lead_diff)) / \n                            sd(lead_diff))\n\n# A tibble: 1 × 1\n   skew1\n   &lt;dbl&gt;\n1 0.0611\n\n\n\n21.4.2 Impact of Matching - Scatterplot and Correlation\nHere, the data are paired by the study through matching on neighborhood, age and exposure to traffic. Each individual child’s outcome value is part of a pair with the outcome value for his/her matching partner. We can see this pairing in several ways, perhaps by drawing a scatterplot of the pairs.\n\nggplot(bloodlead, aes(x = control, y = exposed)) +\n    geom_point(size = 2) + \n    geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE) +\n    geom_text(x = 20, y = 65, col = \"blue\", \n             label = \n                 paste(\"Pearson r = \",\n                       round(cor(bloodlead$control, bloodlead$exposed),2))) +\n    labs(title = \"Paired Samples in Blood Lead study\",\n         x = \"Blood Lead Content (mg/dl) in Control Child\",\n         y = \"Blood Lead Content (mg/dl) in Exposed Child\")\n\n\n\n\n\n\n\nEach point here represents a pair of observations, one from a control child, and one from the matched exposed child. If there is a strong linear relationship (usually with a positive slope, thus positive correlation) between the paired outcomes, then the pairing will be more helpful in terms of improving statistical power of the estimates we build than if there is a weak relationship.\n\nThe stronger the Pearson correlation coefficient, the more helpful pairing will be.\nHere, a straight line model using the control child’s blood lead content accounts for about 3.2% of the variation in blood lead content in the exposed child.\nAs it turns out, pairing will have only a modest impact here on the inferences we draw in the study. We still will treat the data as paired, despite this.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Comparing Means with Paired Samples</span>"
    ]
  },
  {
    "objectID": "21-pairedsamples.html#looking-at-separate-samples-using-pivot_longer",
    "href": "21-pairedsamples.html#looking-at-separate-samples-using-pivot_longer",
    "title": "\n21  Comparing Means with Paired Samples\n",
    "section": "\n21.5 Looking at Separate Samples: Using pivot_longer\n",
    "text": "21.5 Looking at Separate Samples: Using pivot_longer\n\nFor the purpose of estimating the difference between the exposed and control children, the summaries of the paired differences are what we’ll need.\nIn some settings, however, we might also look at a boxplot, or violin plot, or ridgeline plot that showed the distributions of exposed and control children separately. But we will run into trouble because one variable (blood lead content) is spread across multiple columns (control and exposed.) The solution is to “pivot” the tibble from its current format to build a new, tidy tibble. Because the data aren’t tidied here, so that we have one row for each subject and one column for each variable, we have to do some work to get them in that form for our usual plotting strategy to work well.\n\n\npivot_longer() “lengthens” the data, increasing the number of rows and decreasing the number of columns.\n\npivot_wider() performs the inverse of that transformation, “widening” the data.\n\nIn our original bloodlead data, if we drop the lead_diff addition we made, we have wide data, with each row representing two different subjects.\n\nhead(bloodlead_original, 3)\n\n# A tibble: 3 × 3\n  pair  exposed control\n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 P01        38      16\n2 P02        23      18\n3 P03        41      18\n\n\nAnd what we want to accomplish is to have one row for each subject, instead of one row for each pair of subjects. So we want to make the data longer.\n\nbloodlead_longer &lt;- bloodlead_original |&gt;\n    pivot_longer(\n        cols = -c(pair),\n        names_to = \"status\",\n        values_to = \"lead_level\")\n\nbloodlead_longer\n\n# A tibble: 66 × 3\n   pair  status  lead_level\n   &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1 P01   exposed         38\n 2 P01   control         16\n 3 P02   exposed         23\n 4 P02   control         18\n 5 P03   exposed         41\n 6 P03   control         18\n 7 P04   exposed         18\n 8 P04   control         24\n 9 P05   exposed         37\n10 P05   control         19\n# ℹ 56 more rows\n\n\nFor more on this approach (in this case, we’re making the data “longer” and its opposite would be be making the data “wider”), visit the Tidy data chapter in Hadley Wickham and Grolemund (2023) and the tidyr repository on Github at https://github.com/tidyverse/tidyr.\nAnd now, we can plot as usual to compare the two samples.\nFirst, we’ll look at a boxplot, showing all of the data.\n\nggplot(bloodlead_longer, aes(x = status, y = lead_level)) +\n    geom_violin() +\n    geom_boxplot(aes(fill = status), width = 0.2) +\n    stat_summary(fun = \"mean\", geom = \"point\", \n                 shape = 23, size = 3, fill = \"blue\") +\n    scale_fill_viridis_d(alpha = 0.5) +\n    guides(fill = \"none\") + \n    coord_flip() +\n    labs(title = \"Boxplot of Lead Content in Exposed and Control kids\") \n\n\n\n\n\n\n\nWe’ll also look at a ridgeline plot, because Dr. Love likes them, even though they’re really more useful when we’re comparing more than two samples.\n\nggplot(bloodlead_longer, aes(x = lead_level, y = status, fill = status)) +\n    geom_density_ridges(scale = 0.9) +\n    guides(fill = \"none\") + \n    labs(title = \"Lead Content in Exposed and Control kids\") +\n    theme_ridges()\n\nPicking joint bandwidth of 4.01\n\n\n\n\n\n\n\n\nBoth the center and the spread of the distribution are substantially larger in the exposed group than in the controls. Of course, numerical summaries show these patterns, too.\n\nmosaic::favstats(lead_level ~ status, data = bloodlead_longer) |&gt;\n    kbl(digits = 2) |&gt; kable_styling()\n\n\n\nstatus\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\ncontrol\n7\n13\n16\n19\n25\n15.88\n4.54\n33\n0\n\n\nexposed\n10\n21\n34\n39\n73\n31.85\n14.41\n33\n0",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Comparing Means with Paired Samples</span>"
    ]
  },
  {
    "objectID": "21-pairedsamples.html#estimating-the-difference-in-means-with-paired-samples",
    "href": "21-pairedsamples.html#estimating-the-difference-in-means-with-paired-samples",
    "title": "\n21  Comparing Means with Paired Samples\n",
    "section": "\n21.6 Estimating the Difference in Means with Paired Samples",
    "text": "21.6 Estimating the Difference in Means with Paired Samples\nSuppose we want to estimate the difference in the mean blood level across the population of children represented by the sample taken in this study. To do so, we must take advantage of the matched samples design, and complete our estimation on the paired differences, treating them as if they were a single sample of data.\nOne way to accomplish this is simply to run the usual intercept-only linear regression model on the paired differences.\n\nmodel_lead &lt;- lm(lead_diff ~ 1, data = bloodlead)\n\ntidy(model_lead, conf.int = TRUE, conf.level = 0.90) |&gt;\n    kbl(digits = 2) |&gt; kable_styling()\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n(Intercept)\n15.97\n2.76\n5.78\n0\n11.29\n20.65\n\n\n\n\nOur point estimate for the difference (exposed - control) in lead levels is 15.97 mg/dl, and our 90% confidence interval is (11.29, 20.65) mg/dl.\n\n21.6.1 Paired Data in Longer Format?\nIf we had the data in “longer” format, as in bloodlead_longer, with the pairs identified by the pair variable, then we could obtained the same confidence interval using:\n\nmodel2_lead &lt;- lm(lead_level ~ status + factor(pair), data = bloodlead_longer)\n\ntidy(model2_lead, conf.int = TRUE, conf.level = 0.90) |&gt;\n    kbl(digits = 2) |&gt; kable_styling()\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n19.02\n8.05\n2.36\n0.02\n5.38\n32.65\n\n\nstatusexposed\n15.97\n2.76\n5.78\n0.00\n11.29\n20.65\n\n\nfactor(pair)P02\n-6.50\n11.22\n-0.58\n0.57\n-25.50\n12.50\n\n\nfactor(pair)P03\n2.50\n11.22\n0.22\n0.83\n-16.50\n21.50\n\n\nfactor(pair)P04\n-6.00\n11.22\n-0.53\n0.60\n-25.00\n13.00\n\n\nfactor(pair)P05\n1.00\n11.22\n0.09\n0.93\n-18.00\n20.00\n\n\nfactor(pair)P06\n-3.50\n11.22\n-0.31\n0.76\n-22.50\n15.50\n\n\nfactor(pair)P07\n-10.50\n11.22\n-0.94\n0.36\n-29.50\n8.50\n\n\nfactor(pair)P08\n11.50\n11.22\n1.03\n0.31\n-7.50\n30.50\n\n\nfactor(pair)P09\n-3.50\n11.22\n-0.31\n0.76\n-22.50\n15.50\n\n\nfactor(pair)P10\n-1.00\n11.22\n-0.09\n0.93\n-20.00\n18.00\n\n\nfactor(pair)P11\n-6.00\n11.22\n-0.53\n0.60\n-25.00\n13.00\n\n\nfactor(pair)P12\n-13.50\n11.22\n-1.20\n0.24\n-32.50\n5.50\n\n\nfactor(pair)P13\n-7.00\n11.22\n-0.62\n0.54\n-26.00\n12.00\n\n\nfactor(pair)P14\n-13.50\n11.22\n-1.20\n0.24\n-32.50\n5.50\n\n\nfactor(pair)P15\n-11.00\n11.22\n-0.98\n0.33\n-30.00\n8.00\n\n\nfactor(pair)P16\n-9.00\n11.22\n-0.80\n0.43\n-28.00\n10.00\n\n\nfactor(pair)P17\n-7.50\n11.22\n-0.67\n0.51\n-26.50\n11.50\n\n\nfactor(pair)P18\n-15.50\n11.22\n-1.38\n0.18\n-34.50\n3.50\n\n\nfactor(pair)P19\n0.00\n11.22\n0.00\n1.00\n-19.00\n19.00\n\n\nfactor(pair)P20\n-0.50\n11.22\n-0.04\n0.96\n-19.50\n18.50\n\n\nfactor(pair)P21\n-5.50\n11.22\n-0.49\n0.63\n-24.50\n13.50\n\n\nfactor(pair)P22\n0.00\n11.22\n0.00\n1.00\n-19.00\n19.00\n\n\nfactor(pair)P23\n1.00\n11.22\n0.09\n0.93\n-18.00\n20.00\n\n\nfactor(pair)P24\n6.00\n11.22\n0.53\n0.60\n-13.00\n25.00\n\n\nfactor(pair)P25\n4.50\n11.22\n0.40\n0.69\n-14.50\n23.50\n\n\nfactor(pair)P26\n-3.50\n11.22\n-0.31\n0.76\n-22.50\n15.50\n\n\nfactor(pair)P27\n0.00\n11.22\n0.00\n1.00\n-19.00\n19.00\n\n\nfactor(pair)P28\n3.50\n11.22\n0.31\n0.76\n-15.50\n22.50\n\n\nfactor(pair)P29\n2.50\n11.22\n0.22\n0.83\n-16.50\n21.50\n\n\nfactor(pair)P30\n-12.50\n11.22\n-1.11\n0.27\n-31.50\n6.50\n\n\nfactor(pair)P31\n16.00\n11.22\n1.43\n0.16\n-3.00\n35.00\n\n\nfactor(pair)P32\n-9.00\n11.22\n-0.80\n0.43\n-28.00\n10.00\n\n\nfactor(pair)P33\n-7.00\n11.22\n-0.62\n0.54\n-26.00\n12.00\n\n\n\n\n\nand the key elements are found in the statusexposed row, which we can focus on nicely (since the output of the tidy() function is always a tibble) with:\n\ntidy(model2_lead, conf.int = TRUE, conf.level = 0.90) |&gt;\n    filter(term == \"statusexposed\") |&gt;\n    kbl(digits = 2) |&gt; kable_styling()\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\nstatusexposed\n15.97\n2.76\n5.78\n0\n11.29\n20.65\n\n\n\n\nand again, we have our 90% confidence interval estimate of the population mean difference between exposed and control children.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Comparing Means with Paired Samples</span>"
    ]
  },
  {
    "objectID": "21-pairedsamples.html#matched-pairs-vs.-two-independent-samples",
    "href": "21-pairedsamples.html#matched-pairs-vs.-two-independent-samples",
    "title": "\n21  Comparing Means with Paired Samples\n",
    "section": "\n21.7 Matched Pairs vs. Two Independent Samples",
    "text": "21.7 Matched Pairs vs. Two Independent Samples\nThese data were NOT obtained from two independent samples, but rather from matched pairs.\n\nWe only have matched pairs if each individual observation in the “treatment” group is matched to one and only one observation in the “control” group by the way in which the data were gathered. Paired (or matched) data can arise in several ways.\n\nThe most common is a “pre-post” study where subjects are measured both before and after an exposure happens.\nIn observational studies, we often match up subjects who did and did not receive an exposure so as to account for differences on things like age, sex, race and other covariates. This is what happens in the Lead in the Blood of Children study.\n\n\nIf the data are from paired samples, we should (and in fact) must form paired differences, with no subject left unpaired.\n\nIf we cannot line up the data comparing two samples of quantitative data so that the links between the individual “treated” and “control” observations to form matched pairs are evident, then the data are not paired.\nIf the sample sizes were different, we’d know we have independent samples, because matched pairs requires that each subject in the “treated” group be matched to a single, unique member of the “control” group, and thus that we have exactly as many “treated” as “control” subjects.\nBut having as many subjects in one treatment group as the other (which is called a balanced design) is only necessary, and not sufficient, for us to conclude that matched pairs are used.\n\n\n\nAs Bock, Velleman, and De Veaux (2004) suggest,\n\n… if you know the data are paired, you can take advantage of that fact - in fact, you must take advantage of it. … You must decide whether the data are paired from understanding how they were collected and what they mean. … There is no test to determine whether the data are paired.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Comparing Means with Paired Samples</span>"
    ]
  },
  {
    "objectID": "21-pairedsamples.html#estimating-the-population-mean-of-the-paired-differences",
    "href": "21-pairedsamples.html#estimating-the-population-mean-of-the-paired-differences",
    "title": "\n21  Comparing Means with Paired Samples\n",
    "section": "\n21.8 Estimating the Population Mean of the Paired Differences",
    "text": "21.8 Estimating the Population Mean of the Paired Differences\nThere are two main approaches used frequently to estimate the population mean of paired differences.\n\nEstimation using the t distribution (and assuming at least an approximately Normal distribution for the paired differences)\nEstimation using the bootstrap (which doesn’t require the Normal assumption)\n\nIn addition, we might consider estimating an alternate statistic when the data don’t follow a symmetric distribution, like the median, with the bootstrap. In other settings, a rank-based alternative called the Wilcoxon signed rank test is available to estimate a psuedo-median. All of these approaches mirror what we did with a single sample, earlier in these Notes.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Comparing Means with Paired Samples</span>"
    ]
  },
  {
    "objectID": "21-pairedsamples.html#t-based-ci-for-population-mean-of-paired-differences",
    "href": "21-pairedsamples.html#t-based-ci-for-population-mean-of-paired-differences",
    "title": "\n21  Comparing Means with Paired Samples\n",
    "section": "\n21.9 t-based CI for Population Mean of Paired Differences",
    "text": "21.9 t-based CI for Population Mean of Paired Differences\nIn R, there are at least five different methods for obtaining the t-based confidence interval for the population difference in means between paired samples. They are all mathematically identical. The key idea is to calculate the paired differences (exposed - control, for example) in each pair, and then treat the result as if it were a single sample and apply the methods developed for that situation earlier in these Notes.\n\n21.9.1 Method 1\nWe can use the single-sample approach, applied to the variable containing the paired differences. Let’s build a 90% two-sided confidence interval for the population mean of the difference in blood lead content across all possible pairs of an exposed (parent works in a lead-based industry) and a control (parent does not) child.\n\ntt1 &lt;- t.test(bloodlead$lead_diff, conf.level = 0.90, \n                            alt = \"two.sided\")\n\ntt1\n\n\n    One Sample t-test\n\ndata:  bloodlead$lead_diff\nt = 5.783, df = 32, p-value = 2.036e-06\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 11.29201 20.64738\nsample estimates:\nmean of x \n  15.9697 \n\ntidy(tt1) |&gt; kbl(digits = 2) |&gt; kable_styling()\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n15.97\n5.78\n0\n32\n11.29\n20.65\nOne Sample t-test\ntwo.sided\n\n\n\n\nThe 90% confidence interval is (11.29, 20.65) according to this t-based procedure. An appropriate interpretation of the 90% two-sided confidence interval would be:\n\n(11.29, 20.65) milligrams per deciliter is a 90% two-sided confidence interval for the population mean difference in blood lead content between exposed and control children.\nOur point estimate for the true population difference in mean blood lead content is 15.97 mg.dl. The values in the interval (11.29, 20.65) mg/dl represent a reasonable range of estimates for the true population difference in mean blood lead content, and we are 90% confident that this method of creating a confidence interval will produce a result containing the true population mean difference.\nWere we to draw 100 samples of 33 matched pairs from the population described by this sample, and use each such sample to produce a confidence interval in this manner, approximately 90 of those confidence intervals would cover the true population mean difference in blood lead content levels.\n\n21.9.2 Method 2\nOr, we can apply the single-sample approach to a calculated difference in blood lead content between the exposed and control groups. Here, we’ll get a 95% two-sided confidence interval for that difference, instead of the 90% interval we obtained above.\n\ntt2 &lt;- t.test(bloodlead$exposed - bloodlead$control, \n       conf.level = 0.95, alt = \"two.sided\")\n\ntt2\n\n\n    One Sample t-test\n\ndata:  bloodlead$exposed - bloodlead$control\nt = 5.783, df = 32, p-value = 2.036e-06\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 10.34469 21.59470\nsample estimates:\nmean of x \n  15.9697 \n\ntidy(tt2) |&gt; kbl(digits = 2) |&gt; kable_styling()\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n15.97\n5.78\n0\n32\n10.34\n21.59\nOne Sample t-test\ntwo.sided\n\n\n\n\n\n21.9.3 Method 3\nOr, we can provide R with two separate samples (unaffected and affected) and specify that the samples are paired. Here, we’ll get a 99% one-sided confidence interval (lower bound) for the population mean difference in blood lead content.\n\ntt3 &lt;- t.test(bloodlead$exposed, bloodlead$control, conf.level = 0.99,\n       paired = TRUE, alt = \"greater\")\n\ntt3\n\n\n    Paired t-test\n\ndata:  bloodlead$exposed and bloodlead$control\nt = 5.783, df = 32, p-value = 1.018e-06\nalternative hypothesis: true mean difference is greater than 0\n99 percent confidence interval:\n 9.207658      Inf\nsample estimates:\nmean difference \n        15.9697 \n\ntidy(tt3) |&gt; kbl(digits = 2) |&gt; kable_styling()\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n15.97\n5.78\n0\n32\n9.21\nInf\nPaired t-test\ngreater\n\n\n\n\nAgain, the three different methods using t.test for paired samples will all produce identical results if we feed them the same confidence level and type of interval (two-sided, greater than or less than).\n\n21.9.4 Method 4\nWe can also use an intercept-only linear regression model to estimate the population mean of the paired differences with a two-tailed confidence interval, by creating a variable containing those paired differences.\n\nmodel_lead &lt;- lm(lead_diff ~ 1, data = bloodlead)\n\ntidy(model_lead, conf.int = TRUE, conf.level = 0.95) |&gt; \n  kbl(digits = 2) |&gt; kable_styling()\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n(Intercept)\n15.97\n2.76\n5.78\n0\n10.34\n21.59\n\n\n\n\n\n21.9.5 Method 5\nIf we have the data in a longer format, with a variable identifying the matched pairs, we can use a different specification for a linear model to obtain the same estimate.\n\nmodel2_lead &lt;- lm(lead_level ~ status + factor(pair), data = bloodlead_longer)\n\ntidy(model2_lead, conf.int = TRUE, conf.level = 0.95) |&gt;\n    filter(term == \"statusexposed\") |&gt; kbl(digits = 2) |&gt; kable_styling()\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\nstatusexposed\n15.97\n2.76\n5.78\n0\n10.34\n21.59\n\n\n\n\n\n21.9.6 Assumptions\nIf we are building a confidence interval based on a sample of observations drawn from a population, then we must pay close attention to the assumptions of those procedures. The confidence interval procedure for the population mean paired difference using the t distribution assumes that:\n\nWe want to estimate the population mean paired difference.\nWe have drawn a sample of paired differences at random from the population of interest.\nThe sampled paired differences are drawn from the population set of paired differences independently and have identical distributions.\nThe population follows a Normal distribution. At the very least, the sample itself is approximately Normal.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Comparing Means with Paired Samples</span>"
    ]
  },
  {
    "objectID": "21-pairedsamples.html#bootstrap-ci-for-mean-difference-using-paired-samples",
    "href": "21-pairedsamples.html#bootstrap-ci-for-mean-difference-using-paired-samples",
    "title": "\n21  Comparing Means with Paired Samples\n",
    "section": "\n21.10 Bootstrap CI for mean difference using paired samples",
    "text": "21.10 Bootstrap CI for mean difference using paired samples\nThe same bootstrap approach is used for paired differences as for a single sample. We use the smean.cl.boot() function in the Hmisc package to obtain bootstrap confidence intervals for the population mean of the paired differences in blood lead content.\n\nset.seed(431555)\nsmean.cl.boot(bloodlead$lead_diff, B = 1000, conf.int = 0.95)\n\n    Mean    Lower    Upper \n15.96970 10.81742 21.48788 \n\n\nNote that in this case, the confidence interval for the difference in means is a bit less wide than the 95% confidence interval generated by the t test, which was (10.34, 21.59). It’s common for the bootstrap to produce a narrower range (i.e. an apparently more precise estimate) for the population mean, but it’s not automatic that the endpoints from the bootstrap will be inside those provided by the t test, either.\nFor example, this bootstrap CI doesn’t contain the t-test based interval, since its upper bound exceeds that of the t-based interval:\n\nset.seed(431002)\nsmean.cl.boot(bloodlead$lead_diff, B = 1000, conf.int = 0.95)\n\n    Mean    Lower    Upper \n15.96970 10.81667 21.66667 \n\n\nThis demonstration aside, the appropriate thing to do when applying the bootstrap to specify a confidence interval is select a seed and the number (B = 1,000 or 10,000, usually) of desired bootstrap replications, then run the bootstrap just once and move on, rather than repeating the process multiple times looking for a particular result.\n\n21.10.1 Assumptions\nThe bootstrap confidence interval procedure for the population mean (or median) of a set of paired differences assumes that:\n\nWe want to estimate the population mean of the paired differences (or the population median).\nWe have drawn a sample of observations at random from the population of interest.\nThe sampled observations are drawn from the population of paired differences independently and have identical distributions.\nWe are willing to put up with the fact that different people (not using the same random seed) will get somewhat different confidence interval estimates using the same data.\n\nAs we’ve seen, a major part of the bootstrap’s appeal is the ability to relax some assumptions.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Comparing Means with Paired Samples</span>"
    ]
  },
  {
    "objectID": "21-pairedsamples.html#wilcoxon-signed-rank-based-ci-for-paired-samples",
    "href": "21-pairedsamples.html#wilcoxon-signed-rank-based-ci-for-paired-samples",
    "title": "\n21  Comparing Means with Paired Samples\n",
    "section": "\n21.11 Wilcoxon Signed Rank-based CI for paired samples",
    "text": "21.11 Wilcoxon Signed Rank-based CI for paired samples\nWe could also use the Wilcoxon signed rank procedure to generate a CI for the pseudo-median of the paired differences.\n\nwt &lt;- wilcox.test(bloodlead$lead_diff, conf.int = TRUE,\n                  conf.level = 0.90, exact = FALSE)\nwt\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  bloodlead$lead_diff\nV = 499, p-value = 1.155e-05\nalternative hypothesis: true location is not equal to 0\n90 percent confidence interval:\n 10.99992 20.49998\nsample estimates:\n(pseudo)median \n      15.49996 \n\ntidy(wt) |&gt; kbl(digits = 2) |&gt; kable_styling()\n\n\n\nestimate\nstatistic\np.value\nconf.low\nconf.high\nmethod\nalternative\n\n\n15.5\n499\n0\n11\n20.5\nWilcoxon signed rank test with continuity correction\ntwo.sided\n\n\n\n\nAs in the one sample case, we can revise this code slightly to specify a different confidence level, or gather a one-sided rather than a two-sided confidence interval.\n\n21.11.1 Assumptions\nThe Wilcoxon signed rank confidence interval procedure in working with paired differences assumes that:\n\nWe want to estimate the population pseudo-median of the paired differences.\nWe have drawn a sample of observations at random from the population of paired differences of interest.\nThe sampled observations are drawn from the population of paired differences independently and have identical distributions.\nThe population follows a symmetric distribution. At the very least, the sample itself shows no substantial skew, so that the sample pseudo-median is a reasonable estimate for the population median.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Comparing Means with Paired Samples</span>"
    ]
  },
  {
    "objectID": "21-pairedsamples.html#choosing-a-confidence-interval-approach",
    "href": "21-pairedsamples.html#choosing-a-confidence-interval-approach",
    "title": "\n21  Comparing Means with Paired Samples\n",
    "section": "\n21.12 Choosing a Confidence Interval Approach",
    "text": "21.12 Choosing a Confidence Interval Approach\nSuppose we want to find a confidence interval for the population mean difference between two populations based on matched pairs.\n\nIf we are willing to assume that the population distribution is Normal\n\nwe usually use a t-based CI.\n\n\nIf we are unwilling to assume that the population is Normal,\n\nuse a bootstrap procedure to get a CI for the population mean, or even the median\nbut are willing to assume the population is symmetric, consider a Wilcoxon signed rank procedure to get a CI for the median, rather than the mean.\n\n\n\nThe two methods you’ll use most often are the bootstrap (especially if the data don’t appear to be at least pretty well fit by a Normal model) and the t-based confidence intervals (if the data do appear to fit a Normal model reasonably well.)",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Comparing Means with Paired Samples</span>"
    ]
  },
  {
    "objectID": "21-pairedsamples.html#conclusions-for-the-bloodlead-study",
    "href": "21-pairedsamples.html#conclusions-for-the-bloodlead-study",
    "title": "\n21  Comparing Means with Paired Samples\n",
    "section": "\n21.13 Conclusions for the bloodlead study",
    "text": "21.13 Conclusions for the bloodlead study\nUsing any of these procedures, we would conclude that the null hypothesis (that the true mean of the paired Exposed - Control differences is 0 mg/dl) is not consonant with what we see in the 90% confidence interval.\n\n\nProcedure\nComparing\n90% CI\n\n\n\nPaired t\nMeans\n11.3, 20.6\n\n\nWilcoxon signed rank\nPseudo-medians\n11, 20.5\n\n\nBootstrap CI\nMeans\n11.6, 20.6\n\n\n\nNote that one-sided or one-tailed hypothesis testing procedures work the same way for paired samples as they did for a single sample.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Comparing Means with Paired Samples</span>"
    ]
  },
  {
    "objectID": "21-pairedsamples.html#the-sign-test",
    "href": "21-pairedsamples.html#the-sign-test",
    "title": "\n21  Comparing Means with Paired Samples\n",
    "section": "\n21.14 The Sign test",
    "text": "21.14 The Sign test\nThe sign test is something we’ve skipped in our discussion so far. It is a test for consistent differences between pairs of observations, just as the paired t, Wilcoxon signed rank and bootstrap for paired samples can provide. It has the advantage that it is relatively easy to calculate by hand, and that it doesn’t require the paired differences to follow a Normal distribution. In fact, it will even work if the data are substantially skewed.\n\nCalculate the paired difference for each pair, and drop those with difference = 0.\nLet N be the number of pairs that remain, so there are 2N data points.\nLet W, the test statistic, be the number of pairs (out of N) in which the difference is positive.\nAssuming that H0 is true, then W follows a binomial distribution with probability 0.5 on N trials.\n\nFor example, consider our data on blood lead content:\n\nbloodlead$lead_diff\n\n [1] 22  5 23 -6 18 25 13 47 15 16  6  1  2  7  0  4 -9 -3 36 25  1 16 42 30 25\n[26] 23 32 17  9 -3 60 14 14\n\n\n\n\nDifference\n# of Pairs\n\n\n\nGreater than zero\n28\n\n\nEqual to zero\n1\n\n\nLess than zero\n4\n\n\n\nSo we have N = 32 pairs, with \\(W\\) = 28 that are positive. We then use the binom.test approach in R:\n\nbinom.test(x = 28, n = 32, p = 0.5, \n           alternative = \"two.sided\")\n\n\n    Exact binomial test\n\ndata:  28 and 32\nnumber of successes = 28, number of trials = 32, p-value = 1.93e-05\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.7100516 0.9648693\nsample estimates:\nprobability of success \n                 0.875 \n\n\n\nA one-tailed test can be obtained by substituting in “less” or “greater” as the alternative of interest.\nThe confidence interval provided doesn’t relate back to our original population means. It’s just showing the confidence interval around the probability of the exposed mean being greater than the control mean for a pair of children.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Comparing Means with Paired Samples</span>"
    ]
  },
  {
    "objectID": "21-pairedsamples.html#paired-dependent-vs.-independent-samples",
    "href": "21-pairedsamples.html#paired-dependent-vs.-independent-samples",
    "title": "\n21  Comparing Means with Paired Samples\n",
    "section": "\n21.15 Paired (Dependent) vs. Independent Samples",
    "text": "21.15 Paired (Dependent) vs. Independent Samples\nOne area that consistently trips students up in this course is the thought process involved in distinguishing studies comparing means that should be analyzed using dependent (i.e. paired or matched) samples and those which should be analyzed using independent samples. A dependent samples analysis uses additional information about the sample to pair/match subjects receiving the various exposures. That additional information is not part of an independent samples analysis (unpaired testing situation.) The reasons to do this are to (a) increase statistical power, and/or (b) reduce the effect of confounding. Here are a few thoughts on the subject.\nIn the design of experiments, blocking is the term often used for the process of arranging subjects into groups (blocks) that are similar to one another. Typically, a blocking factor is a source of variability that is not of primary interest to the researcher An example of a blocking factor might be the sex of a patient; by blocking on sex, this source of variability is controlled for, thus leading to greater accuracy.\n\nIf the sample sizes are not balanced (not equal), the samples must be treated as independent, since there would be no way to precisely link all subjects. So, if we have 10 subjects receiving exposure A and 12 subjects receiving exposure B, a dependent samples analysis (such as a paired t test) is not correct.\nThe key element is a meaningful link between each observation in one exposure group and a specific observation in the other exposure group. Given a balanced design, the most common strategy indicating dependent samples involves two or more repeated measures on the same subjects. For example, if we are comparing outcomes before and after the application of an exposure, and we have, say, 20 subjects who provide us data both before and after the exposure, then the comparison of results before and after exposure should use a dependent samples analysis. The link between the subjects is the subject itself - each exposed subject serves as its own control.\nThe second most common strategy indicating dependent samples involves deliberate matching of subjects receiving the two exposures. A matched set of observations (often a pair, but it could be a trio or quartet, etc.) is determined using baseline information and then (if a pair is involved) one subject receives exposure A while the other member of the pair receives exposure B, so that by calculating the paired difference, we learn about the effect of the exposure, while controlling for the variables made similar across the two subjects by the matching process.\nIn order for a dependent samples analysis to be used, we need (a) a link between each observation across the exposure groups based on the way the data were collected, and (b) a consistent measure (with the same units of measurement) so that paired differences can be calculated and interpreted sensibly.\nIf the samples are collected to facilitate a dependent samples analysis, the correlation of the outcome measurements across the groups will often be moderately strong and positive. If that’s the case, then the use of a dependent samples analysis will reduce the effect of baseline differences between the exposure groups, and thus provide a more precise estimate. But even if the correlation is quite small, a dependent samples analysis should provide a more powerful estimate of the impact of the exposure on the outcome than would an independent samples analysis with the same number of observations.\n\n\n21.15.1 Three “Tricky” Examples\n\nSuppose we take a convenient sample of 200 patients from the population of patients who complete a blood test in April 2017 including a check of triglycerides, and who have a triglyceride level in the high category (200 to 499 mg/dl). Next, we select a patient at random from this group of 200 patients, and then identify another patient from the group of 200 who is the same age (to within 2 years) and also the same sex. We then randomly assign our intervention to one of these two patients and usual care without our intervention to the other patient. We then set these two patients aside and return to our original sample, repeating the process until we cannot find any more patients in the same age range and of the same gender. This generates a total of 77 patients who receive the intervention and 77 who do not. If we are trying to assess the effect of our intervention on triglyceride level in October 2017 using this sample of 154 people, should we use dependent (paired) or independent samples?\nSuppose we take a convenient sample of 77 patients from the population of patients who complete a blood test in April 2017 including a check of triglycerides, and who have a triglyceride level in the high category (200 to 499 mg/dl). Next, we take a convenient sample of 77 patients from the population of patients who complete a blood test in May 2017 including a check of triglycerides, and who have a triglyceride level in the high category (200 to 499 mg/dl). We flip a coin to determine whether the intervention will be given to each of the 77 patients from April 2017 (if the coin comes up “HEADS”) or instead to each of the 77 patients from May 2017 (if the coin comes up “TAILS”). Then, we assign our intervention to the patients seen in the month specified by the coin and assign usual care without our intervention to the patients seen in the other month. If we are trying to assess the effect of our intervention on triglyceride level in October 2017 using this sample of 154 people, should we use dependent (paired) or independent samples?\nSuppose we take a convenient sample of 200 patients from the population of patients who complete a blood test in April 2017 including a check of triglycerides, and who have a triglyceride level in the high category (200 to 499 mg/dl). For each patient, we re-measure them again in October 2017, again checking their triglyceride level. But in between, we take the first 77 of the patients in a randomly sorted list and assign them to our intervention (which takes place from June through September 2017) and take an additional group of 77 patients from the remaining part of the list and assign them to usual care without our intervention over the same time period. If we are trying to assess the effect of our intervention on each individual’s change in triglyceride level (from April/May to October) using this sample of 154 people, should we use dependent (paired) or independent samples?\n\nAnswers to these “tricky” examples appear at the end of this Chapter.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Comparing Means with Paired Samples</span>"
    ]
  },
  {
    "objectID": "21-pairedsamples.html#a-more-complete-decision-support-tool-comparing-means",
    "href": "21-pairedsamples.html#a-more-complete-decision-support-tool-comparing-means",
    "title": "\n21  Comparing Means with Paired Samples\n",
    "section": "\n21.16 A More Complete Decision Support Tool: Comparing Means",
    "text": "21.16 A More Complete Decision Support Tool: Comparing Means\n\nAre these paired or independent samples?\n\nIf paired samples, then are the paired differences approximately Normally distributed?\n\nIf yes, then a paired t test or confidence interval is likely the best choice.\nIf no, is the main concern outliers (with generally symmetric data), or skew?\n\nIf the paired differences appear to be generally symmetric but with substantial outliers, a Wilcoxon signed rank test is an appropriate choice, as is a bootstrap confidence interval for the population mean of the paired differences.\nIf the paired differences appear to be seriously skewed, then we’ll usually build a bootstrap confidence interval, although a sign test is another reasonable possibility, although it doesn’t provide a confidence interval for the population mean of the paired differences.\n\n\n\n\n\nIf independent, is each sample Normally distributed?\n\nNo –&gt; use Wilcoxon-Mann-Whitney rank sum test or bootstrap via bootdif.\nYes –&gt; are sample sizes equal?\n\nBalanced Design (equal sample sizes) - use pooled t test\nUnbalanced Design - use Welch test\n\n\n\n\n\n\n21.16.1 Answers for the Three “Tricky” Examples\nAnswer for 1. Our first task is to identify the outcome and the exposure groups. Here, we are comparing the distribution of our outcome (triglyceride level in October) across two exposures: (a) receiving the intervention and (b) not receiving the intervention. We have a sample of 77 patients receiving the intervention, and a different sample of 77 patients receiving usual care. Each of the 77 subjects receiving the intervention is matched (on age and sex) to a specific subject not receiving the intervention. So, we can calculate paired differences by taking the triglyceride level for the exposed member of each pair and subtracting the triglyceride level for the usual care member of that same pair. Thus our comparison of the exposure groups should be accomplished using a dependent samples analysis, such as a paired t test.\nAnswer for 2. Again, we begin by identifying the outcome (triglyceride level in October) and the exposure groups. Here, we compare two exposures: (a) receiving the intervention and (b) receiving usual care. We have a sample of 77 patients receiving the intervention, and a different sample of 77 patients receiving usual care. But there is no pairing or matching involved. There is no connection implied by the way that the data were collected that implies that, for example, patient 1 in the intervention group is linked to any particular subject in the usual care group. So we need to analyze the data using independent samples.\nAnswer for 3. Once again, we identify the outcome (now it is the within-subject change in triglyceride level from April to October) and the exposure groups. Here again, we compare two exposures: (a) receiving the intervention and (b) receiving usual care. We have a sample of 77 patients receiving the intervention, and a different sample of 77 patients receiving usual care. But again, there is no pairing or matching between the patients receiving the intervention and the patients receiving usual care. While each outcome value is a difference (or change) in triglyceride levels, there’s no connection implied by the way that the data were collected that implies that, for example, patient 1 in the intervention group is linked to any particular subject in the usual care group. So, again, we need to analyze the data using independent samples.\nFor more background and fundamental material, you might consider the Wikipedia pages on Paired Difference Test and on Blocking (statistics).\n\n\n\n\nBock, David E., Paul F. Velleman, and Richard D. De Veaux. 2004. Stats: Modelling the World. Boston MA: Pearson Addison-Wesley.\n\n\nGood, Phillip I. 2005. Introduction to Statistics Through Resampling Methods and r/s-PLUS. Hoboken, NJ: Wiley.\n\n\nHadley Wickham, Mine Çetinyaka-Rundel, and Garrett Grolemund. 2023. R for Data Science. Second. O’Reilly. https://r4ds.hadley.nz/.\n\n\nMorton, D., A. Saah, S. Silberg, W. Owens, M. Roberts, and M. Saah. 1982. “Lead Absorption in Children of Employees in a Lead Related Industry.” American Journal of Epidemiology 115: 549–55.\n\n\nPruzek, Robert M., and James E. Helmreich. 2009. “Enhancing Dependent Sample Analyses with Graphics.” Journal of Statistics Education 17(1). http://ww2.amstat.org/publications/jse/v17n1/helmreich.html.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Comparing Means with Paired Samples</span>"
    ]
  },
  {
    "objectID": "22-samplesize.html",
    "href": "22-samplesize.html",
    "title": "22  Sample Size and Power for Means",
    "section": "",
    "text": "22.1 Setup: Package Used Here\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(pwr)",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Sample Size and Power for Means</span>"
    ]
  },
  {
    "objectID": "22-samplesize.html#power-and-sample-size-considerations",
    "href": "22-samplesize.html#power-and-sample-size-considerations",
    "title": "22  Sample Size and Power for Means",
    "section": "\n22.2 Power and Sample Size Considerations",
    "text": "22.2 Power and Sample Size Considerations\nI’ll begin here by repeating some of the material first shared in Chapter 18. For most statistical tests, it is theoretically possible to estimate the power of the test in the design stage, (before any data are collected) for various sample sizes, so we can hone in on a sample size choice which will enable us to collect data only on as many subjects as are truly necessary.\nA power calculation is likely the most common element of an scientific grant proposal on which a statistician is consulted. This is a fine idea in theory, but in practice…\n\nThe tests that have power calculations worked out in intensive detail using R are mostly those with more substantial assumptions. Examples include t tests that assume population normality, common population variance and balanced designs in the independent samples setting, or paired t tests that assume population normality in the paired samples setting.\nThese power calculations are also usually based on tests rather than confidence intervals, which would be much more useful in most settings. Simulation is your friend here.\nEven more unfortunately, this process of doing power and related calculations is far more of an art than a science.\nAs a result, the value of many power calculations is negligible, since the assumptions being made are so arbitrary and poorly connected to real data.\nOn several occasions, I have stood in front of a large audience of medical statisticians actively engaged in clinical trials and other studies that require power calculations for funding. When I ask for a show of hands of people who have had power calculations prior to such a study whose assumptions matched the eventual data perfectly, I get lots of laughs. It doesn’t happen.\nEven the underlying framework that assumes a power of 80% with a significance level of 5% is sufficient for most studies is pretty silly.\n\nAll that said, I feel obliged to show you some examples of power calculations done using R, and provide some insight on how to make some of the key assumptions in a way that won’t alert reviewers too much to the silliness of the enterprise. All of the situations described in this Chapter are toy problems, but they may be instructive about some fundamental ideas.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Sample Size and Power for Means</span>"
    ]
  },
  {
    "objectID": "22-samplesize.html#sample-size-in-a-one-sample-t-test",
    "href": "22-samplesize.html#sample-size-in-a-one-sample-t-test",
    "title": "22  Sample Size and Power for Means",
    "section": "\n22.3 Sample Size in a One-Sample t test",
    "text": "22.3 Sample Size in a One-Sample t test\nFor a t test, R can estimate any one of the following elements, given the other four, using the power.t.test command, for either a one-tailed or two-tailed single-sample t test…\n\nn = the sample size\n\n\\(\\delta\\) = delta = the true difference in population means between the null hypothesis value and a particular alternative\ns = sd = the true standard deviation of the population\n\n\\(\\alpha\\) = sig.level = the significance level for the test (maximum acceptable risk of Type I error)\n1 - \\(\\beta\\) = power = the power of the t test to detect the effect of size \\(\\delta\\)\n\n\n\n22.3.1 A Toy Example\nSuppose that in a recent health survey, the average beef consumption in the U.S. per person was 90 pounds per year. Suppose you are planning a new study to see if beef consumption levels have changed. You plan to take a random sample of 25 people to build your new estimate, and test whether the current pounds of beef consumed per year is 90. Suppose you want to do a two-sided (two-tailed) test at 95% confidence (so \\(\\alpha\\) = 0.05), and that you expect that the true difference will need to be at least \\(\\delta\\) = 5 pounds (i.e. 85 or less or 95 or more) in order for the result to be of any real, practical interest. Suppose also that you are willing to assume that the true standard deviation of the measurements in the population is 10 pounds.\nThat is, of course, a lot to suppose.\nNow, we want to know what power the proposed experiment will have to detect a change of 5 pounds (or more) away from the original 90 pounds, with these specifications, and how tweaking these specifications will affect the power of the study.\nSo, we have - n = 25 data points to be collected - \\(\\delta\\) = 5 pounds is the minimum clinically meaningful effect size - s = 10 is the assumed population standard deviation, in pounds per year - \\(\\alpha\\) is 0.05, and we’ll do a two-sided test\n\n22.3.2 Using the power.t.test function\n\npower.t.test(n = 25, delta = 5, sd = 10, sig.level = 0.05, \n             type=\"one.sample\", alternative=\"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 25\n          delta = 5\n             sd = 10\n      sig.level = 0.05\n          power = 0.6697014\n    alternative = two.sided\n\n\nSo, under this study design, we would expect to detect an effect of size \\(\\delta\\) = 5 pounds with just under 67% power, i.e. with a probability of incorrect retention of \\(H_0\\) of just about 1/3. Most of the time, we’d like to improve this power, and to do so, we’d need to adjust our assumptions.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Sample Size and Power for Means</span>"
    ]
  },
  {
    "objectID": "22-samplesize.html#changing-assumptions",
    "href": "22-samplesize.html#changing-assumptions",
    "title": "22  Sample Size and Power for Means",
    "section": "\n22.4 Changing Assumptions",
    "text": "22.4 Changing Assumptions\nWe made assumptions about the sample size n, the minimum clinically meaningful effect size (change in the population mean) \\(\\delta\\), the population standard deviation s, and the significance level \\(\\alpha\\), not to mention decisions about the test, like that we’d do a one-sample t test, rather than another sort of test for a single sample, and that we’d do a two-tailed, or two-sided test. Often, these assumptions are tweaked a bit to make the power look more like what a reviewer/funder is hoping to see.\n\n22.4.1 Increasing Sample Size Increases Power\nSuppose, we committed to using more resources and gathering data from 40 subjects instead of the 25 we assumed initially – what effect would this have on our power?\n\npower.t.test(n = 40, delta = 5, sd = 10, sig.level = 0.05, \n             type=\"one.sample\", alternative=\"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 40\n          delta = 5\n             sd = 10\n      sig.level = 0.05\n          power = 0.8693979\n    alternative = two.sided\n\n\nWith more samples, we should have a more powerful test, able to detect the difference with greater probability. In fact, a sample of 40 paired differences yields 87% power. As it turns out, we would need at least 44 observations with this scenario to get to 90% power, as shown in the calculation below, which puts the power in, but leaves out the sample size.\n\npower.t.test(power=0.9, delta = 5, sd = 10, sig.level = 0.05, \n             type=\"one.sample\", alternative=\"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 43.99552\n          delta = 5\n             sd = 10\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\n\nWe see that we would need at least 44 observations to achieve 90% power. Note: we always round the sample size up in doing a power calculation – if this calculation had actually suggested n = 43.1 paired differences were needed, we would still have rounded up to 44.\n\n22.4.2 Increasing Effect Size will increase Power\nA larger effect should be easier to detect. If we go back to our original calculation, which had 67% power to detect an effect of size \\(\\delta\\) = 5, and now change the desired effect size to \\(\\delta\\) = 6 pounds (i.e. a value of 84 or less or 96 or more), we should obtain a more powerful design.\n\npower.t.test(n = 25, delta = 6, sd = 10, sig.level = 0.05, \n             type=\"one.sample\", alternative=\"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 25\n          delta = 6\n             sd = 10\n      sig.level = 0.05\n          power = 0.8207213\n    alternative = two.sided\n\n\nWe see that this change in effect size from 5 to 6, leaving everything else the same, increases our power from 67% to 82%. To reach 90% power, we’d need to increase the effect size we were trying to detect to at least 6.76 pounds.\n\npower.t.test(n = 25, power = 0.9, sd = 10, sig.level = 0.05, \n             type=\"one.sample\", alternative=\"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 25\n          delta = 6.759051\n             sd = 10\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\n\n\nAgain, note that I am rounding up here.\nUsing \\(\\delta\\) = 6.75 would not quite make it to 90.00% power.\nUsing \\(\\delta\\) = 6.76 guarantees that the power will be 90% or more, and not just round up to 90%.\n\n22.4.3 Decreasing the Standard Deviation will increase Power\nThe choice of standard deviation is usually motivated by a pilot study, or else pulled out of thin air - it’s relatively easy to convince yourself that the true standard deviation might be a little smaller than you’d guessed initially. Let’s see what happens to the power if we reduce the sample standard deviation from 10 pounds to 9. This should make the effect of 5 pounds easier to detect, because it will have smaller variation associated with it.\n\npower.t.test(n = 25, delta = 5, sd = 9, sig.level = 0.05, \n             type=\"one.sample\", alternative=\"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 25\n          delta = 5\n             sd = 9\n      sig.level = 0.05\n          power = 0.759672\n    alternative = two.sided\n\n\nThis change in standard deviation from 10 to 9, leaving everything else the same, increases our power from 67% to nearly 76%. To reach 90% power, we’d need to decrease the standard deviation of the population paired differences to no more than 7.39 pounds.\n\npower.t.test(n = 25, delta = 5, sd = NULL, power = 0.9, sig.level = 0.05, \n             type=\"one.sample\", alternative=\"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 25\n          delta = 5\n             sd = 7.397486\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\n\nNote I am rounding down here.\n\nUsing s = 7.4 pounds would not quite make it to 90.00% power.\n\nNote also that in order to get R to treat the standard debviation as unknown, I must specify it as NULL in the formula.\n\n22.4.4 Larger Significance Level increases Power\nWe can trade off some of our Type II error (lack of power) for Type I error. If we are willing to trade off some Type I error (as described by the \\(\\alpha\\)), we can improve the power. For instance, suppose we decided to run the original test with 90% confidence.\n\npower.t.test(n = 25, delta = 5, sd = 10, sig.level = 0.1, \n             type=\"one.sample\", alternative=\"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 25\n          delta = 5\n             sd = 10\n      sig.level = 0.1\n          power = 0.7833861\n    alternative = two.sided\n\n\nThe calculation suggests that our power would thus increase from 67% to just over 78%.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Sample Size and Power for Means</span>"
    ]
  },
  {
    "objectID": "22-samplesize.html#paired-sample-t-tests-and-powersample-size",
    "href": "22-samplesize.html#paired-sample-t-tests-and-powersample-size",
    "title": "22  Sample Size and Power for Means",
    "section": "\n22.5 Paired Sample t Tests and Power/Sample Size",
    "text": "22.5 Paired Sample t Tests and Power/Sample Size\nFor a paired-samples t test, R can estimate any one of the following elements, given the other four, using the power.t.test command, for either a one-tailed or two-tailed paired t test…\n\nn = the sample size (# of pairs) being compared\n\n\\(\\delta\\) = delta = the true difference in means between the two groups\ns = sd = the true standard deviation of the paired differences\n\n\\(\\alpha\\) = sig.level = the significance level for the comparison (maximum acceptable risk of Type I error)\n1 - \\(\\beta\\) = power = the power of the paired t test to detect the effect of size \\(\\delta\\)\n\n\n\n22.5.1 A Toy Example\nAs a toy example, suppose you are planning a paired samples experiment involving n = 30 subjects who will each provide a “Before” and an “After” result, which is measured in days.\nSuppose you want to do a two-sided (two-tailed) test at 95% confidence (so \\(\\alpha\\) = 0.05), and that you expect that the true difference between the “Before” and “After” groups will have to be at least \\(\\delta\\) = 5 days to be of any real interest. Suppose also that you are willing to assume that the true standard deviation of those paired differences will be 10 days.\nThat is, of course, a lot to suppose.\nNow, we want to know what power the proposed experiment will have to detect this difference with these specifications, and how tweaking these specifications will affect the power of the study.\nSo, we have - n = 30 paired differences will be collected - \\(\\delta\\) = 5 days is the minimum clinically meaningful difference - s = 10 days is the assumed population standard deviation of the paired differences - \\(\\alpha\\) is 0.05, and we’ll do a two-sided test\n\n22.5.2 Using the power.t.test function\n\npower.t.test(n = 30, delta = 5, sd = 10, sig.level = 0.05, \n             type=\"paired\", alternative=\"two.sided\")\n\n\n     Paired t test power calculation \n\n              n = 30\n          delta = 5\n             sd = 10\n      sig.level = 0.05\n          power = 0.7539627\n    alternative = two.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs\n\n\nSo, under this study design, we would expect to detect an effect of size \\(\\delta\\) = 5 days with 75% power, i.e. with a probability of incorrect retention of \\(H_0\\) of 0.25. Most of the time, we’d like to improve this power, and to do so, we’d need to adjust our assumptions.\n\n22.5.3 Changing Assumptions in a Power Calculation\nWe made assumptions about the sample size n, the minimum clinically meaningful difference in means \\(\\delta\\), the population standard deviation s, and the significance level \\(\\alpha\\), not to mention decisions about the test, like that we’d do a paired t test, rather than another sort of test for paired samples, or use an independent samples approach, and that we’d do a two-tailed, or two-sided test. Often, these assumptions are tweaked a bit to make the power look more like what a reviewer/funder is hoping to see.\n\n22.5.4 Changing the Sample Size\nSuppose, we committed to using more resources and gathering “Before” and “After” data from 40 subjects instead of the 30 we assumed initially – what effect would this have on our power?\n\npower.t.test(n = 40, delta = 5, sd = 10, sig.level = 0.05, \n             type=\"paired\", alternative=\"two.sided\")\n\n\n     Paired t test power calculation \n\n              n = 40\n          delta = 5\n             sd = 10\n      sig.level = 0.05\n          power = 0.8693979\n    alternative = two.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs\n\n\nWith more samples, we should have a more powerful test, able to detect the difference with greater probability. In fact, a sample of 40 paired differences yields 87% power. As it turns out, we would need at least 44 paired differences with this scenario to get to 90% power, as shown in the calculation below, which puts the power in, but leaves out the sample size.\n\npower.t.test(power=0.9, delta = 5, sd = 10, sig.level = 0.05, \n             type=\"paired\", alternative=\"two.sided\")\n\n\n     Paired t test power calculation \n\n              n = 43.99552\n          delta = 5\n             sd = 10\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs\n\n\nWe see that we would need at least 44 paired differences to achieve 90% power. Note: we always round the sample size up in doing a power calculation – if this calculation had actually suggested n = 43.1 paired differences were needed, we would still have rounded up to 44.\n\n22.5.5 Changing the Effect Size\nA larger effect should be easier to detect. If we go back to our original calculation, which had 75% power to detect an effect (i.e. a true population mean difference) of size \\(\\delta\\) = 5, and now change the desired effect size to \\(\\delta\\) = 6, we should obtain a more powerful design.\n\npower.t.test(n = 30, delta = 6, sd = 10, sig.level = 0.05, \n             type=\"paired\", alternative=\"two.sided\")\n\n\n     Paired t test power calculation \n\n              n = 30\n          delta = 6\n             sd = 10\n      sig.level = 0.05\n          power = 0.887962\n    alternative = two.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs\n\n\nWe see that this change in effect size from 5 to 6, leaving everything else the same, increases our power from 75% to nearly 89%. To reach 90% power, we’d need to increase the effect size we were trying to detect to at least 6.13 days.\n\nAgain, note that I am rounding up here.\nUsing \\(\\delta\\) = 6.12 would not quite make it to 90.00% power.\nUsing \\(\\delta\\) = 6.13 guarantees that the power will be 90% or more, and not just round up to 90%..\n\n22.5.6 Changing the Standard Deviation\nThe choice of standard deviation is usually motivated by a pilot study, or else pulled out of thin air. It’s relatively easy to convince yourself that the true standard deviation might be a little smaller than you’d guessed initially. Let’s see what happens to the power if we reduce the sample standard deviation from 10 days to 9 days. This should make the effect of 5 days easier to detect as being different from the null hypothesized value of 0, because it will have smaller variation associated with it.\n\npower.t.test(n = 30, delta = 5, sd = 9, sig.level = 0.05, \n             type=\"paired\", alternative=\"two.sided\")\n\n\n     Paired t test power calculation \n\n              n = 30\n          delta = 5\n             sd = 9\n      sig.level = 0.05\n          power = 0.8366514\n    alternative = two.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs\n\n\nThis change in standard deviation from 10 to 9, leaving everything else the same, increases our power from 75% to nearly 84%. To reach 90% power, we’d need to decrease the standard deviation of the population paired differences to no more than 8.16 days.\nNote I am rounding down here, because using \\(s\\) = 8.17 days would not quite make it to 90.00% power. Note also that in order to get R to treat the sd as unknown, I must specify it as NULL in the formula…\n\npower.t.test(n = 30, delta = 5, sd = NULL, power = 0.9, \n             sig.level = 0.05, type=\"paired\", alternative=\"two.sided\")\n\n\n     Paired t test power calculation \n\n              n = 30\n          delta = 5\n             sd = 8.163989\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs\n\n\n\n22.5.7 Changing the Significance Level\nWe can trade off some of our Type II error (lack of power) for Type I error. If we are willing to trade off some Type I error (as described by the \\(\\alpha\\)), we can improve the power. For instance, suppose we decided to run the original test with 90% confidence.\n\npower.t.test(n = 30, delta = 5, sd = 10, sig.level = 0.1, \n             type=\"paired\", alternative=\"two.sided\")\n\n\n     Paired t test power calculation \n\n              n = 30\n          delta = 5\n             sd = 10\n      sig.level = 0.1\n          power = 0.8482542\n    alternative = two.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs\n\n\nThe calculation suggests that our power would thus increase from 75% to nearly 85%.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Sample Size and Power for Means</span>"
    ]
  },
  {
    "objectID": "22-samplesize.html#two-independent-samples-power-for-t-tests",
    "href": "22-samplesize.html#two-independent-samples-power-for-t-tests",
    "title": "22  Sample Size and Power for Means",
    "section": "\n22.6 Two Independent Samples: Power for t Tests",
    "text": "22.6 Two Independent Samples: Power for t Tests\nFor an independent-samples t test, with a balanced design (so that \\(n_1\\) = \\(n_2\\)), R can estimate any one of the following elements, given the other four, using the power.t.test command, for either a one-tailed or two-tailed t test…\n\nn = the sample size in each of the two groups being compared\n\n\\(\\delta\\) = delta = the true difference in means between the two groups\ns = sd = the true standard deviation of the individual values in each group (assumed to be constant – since we assume equal population variances)\n\n\\(\\alpha\\) = sig.level = the significance level for the comparison (maximum acceptable risk of Type I error)\n1 - \\(\\beta\\) = power = the power of the t test to detect the effect of size \\(\\delta\\)\n\n\nThis method only produces power calculations for balanced designs – where the sample size is equal in the two groups. If you want a two-sample power calculation for an unbalanced design, you will need to use a different library and function in R, as we’ll see.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Sample Size and Power for Means</span>"
    ]
  },
  {
    "objectID": "22-samplesize.html#a-new-example",
    "href": "22-samplesize.html#a-new-example",
    "title": "22  Sample Size and Power for Means",
    "section": "\n22.7 A New Example",
    "text": "22.7 A New Example\nSuppose we plan a study of the time to relapse for patients in a drug trial, where subjects will be assigned randomly to a (new) treatment or to a placebo. Suppose we anticipate that the placebo group will have a mean of about 9 months, and want to detect an improvement (increase) in time to relapse of 50%, so that the treatment group would have a mean of at least 13.5 months. We’ll use \\(\\alpha\\) = .10 and \\(\\beta\\) = .10, as well. Assume we’d do a two-sided test, with an equal number of observations in each group, and we’ll assume the observed standard deviation of 9 months in a pilot study will hold here, as well.\nWe want the sample size required by the test under a two sample setting where:\n\n\n\\(\\alpha\\) = .10,\nwith 90% power (so that \\(\\beta\\) = .10),\nand where we will have equal numbers of samples in the placebo group (group 1) and the treatment group (group 2).\n\nWe’ll plug in the observed standard deviation of 9 months.\nWe’ll look at detecting a change from 9 [the average in the placebo group] to 13.5 (a difference of 50%, giving delta = 4.5)\nusing a two-sided pooled t-test.\n\nThe appropriate R command is:\n\npower.t.test(delta = 4.5, sd = 9, \n             sig.level = 0.10, power = 0.9, \n             type=\"two.sample\", \n             alternative=\"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 69.19782\n          delta = 4.5\n             sd = 9\n      sig.level = 0.1\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThis suggests that we will need a sample of at least 70 subjects in the treated group and an additional 70 subjects in the placebo group, for a total of 140 subjects.\n\n22.7.1 Another Scenario\nWhat if resources are sparse, and we’ll be forced to do the study with no more than 120 subjects, overall? If we require 90% confidence in a two-sided test, what power will we have?\n\npower.t.test(n = 60, delta = 4.5, sd = 9, \n             sig.level = 0.10,\n             type=\"two.sample\", \n             alternative=\"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 60\n          delta = 4.5\n             sd = 9\n      sig.level = 0.1\n          power = 0.859484\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nIt looks like the power under those circumstances would be just under 86%. Note that the n = 60 refers to half of the total sample size, since we’ll need 60 drug and 60 placebo subjects in this balanced design.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Sample Size and Power for Means</span>"
    ]
  },
  {
    "objectID": "22-samplesize.html#power-for-independent-sample-t-tests-with-unbalanced-designs",
    "href": "22-samplesize.html#power-for-independent-sample-t-tests-with-unbalanced-designs",
    "title": "22  Sample Size and Power for Means",
    "section": "\n22.8 Power for Independent Sample T tests with Unbalanced Designs",
    "text": "22.8 Power for Independent Sample T tests with Unbalanced Designs\nUsing the pwr package, R can do sample size calculations that describe the power of a two-sample t test that does not require a balanced design using the pwr.t2n.test command.\nSuppose we wanted to do the same study as we described above, using 100 “treated” patients but as few “placebo” patients as possible. What sample size would be required to maintain 90% power? There is one change here – the effect size d in the pwr.t2n.test command is specified using the difference in means \\(\\delta\\) that we used previously, divided by the standard deviation s that we used previously. So, in our old setup, we assumed delta = 4.5, sd = 9, so now we’ll assume d = 4.5/9 instead.\n\npwr.t2n.test(n1 = 100, d = 4.5/9, \n             sig.level = 0.1, power = 0.9,\n             alternative=\"two.sided\")\n\n\n     t test power calculation \n\n             n1 = 100\n             n2 = 52.82433\n              d = 0.5\n      sig.level = 0.1\n          power = 0.9\n    alternative = two.sided\n\n\nWe would need at least 53 subjects in the “placebo” group.\n\n22.8.1 The most efficient design for an independent samples comparison will be balanced.\n\nNote that if we use \\(n_1\\) = 100 subjects in the treated group, we need at least \\(n_2\\) = 53 in the placebo group to achieve 90% power, and a total of 153 subjects.\nCompare this to the balanced design, where we needed 70 subjects in each group to achieve the same power, thus, a total of 140 subjects.\n\nWe saw earlier that a test with 60 subjects in each group would yield just under 86% power. Suppose we instead built a test with 80 subjects in the treated group, and 40 in the placebo group, then what would our power be?\n\npwr.t2n.test(n1 = 80, n2 = 40, d = 4.5/9, \n             sig.level = 0.10,\n             alternative=\"two.sided\")\n\n\n     t test power calculation \n\n             n1 = 80\n             n2 = 40\n              d = 0.5\n      sig.level = 0.1\n          power = 0.821823\n    alternative = two.sided\n\n\nAs we’d expect, the power is stronger for a balanced design than for an unbalanced design with the same overall sample size.\nNote that I used a two-sided test to establish my power calculation – in general, this is the most conservative and defensible approach for any such calculation, unless there is a strong and specific reason to use a one-sided approach in building a power calculation, don’t.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Sample Size and Power for Means</span>"
    ]
  },
  {
    "objectID": "23-examples_2means.html",
    "href": "23-examples_2means.html",
    "title": "\n23  Two Examples Comparing Means\n",
    "section": "",
    "text": "23.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nsource(\"data/Love-boost.R\")\nlibrary(kableExtra)\nlibrary(patchwork)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\nIn addition to the Love-boost.R script, we will also use the favstats function from the mosaic package.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Two Examples Comparing Means</span>"
    ]
  },
  {
    "objectID": "23-examples_2means.html#a-study-of-battery-life",
    "href": "23-examples_2means.html#a-study-of-battery-life",
    "title": "\n23  Two Examples Comparing Means\n",
    "section": "\n23.2 A Study of Battery Life",
    "text": "23.2 A Study of Battery Life\nShould you buy generic rather than brand-name batteries? Bock, Velleman, and De Veaux (2004) descibe a designed experiment to test battery life. A (male) student obtained six pairs of AA alkaline batteries from two major battery manufacturers; a well-known brand name and a generic brand, so that battery brand was the factor of interest.\nTo estimate the difference in mean lifetimes across the two manufacturers, the student kept a battery-powered CD player with the same CD running continuously, with the volume control fixed at 5, and measured the time until no more music was heard through the headphones. (He ran an initial trial to find out approximately how long that would take, so he didn’t have to spend the first 3 hours of each run listening to the same CD.) The outcome was the time in minutes until the sound stopped. To account for changes in the CD player’s performance over time, he randomized the run order by choosing pairs of batteries (the CD-player required two batteries to run) at random.\nHere are the results for the 6 brand name and 6 generic tests, in minutes, found in the battery.csv data file, where run indicates the order in which the tests were run…\n\nbattery &lt;- read_csv(\"data/battery.csv\",\n                    show_col_types = FALSE)\n\nbattery\n\n# A tibble: 12 × 4\n     run  test type        time\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1     1     1 brand name  191.\n 2     2     2 brand name  206.\n 3     6     3 brand name  199.\n 4     8     4 brand name  172.\n 5     9     5 brand name  184 \n 6    12     6 brand name  170.\n 7     3     1 generic     194 \n 8     4     2 generic     204.\n 9     5     3 generic     204.\n10     7     4 generic     206.\n11    10     5 generic     222.\n12    11     6 generic     209.\n\n\n\n23.2.1 Question 1. What is the outcome under study?\nWe are studying battery lifetimes (time until the sound stopped) in minutes.\n\n23.2.2 Question 2. What are the treatment/exposure groups?\nWe are comparing the two brands of batteries: the well-known vs. the generic.\n\n23.2.3 Question 3. Are the data collected using paired or independent samples?\nOf course, if we had different numbers of samples in the two groups, then we’d know without further thought that independent samples were required. Since we have 6 observations in the brand name group, and also have 6 observations in the generic group, i.e. a balanced design, we need to pause now to decide whether paired or independent samples testing is appropriate in this setting.\nTwo samples are paired if each data point in one sample is naturally linked to a specific data point in the other sample. So, do we have paired or independent samples?\n\nDespite the way I’ve set up the data table, there is no particular reason to pair, say, run #1 (a brand name run) with any particular experimental run in the generic group. So the samples are independent. This is not a matched-pairs design.\nIn each trial, the student either used two of the well-known batteries, or two of the generic batteries.\nAny of the tests/confidence intervals for the independent samples methods suggests a statistically significant (at the 5% level) difference between the generic and brand name batteries.\n\n23.2.4 Question 4. Are the data a random sample from the population of interest?\nProbably not. The data are likely to come from a convenient sample of batteries. I don’t know how this might bias the study, though. It seems unlikely that there would be a particular bias unless, for example, the well-known batteries were substantially older or younger than the generic.\n\n23.2.5 Question 5. What significance level will we use?\nWe have no reason not to use a 95% confidence level.\n\n23.2.6 Question 6. Are we using a one-sided or two-sided comparison?\nWe could argue for a one-sided comparison, but I’ll be safe and use the two-sided version.\n\n23.2.7 Question 9. What does the distribution of outcomes in each group tell us?\n\nggplot(battery, aes(x = type, y = time, fill = type)) +\n  geom_jitter(alpha = 0.75, width = 0.125) +\n  geom_boxplot(alpha = 0.5) +\n  stat_summary(fun = \"mean\", geom = \"point\", \n               shape = 23, size = 3, fill = \"white\") +\n  coord_flip() +\n  guides(fill = \"none\", col = \"none\") +\n  labs(title = \"Battery Running Time, by Manufacturer\",\n       y = \"Running Time (minutes)\", x = \"Manufacturer\")\n\n\n\n\n\n\n\nWe can generate histograms, too, but that’s an issue, because we have so few observations.\n\nggplot(battery, aes(x = time, fill = type)) +\n  geom_histogram(bins = 6, col = \"white\") +\n  facet_wrap(~ type) +\n  guides(fill = \"none\") + \n  labs(title = \"Battery Running Time, by Manufacturer\")\n\n\n\n\n\n\n\n\nmosaic::favstats(time ~ type, data = battery) |&gt;\n  kbl(digits = 2) |&gt; kable_styling()\n\n\n\ntype\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\nbrand name\n169.5\n175.3\n187.35\n197.07\n205.5\n186.88\n14.37\n6\n0\n\n\ngeneric\n194.0\n203.5\n205.00\n208.68\n222.5\n206.57\n9.37\n6\n0\n\n\n\n\n\nIt sure looks like the generic batteries lasted longer. And they also look like they were more consistent. The sample means are 206.6 for the generic group, 186.9 minutes for brand name, so the point estimate of the difference is 19.7 minutes.\nThe question is: can we be confident that the difference we observe here is more than just random fluctuation, at a 5% significance level?\n\n23.2.8 Inferential Results for the Battery Study\nIn the table below, I have summarized the two-sided testing results for most of the ways in which we have looked at a two sample comparison so far, with 95% confidence intervals. If the samples really are paired, then we must choose from the paired samples comparisons described in the table. If the samples really are independent, then we must choose from the independent samples comparisons.\n\n23.2.9 Paired Samples Approaches\n\n\nMethod\n\np Value\n95% CI for Generic - Brand Name\n\n\n\nPaired t\n0.058\n-1.0, 40.4\n\n\nWilcoxon signed rank\n0.063\n-2.0, 39.9\n\n\nBootstrap via smean.cl.boot\n\n–\n6.7, 33.0\n\n\n\n23.2.10 Independent Samples Approaches\n\n\nMethod\n\np Value\n95% CI for Generic - Brand Name\n\n\n\nPooled t\n0.018\n4.1, 35.3\n\n\nWelch’s t\n0.021\n3.7, 35.6\n\n\nWilcoxon Mann Whitney rank sum\n0.030\n3.3, 37.0\n\n\nBootstrap via bootdif\n\n–\n7.7, 32.2",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Two Examples Comparing Means</span>"
    ]
  },
  {
    "objectID": "23-examples_2means.html#the-breakfast-study-does-oat-bran-cereal-lower-serum-ldl-cholesterol",
    "href": "23-examples_2means.html#the-breakfast-study-does-oat-bran-cereal-lower-serum-ldl-cholesterol",
    "title": "\n23  Two Examples Comparing Means\n",
    "section": "\n23.3 The Breakfast Study: Does Oat Bran Cereal Lower Serum LDL Cholesterol?",
    "text": "23.3 The Breakfast Study: Does Oat Bran Cereal Lower Serum LDL Cholesterol?\nNorman and Streiner (2014) describe a crossover study that was conducted to investigate whether oat bran cereal helps to lower serum cholesterol levels in hypercholesterolemic males. Fourteen such individuals were randomly placed on a diet that included either oat bran or corn flakes; after two weeks, their low-density lipoprotein (LDL) cholesterol levels, in mmol/l were recorded. Each subject was then switched to the alternative diet. After a second two-week period, the LDL cholesterol level of each subject was again recorded.\n\nbreakfast &lt;- read_csv(\"data/breakfast.csv\",\n                    show_col_types = FALSE)\n\nbreakfast\n\n# A tibble: 14 × 3\n   subject cornflakes oatbran\n     &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1       1       4.61    3.84\n 2       2       6.42    5.57\n 3       3       5.4     5.85\n 4       4       4.54    4.8 \n 5       5       3.98    3.68\n 6       6       3.82    2.96\n 7       7       5.01    4.41\n 8       8       4.34    3.72\n 9       9       3.8     3.49\n10      10       4.56    3.84\n11      11       5.35    5.26\n12      12       3.89    3.73\n13      13       2.25    1.84\n14      14       4.24    4.14\n\n\n\n23.3.1 Question 1. What is the outcome under study?\nWe are studying levels of LDL cholesterol, in mmol/l. Note that if we wanted to convert to a more familiar scale, specifically mg/dl, we would multiply the mmol/l by 18, as it turns out.\n\n23.3.2 Question 2. What are the treatment/exposure groups?\nWe are comparing subjects after two weeks of eating corn flakes to the same subjects after two weeks of eating oat bran.\n\n23.3.3 Question 3. Are the data collected using paired or independent samples?\nThese are matched pairs, paired by subject. Each subject produced an oat bran result and a corn flakes result.\n\n23.3.4 Question 4. Are the data a random sample from the population of interest?\nProbably not. The data are likely to come from a convenient sample of 14 individuals but they were randomly assigned to cornflakes first or to oat bran first, then crossed over.\n\n23.3.5 Question 5. What significance level will we use?\nWe have no reason not to use our usual 95% confidence level, so alpha = 0.05\n\n23.3.6 Question 6. Are we using a one-sided or two-sided comparison?\nWe could argue for a one-sided comparison, but I’ll be safe and use the two-sided version.\n\n23.3.7 Question 7. Did pairing help reduce nuisance variation?\nAfter we drop the breakfast.csv file into the breakfast data frame, we look at the correlation of cornflakes and oatbran results across our 14 subjects.\n\ncor(breakfast$cornflakes, breakfast$oatbran)\n\n[1] 0.9233247\n\n\nThe sample Pearson correlation coefficient is very strong and positive at 0.92, so the paired samples approach will use these data far more effectively than the (incorrect) independent samples approach.\n\n23.3.8 Question 8. What does the distribution of paired differences tell us?\nWe summarize the distribution of the paired differences (cornflakes - oatbran) below.\n\nbreakfast &lt;- breakfast |&gt;\n  mutate(diffs = cornflakes - oatbran)\n\np1 &lt;- ggplot(breakfast, aes(sample = diffs)) +\n  geom_qq(col = \"darkslategray\") + geom_qq_line(col = \"navy\") + \n  theme(aspect.ratio = 1) + \n  labs(title = \"Normal Q-Q plot\")\n\np2 &lt;- ggplot(breakfast, aes(x = diffs)) +\n  geom_histogram(aes(y = stat(density)), \n                 bins = 5, fill = \"darkslategray\", col = \"white\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(breakfast$diffs), \n                            sd = sd(breakfast$diffs)),\n                col = \"navy\", lwd = 1.5) +\n  labs(title = \"Histogram with Normal Density\")\n\np3 &lt;- ggplot(breakfast, aes(x = diffs, y = \"\")) +\n  geom_boxplot(fill = \"darkslategray\", outlier.color = \"darkslategray\") + \n  stat_summary(fun = \"mean\", geom = \"point\", \n               shape = 23, size = 3, fill = \"white\") +\n  labs(title = \"Boxplot\", y = \"\")\n\np1 + (p2 / p3 + plot_layout(heights = c(4,1))) + \n    plot_annotation(title = \"Difference in LDL (Corn Flakes - Oat Bran)\")\n\nWarning: `stat(density)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\nThe Normal distribution doesn’t look too ridiculous in this case for the paired (cornflakes-oatbran) differences. Suppose we assume Normality and run the paired t test.\n\nt.test(breakfast$cornflakes - breakfast$oatbran)\n\n\n    One Sample t-test\n\ndata:  breakfast$cornflakes - breakfast$oatbran\nt = 3.3444, df = 13, p-value = 0.005278\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.1284606 0.5972537\nsample estimates:\nmean of x \n0.3628571 \n\n\nBased on this sample of 14 subjects in the crossover study, we observe a 95% confidence interval for the difference between the LDL cholesterol levels after eating corn flakes and eating oat bran that is entirely positive, suggesting that LDL levels were detectably higher (according to this t test procedure) after eating corn flakes than after eating oat bran.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Two Examples Comparing Means</span>"
    ]
  },
  {
    "objectID": "23-examples_2means.html#power-sample-size-and-the-breakfast-study",
    "href": "23-examples_2means.html#power-sample-size-and-the-breakfast-study",
    "title": "\n23  Two Examples Comparing Means\n",
    "section": "\n23.4 Power, Sample Size and the Breakfast Study",
    "text": "23.4 Power, Sample Size and the Breakfast Study\nAs a preview of what’s next to come, let’s investigate these promising results a bit further. Suppose that in a new study, you wish to be able to detect a difference in LDL cholesterol between two exposures: subjects who eat cornflakes (as in the original study) and subjects who continue to eat cornflakes but also take a supplemental dosage of what you believe to be the crucial ingredient in oatbran.\nSuppose you believe that the effect of taking the new supplement will be about half the size of the effect you observed in the original breakfast study on hypercholesterolemic males, but that males generally may be more likely to take your supplement regularly than switch from cornflakes to a less appetizing breakfast choice, making your supplement attractive.\nWhat sample size will be required to yield 90% power to detect an effect half the size of the effect we observed in the breakfast study, in a new paired samples study using a two-tailed 5% significance level? What if we only required 80% power?\n\n23.4.1 The Setup\nWe want to know n, the minimum required sample size for the new study, and we have:\n\nA specified effect size of half of what we saw in the breakfast study, where the sample mean difference between cornflakes and oatbran was 0.36 mmol/l, so our effect size is assumed to be delta = 0.18 mmol/l.\nAn assumed standard deviation equal to the standard deviation of the differences in the pilot breakfast study, which turns out to have been s = 0.41 mmol/l.\nWe also have a pre-specified alpha = 0.05 using a two-tailed test.\nWe also want the power to be at least 90% for our new study.\n\n23.4.2 The R Calculations\nQuestion 1. What sample size will be required to yield 90% power to detect an effect half the size of the effect we observed in the breakfast study, in a new paired samples study using a two-tailed 5% significance level?\n\npower.t.test(delta = 0.18, sd = 0.41, sig.level = 0.05, \n             power = 0.9, type=\"paired\", alternative=\"two.sided\")\n\n\n     Paired t test power calculation \n\n              n = 56.47119\n          delta = 0.18\n             sd = 0.41\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs\n\n\nAnd so our new study will require at least 57 subjects (each measured in two circumstances, so 114 total measurements) in order to achieve at least 90% power to detect the difference of 0.18 mmol/l while meeting these specifications.\nQuestion 2. What if we were willing to accept only 80% power?\n\npower.t.test(delta = 0.18, sd = 0.41, sig.level = 0.05, \n             power = 0.8, type=\"paired\", alternative=\"two.sided\")\n\n\n     Paired t test power calculation \n\n              n = 42.68269\n          delta = 0.18\n             sd = 0.41\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs\n\n\nIt turns out that this would require at least 43 subjects.\n\n23.4.3 Independent samples, instead of paired samples?\nWhat would happen if, instead of doing a paired samples study, we did one using independent samples? Assuming we used a balanced design, and assigned the same number of different people at random to either the oatbran supplement or regular cornflakes alone, we could do such a study, but it would require many more people to obtain similar power to the paired samples study.\n\npower.t.test(delta = 0.18, sd = 0.41, sig.level = 0.05, \n             power = 0.9, type=\"two.sample\", alternative=\"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 110\n          delta = 0.18\n             sd = 0.41\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nIn all, 220 people would be required in the independent samples study (110 in each exposure group), as compared to only 57 people (each measured twice) in the paired study.\n\n\n\n\nBock, David E., Paul F. Velleman, and Richard D. De Veaux. 2004. Stats: Modelling the World. Boston MA: Pearson Addison-Wesley.\n\n\nNorman, Geoffrey R., and David L. Streiner. 2014. Biostatistics: The Bare Essentials. Fourth. People’s Medical Publishing House.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Two Examples Comparing Means</span>"
    ]
  },
  {
    "objectID": "24-anova.html",
    "href": "24-anova.html",
    "title": "\n24  Analysis of Variance\n",
    "section": "",
    "text": "24.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(ggridges)\nlibrary(janitor)\nlibrary(mosaic)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "24-anova.html#national-youth-fitness-survey",
    "href": "24-anova.html#national-youth-fitness-survey",
    "title": "\n24  Analysis of Variance\n",
    "section": "\n24.2 National Youth Fitness Survey",
    "text": "24.2 National Youth Fitness Survey\nRecall the National Youth Fitness Survey, which we explored a small piece of in Chapter 10. We’ll look at a different part of the same survey here - specifically the 280 children whose data are captured in the nyfs2 file.\n\nnyfs2 &lt;- read_csv(\"data/nyfs2.csv\", show_col_types = FALSE) |&gt;\n  clean_names()\n\nnyfs2\n\n# A tibble: 280 × 21\n   subject_id sex    age_exam race_eth         english income_cat3 income_detail\n        &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;        \n 1      73228 Male          4 5 Other or Mult…       1 Low (below… 0 to 4999    \n 2      72393 Male          4 2 Non-Hispanic …       1 Low (below… 0 to 4999    \n 3      73303 Male          3 2 Non-Hispanic …       1 Low (below… 0 to 4999    \n 4      72786 Male          5 1 Non-Hispanic …       1 Low (below… 0 to 4999    \n 5      73048 Male          3 2 Non-Hispanic …       1 Low (below… 0 to 4999    \n 6      72556 Female        4 2 Non-Hispanic …       1 Low (below… 0 to 4999    \n 7      72580 Female        5 2 Non-Hispanic …       1 Low (below… 0 to 4999    \n 8      72532 Female        4 4 Other Hispanic       0 Low (below… 0 to 4999    \n 9      73012 Male          4 1 Non-Hispanic …       1 Low (below… 0 to 4999    \n10      72099 Male          6 1 Non-Hispanic …       1 Low (below… 0 to 4999    \n# ℹ 270 more rows\n# ℹ 14 more variables: inc_to_pov &lt;dbl&gt;, weight_kg &lt;dbl&gt;, height_cm &lt;dbl&gt;,\n#   bmi &lt;dbl&gt;, bmi_group &lt;dbl&gt;, bmi_cat &lt;chr&gt;, arm_length &lt;dbl&gt;,\n#   arm_circ &lt;dbl&gt;, waist_circ &lt;dbl&gt;, calf_circ &lt;dbl&gt;, calf_skinfold &lt;dbl&gt;,\n#   triceps_skinfold &lt;dbl&gt;, subscap_skinfold &lt;dbl&gt;, gmq &lt;dbl&gt;",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "24-anova.html#comparing-gross-motor-quotient-scores-by-income-level-3-categories",
    "href": "24-anova.html#comparing-gross-motor-quotient-scores-by-income-level-3-categories",
    "title": "\n24  Analysis of Variance\n",
    "section": "\n24.3 Comparing Gross Motor Quotient Scores by Income Level (3 Categories)",
    "text": "24.3 Comparing Gross Motor Quotient Scores by Income Level (3 Categories)\n\nnyfs2a &lt;- nyfs2 |&gt;\n    select(subject_id, income_cat3, gmq) |&gt;\n    arrange(subject_id)\n\nIn this first analysis, we’ll compare the population mean on the Gross Motor Quotient evaluation of these kids across three groups defined by income level. Higher values of this GMQ measure indicate improved levels of gross motor development, both in terms of locomotor and object control. See https://wwwn.cdc.gov/Nchs/Nnyfs/Y_GMX.htm for more details.\n\nnyfs2a |&gt;\n    group_by(income_cat3) |&gt;\n    summarise(n = n(), mean(gmq), median(gmq))\n\n# A tibble: 3 × 4\n  income_cat3            n `mean(gmq)` `median(gmq)`\n  &lt;chr&gt;              &lt;int&gt;       &lt;dbl&gt;         &lt;dbl&gt;\n1 High (65K or more)    92        95.7            97\n2 Low (below 25K)       98        97.0            97\n3 Middle (25 - 64K)     90        95.4            94\n\n\nUh, oh. We should rearrange those income categories to match a natural order from low to high.\n\nnyfs2a &lt;- nyfs2a |&gt; \n  mutate(income_cat3 = fct_relevel(income_cat3, \n         \"Low (below 25K)\", \"Middle (25 - 64K)\", \"High (65K or more)\"))\n\nWhen working with three independent samples, I use graphs analogous to those we built for two independent samples.\n\nggplot(nyfs2a, aes(x = income_cat3, y = gmq, fill = income_cat3)) +\n  geom_violin(aes(col = income_cat3), alpha = 0.5) +\n  geom_boxplot(notch = TRUE, alpha = 0.75, width = 0.3) +\n  stat_summary(fun = \"mean\", geom = \"point\", \n               shape = 23, size = 3, fill = \"white\") +\n  coord_flip() +\n  guides(fill = \"none\", col = \"none\") +\n  labs(title = \"GMQ Scores for 280 Children in NNYFS\",\n       y = \"GMQ Score, in points\", x = \"Income Category\")\n\n\n\n\n\n\n\nIn addition to this comparison boxplot, we might consider faceted histograms.\n\nggplot(nyfs2a, aes(x = gmq, fill = income_cat3)) +\n  geom_histogram(bins = 15, col = \"white\") +\n  guides(fill = \"none\") +\n  facet_wrap(~ income_cat3)\n\n\n\n\n\n\n\nOr, if we want to ignore the (modest) sample size differences, we might consider density functions, perhaps through a ridgeline plot.\n\nggplot(nyfs2a, aes(x = gmq, y = income_cat3, fill = income_cat3)) +\n    geom_density_ridges(scale = 0.9) +\n    guides(fill = \"none\") + \n    labs(title = \"GMQ Score (in points) by Income Group\",\n         x = \"GMQ Score\", y = \"\") +\n    theme_ridges()\n\nPicking joint bandwidth of 4.7\n\n\n\n\n\n\n\n\n\nby(nyfs2a$gmq, nyfs2a$income_cat3, favstats)\n\nnyfs2a$income_cat3: Low (below 25K)\n min Q1 median  Q3 max     mean       sd  n missing\n  55 91     97 106 130 97.03061 14.79444 98       0\n------------------------------------------------------------ \nnyfs2a$income_cat3: Middle (25 - 64K)\n min Q1 median  Q3 max     mean       sd  n missing\n  67 85     94 106 136 95.36667 14.15123 90       0\n------------------------------------------------------------ \nnyfs2a$income_cat3: High (65K or more)\n min Q1 median  Q3 max     mean       sd  n missing\n  64 85     97 103 145 95.72826 14.49525 92       0",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "24-anova.html#alternative-procedures-for-comparing-more-than-two-means",
    "href": "24-anova.html#alternative-procedures-for-comparing-more-than-two-means",
    "title": "\n24  Analysis of Variance\n",
    "section": "\n24.4 Alternative Procedures for Comparing More Than Two Means",
    "text": "24.4 Alternative Procedures for Comparing More Than Two Means\nNow, if we only had two independent samples, we’d be choosing between a pooled t test, a Welch t test, and a non-parametric procedure like the Wilcoxon-Mann-Whitney rank sum test, or even perhaps a bootstrap alternative.\nIn the case of more than two independent samples, we have methods analogous to the Welch test, and the rank sum test, and even the bootstrap, but we’re going to be far more likely to select the analysis of variance (ANOVA) or an equivalent regression-based approach. These are the extensions of the pooled t test. Unless the sample outcome data are very clearly not Normally distributed, and no transformation is available which makes them appear approximately Normal in all of the groups we are comparing, we will stick with ANOVA.\n\n24.4.1 Extending the Welch Test to &gt; 2 Independent Samples\nIt is possible to extend the Welch two-sample t test (not assuming equal population variances) into an analogous one-factor analysis for comparing population means based on independent samples from more than two groups.\nIf we want to compare the population mean GMQ levels across those three income groups without assuming equal population variances, oneway.test is up to the task. The hypotheses being tested here are:\n\nH0: All three means are the same vs.\nHA: At least one of the population means is different than the others.\n\n\noneway.test(gmq ~ income_cat3, data = nyfs2a)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  gmq and income_cat3\nF = 0.3416, num df = 2.00, denom df = 184.41, p-value = 0.7111\n\n\nWe get a p value, but this isn’t much help, though, because we don’t have any measure of effect size, nor do we have any confidence intervals. Like the analogous Welch t test, this approach allows us to forego the assumption of equal population variances in each of the three income groups, but it still requires us to assume that the populations are Normally distributed.\nThat said, most of the time when we have more than two levels of the factor of interest, we won’t bother worrying about the equal population variance assumption, and will just use the one-factor ANOVA approach (with pooled variances) described below, to make the comparisons of interest.\n\n24.4.2 Extending the Rank Sum Test to &gt; 2 Independent Samples\nIt is also possible to extend the Wilcoxon-Mann-Whitney two-sample test into an analogous one-factor analysis called the Kruskal-Wallis test for comparing population measures of location based on independent samples from more than two groups.\nIf we want to compare the centers of the distributions of population GMQ score across our three income groups without assuming Normality, we can use kruskal.test.\nThe hypotheses being tested here are still as before, but for a measure of location other than the population mean\n\nkruskal.test(gmq ~ income_cat3, data = nyfs2a)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  gmq by income_cat3\nKruskal-Wallis chi-squared = 2.3202, df = 2, p-value = 0.3135\n\n\nAgain, note that this isn’t much help, though, because we don’t have any measure of effect size, nor do we have any confidence intervals.\nThat said, most of the time when we have more than two levels of the factor of interest, we won’t bother worrying about potential violations of the Normality assumption unless they are glaring, and will just use the usual one-factor ANOVA approach (with pooled variances) described below, to make the comparisons of interest.\n\n24.4.3 Can we use the bootstrap to compare more than two means?\nSure. There are both ANOVA and ANCOVA analogues using the bootstrap, and in fact, there are power calculations based on the bootstrap, too. If you want to see some example code, look at https://sammancuso.com/2017/11/01/model-based-bootstrapped-anova-and-ancova/",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "24-anova.html#the-analysis-of-variance",
    "href": "24-anova.html#the-analysis-of-variance",
    "title": "\n24  Analysis of Variance\n",
    "section": "\n24.5 The Analysis of Variance",
    "text": "24.5 The Analysis of Variance\nExtending the two-sample t test (assuming equal population variances) into a comparison of more than two samples uses the analysis of variance or ANOVA.\nThis is an analysis of a continuous outcome variable on the basis of a single categorical factor, in fact, it’s often called one-factor ANOVA or one-way ANOVA to indicate that the outcome is being split up into the groups defined by a single factor.\nThe null hypothesis is that the population means are all the same, and the alternative is that this is not the case. When there are just two groups, then this boils down to an F test that is equivalent to the Pooled t test.\n\n24.5.1 The oneway.test approach\nR will produce some elements of a one-factor ANOVA using the oneway.test command:\n\noneway.test(gmq ~ income_cat3, data = nyfs2a, var.equal=TRUE)\n\n\n    One-way analysis of means\n\ndata:  gmq and income_cat3\nF = 0.34687, num df = 2, denom df = 277, p-value = 0.7072\n\n\nThis isn’t the full analysis, though, which would require a more complete ANOVA table. There are two equivalent approaches to obtaining the full ANOVA table when comparing a series of 2 or more population means based on independent samples.\n\n24.5.2 Using the aov approach and the summary function\nHere’s one possible ANOVA table, which doesn’t require directly fitting a linear model.\n\nsummary(aov(gmq ~ income_cat3, data = nyfs2a))\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)\nincome_cat3   2    146   72.85   0.347  0.707\nResiduals   277  58174  210.01               \n\n\n\n24.5.3 Using the anova function after fitting a linear model\nAn equivalent way to get identical results in a slightly different format runs the linear model behind the ANOVA approach directly.\n\nanova(lm(gmq ~ income_cat3, data = nyfs2a))\n\nAnalysis of Variance Table\n\nResponse: gmq\n             Df Sum Sq Mean Sq F value Pr(&gt;F)\nincome_cat3   2    146  72.848  0.3469 0.7072\nResiduals   277  58174 210.014",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "24-anova.html#interpreting-the-anova-table",
    "href": "24-anova.html#interpreting-the-anova-table",
    "title": "\n24  Analysis of Variance\n",
    "section": "\n24.6 Interpreting the ANOVA Table",
    "text": "24.6 Interpreting the ANOVA Table\n\n24.6.1 What are we Testing?\nThe null hypothesis for the ANOVA table is that the population means of the outcome across the various levels of the factor of interest are all the same, against a two-sided alternative hypothesis that the level-specific population means are not all the same.\nSpecifically, if we have a grouping factor with k levels, then we are testing:\n\nH0: All k population means are the same.\nHA: At least one of the population means is different from the others.\n\n24.6.2 Elements of the ANOVA Table\nThe ANOVA table breaks down the variation in the outcome explained by the k levels of the factor of interest, and the variation in the outcome which remains (the Residual, or Error).\nSpecifically, the elements of the ANOVA table are:\n\nthe degrees of freedom (labeled Df) for the factor of interest and for the Residuals\nthe sums of squares (labeled Sum Sq) for the factor of interest and for the Residuals\nthe mean square (labeled Mean Sq) for the factor of interest and for the Residuals\nthe ANOVA F test statistic (labeled F value), which is used to generate\nthe p value for the comparison assessed by the ANOVA model, labeled Pr(&gt;F)\n\n24.6.3 The Degrees of Freedom\n\nanova(lm(gmq ~ income_cat3, data = nyfs2a))\n\nAnalysis of Variance Table\n\nResponse: gmq\n             Df Sum Sq Mean Sq F value Pr(&gt;F)\nincome_cat3   2    146  72.848  0.3469 0.7072\nResiduals   277  58174 210.014               \n\n\n\nThe degrees of freedom attributable to the factor of interest (here, Income category) is the number of levels of the factor minus 1. Here, we have three Income categories (levels), so df(income_cat3) = 2.\nThe total degrees of freedom are the number of observations (across all levels of the factor) minus 1. We have 280 GMQ scores in the nyfs2a data, so the df(Total) must be 279, although the Total row isn’t shown by R in its output.\nThe Residual degrees of freedom are the Total df - Factor df. So, here, that’s 279 - 2 = 277.\n\n24.6.4 The Sums of Squares\n\nanova(lm(gmq ~ income_cat3, data = nyfs2a))\n\nAnalysis of Variance Table\n\nResponse: gmq\n             Df Sum Sq Mean Sq F value Pr(&gt;F)\nincome_cat3   2    146  72.848  0.3469 0.7072\nResiduals   277  58174 210.014               \n\n\n\nThe sum of squares (often abbreviated SS or Sum Sq) represents variation explained.\nThe factor SS is the sum across all levels of the factor of the sample size for the level multiplied by the squared difference between the level mean and the overall mean across all levels. Here, SS(income_cat3) = 146\nThe total SS is the sum across all observations of the square of the difference between the individual values and the overall mean. Here, that is 146 + 58174 = 58320\nResidual SS = Total SS - Factor SS.\nAlso of interest is a calculation called \\(\\eta^2\\), (“eta-squared”), which is equivalent to \\(R^2\\) in a linear model.\n\nSS(Factor) / SS(Total) = the proportion of variation in our outcome (here, GMQ) explained by the variation between groups (here, income groups)\nIn our case, \\(\\eta^2\\) = 146 / (146 + 58174) = 146 / 58320 = 0.0025\nSo, Income Category alone accounts for about 0.25% of the variation in GMQ levels observed in these data.\n\n\n\n24.6.5 The Mean Square\n\nanova(lm(gmq ~ income_cat3, data = nyfs2a))\n\nAnalysis of Variance Table\n\nResponse: gmq\n             Df Sum Sq Mean Sq F value Pr(&gt;F)\nincome_cat3   2    146  72.848  0.3469 0.7072\nResiduals   277  58174 210.014               \n\n\n\nThe Mean Square is the Sum of Squares divided by the degrees of freedom, so MS(Factor) = SS(Factor)/df(Factor).\nIn our case, MS(income_cat3) = SS(income_cat3)/df(income_cat3) = 146 / 2 = 72.848 (notice that R maintains more decimal places than it shows for these calculations) and\nMS(Residuals) = SS(Residuals) / df(Residuals) = 58174 / 277 = 210.014.\n\nMS(Residuals) or MS(Error) is an estimate of the residual variance which corresponds to \\(\\sigma^2\\) in the underlying linear model for the outcome of interest, here GMQ.\n\n\n\n24.6.6 The F Test Statistic and p Value\n\nanova(lm(gmq ~ income_cat3, data = nyfs2a))\n\nAnalysis of Variance Table\n\nResponse: gmq\n             Df Sum Sq Mean Sq F value Pr(&gt;F)\nincome_cat3   2    146  72.848  0.3469 0.7072\nResiduals   277  58174 210.014               \n\n\n\nThe ANOVA F test is obtained by calculating MS(Factor) / MS(Residuals). So in our case, F = 72.848 / 210.014 = 0.3469\nThe F test statistic is then compared to a specific F distribution to obtain a p value, which is shown here to be 0.7072\nSpecifically, the observed F test statistic is compared to an F distribution with numerator df = Factor df, and denominator df = Residual df to obtain the p value.\n\nHere, we have SS(Factor) = 146 (approximately), and df(Factor) = 2, leaving MS(Factor) = 72.848\nWe have SS(Residual) = 58174, and df(Residual) = 277, leaving MS(Residual) = 210.014\nMS(Factor) / MS(Residual) = F value = 0.3469, which, when compared to an F distribution with 2 and 277 degrees of freedom, yields a p value of 0.7072",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "24-anova.html#the-residual-standard-error",
    "href": "24-anova.html#the-residual-standard-error",
    "title": "\n24  Analysis of Variance\n",
    "section": "\n24.7 The Residual Standard Error",
    "text": "24.7 The Residual Standard Error\nThe residual standard error is simply the square root of the variance estimate MS(Residual). Here, MS(Residual) = 210.014, so the Residual standard error = 14.49 points.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "24-anova.html#the-proportion-of-variance-explained-by-the-factor",
    "href": "24-anova.html#the-proportion-of-variance-explained-by-the-factor",
    "title": "\n24  Analysis of Variance\n",
    "section": "\n24.8 The Proportion of Variance Explained by the Factor",
    "text": "24.8 The Proportion of Variance Explained by the Factor\nWe will often summarize the proportion of the variation explained by the factor. The summary statistic is called eta-squared (\\(\\eta^2\\)), and is equivalent to the \\(R^2\\) value we have seen previously in linear regression models.\nAgain, \\(\\eta^2\\) = SS(Factor) / SS(Total)\nHere, we have - SS(income_cat3) = 146 and SS(Residuals) = 58174, so SS(Total) = 58320 - Thus, \\(\\eta^2\\) = SS(Factor)/SS(Total) = 146/58320 = 0.0025\nThe income category accounts for 0.25% of the variation in GMQ levels: only a tiny fraction.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "24-anova.html#the-regression-approach-to-compare-population-means-based-on-independent-samples",
    "href": "24-anova.html#the-regression-approach-to-compare-population-means-based-on-independent-samples",
    "title": "\n24  Analysis of Variance\n",
    "section": "\n24.9 The Regression Approach to Compare Population Means based on Independent Samples",
    "text": "24.9 The Regression Approach to Compare Population Means based on Independent Samples\nThis approach is equivalent to the ANOVA approach, and thus also (when there are just two samples to compare) to the pooled-variance t test. We run a linear regression model to predict the outcome (here, GMQ) on the basis of the categorical factor with three levels (here, income_cat3)\n\nsummary(lm(gmq ~ income_cat3, data=nyfs2a))\n\n\nCall:\nlm(formula = gmq ~ income_cat3, data = nyfs2a)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-42.031  -9.031  -0.031   8.969  49.272 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     97.031      1.464  66.282   &lt;2e-16 ***\nincome_cat3Middle (25 - 64K)    -1.664      2.116  -0.786    0.432    \nincome_cat3High (65K or more)   -1.302      2.104  -0.619    0.536    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.49 on 277 degrees of freedom\nMultiple R-squared:  0.002498,  Adjusted R-squared:  -0.004704 \nF-statistic: 0.3469 on 2 and 277 DF,  p-value: 0.7072\n\n\n\n24.9.1 Interpreting the Regression Output\nThis output tells us many things, but for now, we’ll focus just on the coefficients output, which tells us that:\n\nthe point estimate for the population mean GMQ score across “Low” income subjects is 97.03\nthe point estimate (sample mean difference) for the difference in population mean GMQ level between the “Middle” and “Low” income subjects is -1.66 (in words, the Middle income kids have lower GMQ scores than the Low income kids by 1.66 points on average.)\nthe point estimate (sample mean difference) for the difference in population mean GMQ level between the “High” and “Low” income subjects is -1.30 (in words, the High income kids have lower GMQ scores than the Low income kids by 1.30 points on average.)\n\nOf course, we knew all of this already from a summary of the sample means.\n\nnyfs2a |&gt;\n    group_by(income_cat3) |&gt;\n    summarise(n = n(), mean(gmq))\n\n# A tibble: 3 × 3\n  income_cat3            n `mean(gmq)`\n  &lt;fct&gt;              &lt;int&gt;       &lt;dbl&gt;\n1 Low (below 25K)       98        97.0\n2 Middle (25 - 64K)     90        95.4\n3 High (65K or more)    92        95.7\n\n\nThe model for predicting GMQ is based on two binary (1/0) indicator variables, specifically, we have:\n\nEstimated GMQ = 97.03 - 1.66 x [1 if Middle income or 0 if not] - 1.30 x [1 if High income or 0 if not]\n\nThe coefficients section also provides a standard error and t statistic and two-sided p value for each coefficient.\n\n24.9.2 The Full ANOVA Table\nTo see the full ANOVA table corresponding to any linear regression model, we run…\n\nanova(lm(gmq ~ income_cat3, data=nyfs2a))\n\nAnalysis of Variance Table\n\nResponse: gmq\n             Df Sum Sq Mean Sq F value Pr(&gt;F)\nincome_cat3   2    146  72.848  0.3469 0.7072\nResiduals   277  58174 210.014               \n\n\n\n24.9.3 ANOVA Assumptions\nThe assumptions behind analysis of variance are the same as those behind a linear model. Of specific interest are:\n\nThe samples obtained from each group are independent.\nIdeally, the samples from each group are a random sample from the population described by that group.\nIn the population, the variance of the outcome in each group is equal. (This is less of an issue if our study involves a balanced design.)\nIn the population, we have Normal distributions of the outcome in each group.\n\nHappily, the F test is fairly robust to violations of the Normality assumption.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "24-anova.html#equivalent-approach-to-get-anova-results",
    "href": "24-anova.html#equivalent-approach-to-get-anova-results",
    "title": "\n24  Analysis of Variance\n",
    "section": "\n24.10 Equivalent approach to get ANOVA Results",
    "text": "24.10 Equivalent approach to get ANOVA Results\n\nsummary(aov(gmq ~ income_cat3, data = nyfs2a))\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)\nincome_cat3   2    146   72.85   0.347  0.707\nResiduals   277  58174  210.01               \n\n\nSo which of the pairs of means are driving the differences we see?",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "24-anova.html#the-problem-of-multiple-comparisons",
    "href": "24-anova.html#the-problem-of-multiple-comparisons",
    "title": "\n24  Analysis of Variance\n",
    "section": "\n24.11 The Problem of Multiple Comparisons",
    "text": "24.11 The Problem of Multiple Comparisons\n\nSuppose we compare High to Low, using a test with \\(\\alpha\\) = 0.05\nThen we compare Middle to Low on the same outcome, also using \\(\\alpha\\) = 0.05\nThen we compare High to Middle, also with \\(\\alpha\\) = 0.05\n\nWhat is our overall \\(\\alpha\\) level across these three comparisons?\n\nIt could be as bad as 0.05 + 0.05 + 0.05, or 0.15.\nRather than our nominal 95% confidence, we have something as low as 85% confidence across this set of simultaneous comparisons.\n\n\n24.11.1 The Bonferroni solution\n\nSuppose we compare High to Low, using a test with \\(\\alpha\\) = 0.05/3\nThen we compare Middle to Low on the same outcome, also using \\(\\alpha\\) = 0.05/3\nThen we compare High to Middle, also with \\(\\alpha\\) = 0.05/3\n\nThen across these three comparisons, our overall \\(\\alpha\\) can be (at worst)\n\n0.05/3 + 0.05/3 + 0.05/3 = 0.05\nSo by changing our nominal confidence level from 95% to 98.333% in each comparison, we wind up with at least 95% confidence across this set of simultaneous comparisons.\nThis is a conservative (worst case) approach.\n\nGoal: Simultaneous comparisons of White vs AA, AA vs Other and White vs Other\n\npairwise.t.test(nyfs2a$gmq, nyfs2a$income_cat3, p.adjust=\"bonferroni\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  nyfs2a$gmq and nyfs2a$income_cat3 \n\n                   Low (below 25K) Middle (25 - 64K)\nMiddle (25 - 64K)  1               -                \nHigh (65K or more) 1               1                \n\nP value adjustment method: bonferroni \n\n\nThese p values are very large.\n\n24.11.2 Pairwise Comparisons using Tukey’s HSD Method\nGoal: Simultaneous (less conservative) confidence intervals and p values for our three pairwise comparisons (High vs. Low, High vs. Middle, Middle vs. Low)\n\nTukeyHSD(aov(gmq ~ income_cat3, data = nyfs2a))\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = gmq ~ income_cat3, data = nyfs2a)\n\n$income_cat3\n                                           diff       lwr      upr     p adj\nMiddle (25 - 64K)-Low (below 25K)    -1.6639456 -6.649518 3.321627 0.7116745\nHigh (65K or more)-Low (below 25K)   -1.3023514 -6.259595 3.654892 0.8098084\nHigh (65K or more)-Middle (25 - 64K)  0.3615942 -4.701208 5.424396 0.9845073\n\n\n\n24.11.3 Plotting the Tukey HSD results\n\nplot(TukeyHSD(aov(gmq ~ income_cat3, data = nyfs2a)))\n\n\n\n\n\n\n\nNote that the default positioning of the y axis in the plot of Tukey HSD results can be problematic. If we have longer names, in particular, for the levels of our factor, R will leave out some of the labels. We can alleviate that problem either by using the fct_recode function in the forcats package to rename the factor levels, or we can use the following code to reconfigure the margins of the plot.\n\nmar.default &lt;- c(5,6,4,2) + 0.1 # save default plotting margins\n\npar(mar = mar.default + c(0, 12, 0, 0)) \nplot(TukeyHSD(aov(gmq ~ income_cat3, data = nyfs2a)), las = 2)\n\n\n\n\n\n\npar(mar = mar.default) # return to normal plotting margins",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "24-anova.html#what-if-we-consider-another-outcome-bmi",
    "href": "24-anova.html#what-if-we-consider-another-outcome-bmi",
    "title": "\n24  Analysis of Variance\n",
    "section": "\n24.12 What if we consider another outcome, BMI?",
    "text": "24.12 What if we consider another outcome, BMI?\nWe’ll look at the full data set in nyfs2 now, so we can look at BMI as a function of income.\n\nnyfs2$income_cat3 &lt;- \n    fct_relevel(nyfs2$income_cat3,\n                \"Low (below 25K)\", \"Middle (25 - 64K)\", \"High (65K or more)\")\n\nggplot(nyfs2, aes(x = bmi, y = income_cat3, fill = income_cat3)) +\n    geom_density_ridges(scale = 0.9) +\n    guides(fill = \"none\") + \n    labs(title = \"Body-Mass Index by Income Group\",\n         x = \"Body-Mass Index\", y = \"\") +\n    theme_ridges()\n\n\n\n\n\n\n\n\nggplot(nyfs2, aes(x = income_cat3, y = bmi, fill = income_cat3)) +\n  geom_violin(aes(col = income_cat3), alpha = 0.5) +\n  geom_boxplot(width = 0.3, notch = TRUE, alpha = 0.75) +\n  stat_summary(fun = \"mean\", geom = \"point\", \n               shape = 23, size = 3, fill = \"white\") +\n  coord_flip() +\n  guides(fill = \"none\", col = \"none\") +\n  labs(title = \"BMI for 280 Children in NNYFS\",\n       y = \"Body-Mass Index\", x = \"Income Category\")\n\n\n\n\n\n\n\nHere are the descriptive numerical summaries:\n\nmosaic::favstats(bmi ~ income_cat3, data = nyfs2)\n\n         income_cat3  min     Q1 median   Q3  max     mean       sd  n missing\n1    Low (below 25K) 13.8 15.500  16.50 17.9 25.2 16.98163 2.194574 98       0\n2  Middle (25 - 64K) 13.7 15.225  16.05 16.9 24.1 16.37111 1.898920 90       0\n3 High (65K or more) 13.8 15.300  15.85 17.2 21.8 16.27065 1.614395 92       0\n\n\nHere is the ANOVA table.\n\nanova(lm(bmi ~ income_cat3, data = nyfs2))\n\nAnalysis of Variance Table\n\nResponse: bmi\n             Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \nincome_cat3   2   28.32 14.1583  3.8252 0.02298 *\nResiduals   277 1025.26  3.7013                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLet’s consider the Tukey HSD results. First, we’ll create a factor with shorter labels.\n\nnyfs2$inc.new &lt;- \n    fct_recode(nyfs2$income_cat3, \n               \"Low\" = \"Low (below 25K)\", \"Middle\" = \"Middle (25 - 64K)\",\n               \"High\" = \"High (65K or more)\")\n\nplot(TukeyHSD(aov(bmi ~ inc.new, data = nyfs2),\n                  conf.level = 0.90))\n\n\n\n\n\n\n\nIt appears that there is a detectable difference between the bmi means of the “Low” group and both the “High” and “Middle” group at the 90% confidence level, but no detectable difference between “Middle” and “High.” Details of those confidence intervals for those pairwise comparisons follow.\n\nTukeyHSD(aov(bmi ~ inc.new, data = nyfs2),\n                  conf.level = 0.90)\n\n  Tukey multiple comparisons of means\n    90% family-wise confidence level\n\nFit: aov(formula = bmi ~ inc.new, data = nyfs2)\n\n$inc.new\n                  diff        lwr         upr     p adj\nMiddle-Low  -0.6105215 -1.1893722 -0.03167084 0.0775491\nHigh-Low    -0.7109805 -1.2865420 -0.13541892 0.0306639\nHigh-Middle -0.1004589 -0.6882764  0.48735849 0.9339289",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "25-one_proportion.html",
    "href": "25-one_proportion.html",
    "title": "\n25  Estimating a Population Proportion\n",
    "section": "",
    "text": "25.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nsource(\"data/Love-boost.R\")\nlibrary(janitor)\nlibrary(kableExtra)\nlibrary(mosaic)\nlibrary(broom)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Estimating a Population Proportion</span>"
    ]
  },
  {
    "objectID": "25-one_proportion.html#a-first-example-serum-zinc-in-the-normal-range",
    "href": "25-one_proportion.html#a-first-example-serum-zinc-in-the-normal-range",
    "title": "\n25  Estimating a Population Proportion\n",
    "section": "\n25.2 A First Example: Serum Zinc in the “Normal” Range?",
    "text": "25.2 A First Example: Serum Zinc in the “Normal” Range?\nRecall that in the serum zinc study discussed in Chapter 19, we have 462 teenage male subjects, of whom 395 (or 85.5%) fell in the “normal range” of 66 to 110 micrograms per deciliter.\n\nserzinc &lt;- read_csv(\"data/serzinc.csv\", show_col_types = FALSE)\n\nserzinc &lt;- serzinc |&gt; \n  mutate(in_range = ifelse(zinc &gt;= 66 & zinc &lt;= 110, 1, 0))\n\nserzinc |&gt; tabyl(in_range) |&gt; \n  adorn_totals() |&gt; adorn_pct_formatting()\n\n in_range   n percent\n        0  67   14.5%\n        1 395   85.5%\n    Total 462  100.0%\n\n\nPreviously, we estimated a confidence interval for the mean of the population zinc levels. Now, we want to estimate a confidence interval for the proportion of the population whose serum zinc levels are in the range of 66 to 110. We want to build both a point estimate for the population proportion, and a confidence interval for the population proportion.\nNow, let’s identify a 95% confidence interval for the proportion of the population whose zinc levels are within the “normal” range. We have seen that 395 / 462 subjects (or a proportion of 0.855) fall in the “normal range” in our sample. For now, that will also be our point estimate of the proportion in the “normal range” across the entire population of teenagers like those in our sample.\n\nserzinc &lt;- serzinc |&gt;\n    mutate(in_range = ifelse(zinc &gt; 65 & zinc &lt; 111, 1, 0))\n\nserzinc |&gt; tabyl(in_range)\n\n in_range   n   percent\n        0  67 0.1450216\n        1 395 0.8549784\n\n\nOnce we’ve created this 0-1 variable, there are several available approaches for wrapping a confidence interval around this proportion.\n\n25.2.1 Using an Intercept-Only Regression Again?\nWe might consider taking the same approach as we did with the population mean earlier:\n\nmodel_zincprop &lt;- lm(in_range ~ 1, data = serzinc)\n\ntidy(model_zincprop, conf.int = TRUE, conf = 0.90) |&gt; \n    kbl(digits = 3) |&gt; kable_minimal()\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n(Intercept)\n0.855\n0.016\n52.133\n0\n0.828\n0.882\n\n\n\n\nWhile there are more powerful approaches to estimate a confidence interval around this proportion, this simple approach turns out not to be too bad, so long as the sample proportion isn’t very close to either 0 or 1.\n\n25.2.2 A 100(1-\\(\\alpha\\))% Confidence Interval for a Population Proportion\nSuppose we want to estimate a confidence interval for an unknown population proportion, \\(\\pi\\), on the basis of a random sample of n observations from that population which yields a sample proportion of p. Note that this p is the sample proportion – it’s not a p value.\n\nIn our serum zinc example, we have n = 462 observations, with a sample proportion (“in range”) of p = 0.855.\n\nA 100(1-\\(\\alpha\\))% confidence interval for the population proportion \\(\\pi\\) can be created by using the standard normal distribution, the sample proportion, p, and the standard error of a sample proportion, which is defined as the square root of p multiplied by (1-p) divided by the sample size, n. \n\nSo the standard error is estimated in our serum zinc example as:\n\n\\[\n\\sqrt{\\frac{p (1-p)}{n}} = \\sqrt{\\frac{0.855(1-0.855)}{462}} = \\sqrt{0.000268} = 0.016\n\\]\nAnd thus, our confidence interval is \\(p \\pm Z_{\\alpha/2} \\sqrt{\\frac{p(1-p)}{n}}\\)\nwhere \\(Z_{\\alpha/2}\\) = the value from a standard Normal distribution cutting off the top \\(\\alpha/2\\) of the distribution, obtained in R by substituting the desired \\(\\alpha/2\\) value into the following command: qnorm(alpha/2, lower.tail=FALSE).\nNote: This interval is reasonably accurate so long as np and n(1-p) are each at least 5.\n\nFor the serum zinc data, we have np = (462)(0.855) = 395 and n(1-p) = 462(1 - 0.855) = 67, so this should be ok.\nFor \\(\\alpha\\) = 0.05, we have \\(Z_{\\alpha/2}\\) = 1.96, approximately.\n\n\nqnorm(0.025, lower.tail = FALSE)\n\n[1] 1.959964\n\n\n\nThus, for the serum zinc estimate, this confidence interval would be:\n\n\\[\np \\pm Z_{\\alpha/2} \\sqrt{\\frac{p(1-p)}{n}} =\n\\frac{395}{462} \\pm 1.96 \\sqrt{\\frac{0.855(1-0.855)}{462}} = 0.855 \\pm 0.032\n\\]\nor (0.823, 0.887).",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Estimating a Population Proportion</span>"
    ]
  },
  {
    "objectID": "25-one_proportion.html#using-binom.test-from-the-mosaic-package",
    "href": "25-one_proportion.html#using-binom.test-from-the-mosaic-package",
    "title": "\n25  Estimating a Population Proportion\n",
    "section": "\n25.3 Using binom.test from the mosaic package",
    "text": "25.3 Using binom.test from the mosaic package\nI am aware of at least seven different procedures for estimating a confidence interval for a population proportion using R. All have minor weaknesses: none is importantly different from the others in many practical situations. Five of these methods are available using the binom.test function from the mosaic package in R.\nThe general format for using the binom.test function is as follows:\nbinom.test(x = 395,  # substitute in number of successes\n           n = 462,  # substitute in number of trials\n           conf.level = 0.95, # default confidence level\n           p = 0.5,  # default null hypothesis proportion\n           ci.method = \"XXX\") # see below for XXX options\nwhere the appropriate ci.method is obtained from the table below.\n\n\nApproach\n\nci.method to be used\n\n\n\nWald\n“Wald”\n\n\nClopper-Pearson\n“Clopper-Pearson” or “binom.test”\n\n\nScore\n“Score” or “prop.test”\n\n\nAgresti-Coull\n“agresti-coull”\n\n\nPlus4\n“plus4”\n\n\n\n\n25.3.1 The Wald test approach\nThe Wald approach can be used to establish a very similar confidence interval to the one we calculated above, based on something called the Wald test.\nHere, we specify the x and n values. n is the total number of observations, and x is the number where the event of interest (in this case, serum zinc levels in the normal range) occurs. So x = 395 and n = 462.\n\nm_wald &lt;- binom.test(x = 395, n = 462,\n                     conf.level = 0.95,\n                     ci.method = \"Wald\")\nm_wald\n\n\n    Exact binomial test (Wald CI)\n\ndata:  395 out of 462\nnumber of successes = 395, number of trials = 462, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.8228698 0.8870869\nsample estimates:\nprobability of success \n             0.8549784 \n\n\nThe Wald confidence interval is always symmetric around our point estimate, and can dip below 0 or above 1.\nWhen I fit intervals using the approaches in mosaic::binom_test() I will usually tidy them.\n\ntidy(m_wald) |&gt; \n  select(estimate, conf.low, conf.high, statistic, parameter) |&gt;\n  kbl(digits = 3) |&gt; kable_classic(full_width = F)\n\n\n\nestimate\nconf.low\nconf.high\nstatistic\nparameter\n\n\n0.855\n0.823\n0.887\n395\n462\n\n\n\n\nThe other elements of the tidied result are shown below.\n\ntidy(m_wald) |&gt;\n  select(method, alternative, p.value) |&gt;\n  kbl(digits = 3) |&gt; kable_classic(full_width = F)\n\n\n\nmethod\nalternative\np.value\n\n\nExact binomial test (Wald CI)\ntwo.sided\n0\n\n\n\n\n\n25.3.2 The Clopper-Pearson approach\nThe binom.test command can be used to establish an “exact” confidence interval. This uses the method of Clopper and Pearson from 1934, and is exact in the sense that it guarantees, for instance, that the confidence level associated with the interval is at least as large as the nominal level of 95%, but not that the interval isn’t wider than perhaps it needs to be.\n\nm_clopper &lt;- binom.test(x = 395, n = 462,\n                        conf.level = 0.95,\n                        ci.method = \"Clopper-Pearson\")\n\nm_clopper\n\n\n\n\ndata:  395 out of 462\nnumber of successes = 395, number of trials = 462, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.8195187 0.8858100\nsample estimates:\nprobability of success \n             0.8549784 \n\n\nClopper-Pearson is used by stats::binom.test() in R as well. Again, it guarantees coverage at least as large as the nominal coverage rate, but may produce wider intervals than the other methods we’ll see. The 95% confidence interval by this method is (0.820, 0.886), which is in the same general range as our previous estimates.\n\ntidy(m_clopper) |&gt; \n  select(estimate, conf.low, conf.high, statistic, parameter) |&gt;\n  kbl(digits = 3) |&gt; kable_classic(full_width = F)\n\n\n\nestimate\nconf.low\nconf.high\nstatistic\nparameter\n\n\n0.855\n0.82\n0.886\n395\n462\n\n\n\n\n\n25.3.3 The Score approach\n\nm_score &lt;- mosaic::binom.test(x = 395, n = 462,\n                              conf.level = 0.95,\n                              ci.method = \"Score\")\n\nm_score\n\n\n    Exact binomial test (Score CI without continuity correction)\n\ndata:  395 out of 462\nnumber of successes = 395, number of trials = 462, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.8199415 0.8841607\nsample estimates:\nprobability of success \n             0.8549784 \n\n\nThe Score approach is also used by stats::prop.test() and creates CIs by inverting p-values from score tests. It can be applied with a continuity correction (use ci.method = “prop.test”) or without.\nIn this case, we see that the Score approach and the Clopper-Pearson approach give very similar results.\n\ntidy(m_score) |&gt; \n  select(estimate, conf.low, conf.high, statistic, parameter) |&gt;\n  kbl(digits = 3) |&gt; kable_classic_2(full_width = F)\n\n\n\nestimate\nconf.low\nconf.high\nstatistic\nparameter\n\n\n0.855\n0.82\n0.884\n395\n462\n\n\n\n\nAs mentioned, the score test can also be run incorporating something called a continuity correction, since we are using a Normal approximation to the exact binomial distribution to establish our margin for error. R, by default, includes this continuity correction for the Score test when we use prop.test to collect it.\n\nm_score_cor &lt;- binom.test(x = 395, n = 462,\n                          conf.level = 0.95,\n                          ci.method = \"prop.test\")\n\nm_score_cor\n\n\n    Exact binomial test (Score CI with continuity correction)\n\ndata:  395 out of 462\nnumber of successes = 395, number of trials = 462, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.8187706 0.8851359\nsample estimates:\nprobability of success \n             0.8549784 \n\n\n\ntidy(m_score_cor) |&gt; \n  select(estimate, conf.low, conf.high, statistic, parameter) |&gt;\n  kbl(digits = 3) |&gt; kable_paper(full_width = F)\n\n\n\nestimate\nconf.low\nconf.high\nstatistic\nparameter\n\n\n0.855\n0.819\n0.885\n395\n462\n\n\n\n\n\n25.3.4 The Agresti-Coull Approach\n\nm_agresti &lt;- binom.test(x = 395, n = 462,\n                        conf.level = 0.95,\n                        ci.method = \"agresti-coull\")\n\nm_agresti\n\n\n    Exact binomial test (Agresti-Coull CI)\n\ndata:  395 out of 462\nnumber of successes = 395, number of trials = 462, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.8198094 0.8842928\nsample estimates:\nprobability of success \n             0.8549784 \n\n\nThe Agresti-Coull approach is the Wald method after adding Z successes and Z failures to the data, where Z is the appropriate quantile for a standard Normal distribution (1.96 for a 95% CI).\n\ntidy(m_agresti) |&gt; \n  select(estimate, conf.low, conf.high, statistic, parameter) |&gt;\n  kbl(digits = 3) |&gt; kable_paper(full_width = F)\n\n\n\nestimate\nconf.low\nconf.high\nstatistic\nparameter\n\n\n0.855\n0.82\n0.884\n395\n462\n\n\n\n\n\n25.3.5 The “Plus 4” approach\nThis approach is just the Wald method after adding 2 successes and 2 failures (so 4 observations) to the data. It will be very similar to the Agresti-Coull method if we are working with a 95% confidence interval.\n\nm_plus4 &lt;- binom.test(x = 395, n = 462,\n                      conf.level = 0.95,\n                      ci.method = \"plus4\")\n\nm_plus4\n\n\n    Exact binomial test (Plus 4 CI)\n\ndata:  395 out of 462\nnumber of successes = 395, number of trials = 462, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.8196844 0.8841783\nsample estimates:\nprobability of success \n             0.8549784 \n\n\n\ntidy(m_plus4) |&gt; \n  select(estimate, conf.low, conf.high, statistic, parameter) |&gt;\n  kbl(digits = 3) |&gt; kable_paper(full_width = F)\n\n\n\nestimate\nconf.low\nconf.high\nstatistic\nparameter\n\n\n0.855\n0.82\n0.884\n395\n462\n\n\n\n\n\n25.3.6 SAIFS: single augmentation with an imaginary failure or success\nSAIFS stands for “single augmentation with an imaginary failure or success” and the method I’ll describe is one of several similar approaches. The next subsection describes the R code for calculating the relevant confidence interval.\nAn approach I like for the estimation of a confidence interval for a single population proportion/rate is to estimate the lower bound of a confidence interval with an imaginary failure added to the observed data, and estimate the upper bound of a confidence interval with an imaginary success added to the data.\nSuppose we have X successes in n trials, and we want to establish a confidence interval for the population proportion of successes.\nLet \\(p_1 = (X+0)/(n+1), p_2 = (X+1)/(n+1), q_1 = 1 - p_1, q_2 = 1 - p_2\\)\n\nThe lower bound of a 100(1-\\(\\alpha\\))% confidence interval for the population proportion of successes using the SAIFS procedure is then \\(LB_{SAIFS}(x,n,\\alpha) = p_1 - t_{\\alpha/2, n-1}\\sqrt{\\frac{p_1 q_1}{n}}\\)\nThe upper bound of that same 100(1-\\(\\alpha\\))% confidence interval for the population proportion of successes using the SAIFS procedure is \\(UB_{SAIFS}(x,n,\\alpha) = p_2 + t_{\\alpha/2, n-1}\\sqrt{\\frac{p_2 q_2}{n}}\\)\n\nReturning to the serum zinc example, we’ve got 395 “successes” (value in the normal range) out of 462 “trials” (values measured), so that X = 395 and n = 462\nSo we have \\(p_1 = \\frac{X + 0}{n + 1} = \\frac{395}{463} = 0.8531\\), \\(p_2 = \\frac{X + 1}{n + 1} = \\frac{396}{463} = 0.8553\\), and \\(q_1 = 1 - p_1 = 0.1469\\) and \\(q_2 = 1 - p_2 = 0.1447\\)\nWe have \\(n = 462\\) so if we want a 95% confidence interval (\\(\\alpha = 0.05\\)), then we have \\(t_{\\alpha/2, n-1} = t_{0.025, 461} = 1.9651\\), which I determined using R’s qt function:\n\nqt(0.025, df = 461, lower.tail=FALSE)\n\n[1] 1.965123\n\n\n\nThus, our lower bound for a 95% confidence interval is \\(p_1 - t_{\\alpha/2, n-1}\\sqrt{\\frac{p_1 q_1}{n}}\\), or \\(0.8531 - 1.9651 \\sqrt{\\frac{0.8531(0.1469)}{462}}\\), which is 0.8531 - 0.0324 or 0.8207.\nOur upper bound is \\(p_2 + t_{\\alpha/2, n-1}\\sqrt{\\frac{p_2 q_2}{n}}\\), or \\(0.8553 - 1.9651 \\sqrt{\\frac{0.8553(0.1447)}{462}}\\), which is 0.8553 + 0.0323, or 0.8876.\n\nSo the 95% SAIFS confidence interval estimate for the population proportion, \\(\\pi\\), of teenage males whose serum zinc levels fall within the “normal range” is (0.821, 0.888).\n\n25.3.7 A Function in R to Calculate the SAIFS Confidence Interval\nI built an R function, called saifs.ci and contained in the Markdown for this document as well as the Love-boost.R script on the web site, which takes as its arguments a value for X = the number of successes, n = the number of trials, and conf.level = the confidence level, and produces the sample proportion, the SAIFS lower bound and upper bound for the specified two-sided confidence interval for the population proportion, using the equations above.\nHere, for instance, are 95%, 90% and 99% confidence intervals for the population proportion \\(\\pi\\) that we have been studying in the serum zinc data.\n\nsaifs.ci(x = 395, n = 462)\n\nSample Proportion             0.025             0.975 \n            0.855             0.821             0.887 \n\nsaifs.ci(x = 395, n = 462, conf=0.9)\n\nSample Proportion              0.05              0.95 \n            0.855             0.826             0.882 \n\nsaifs.ci(x = 395, n = 462, conf=0.99, dig=5)\n\nSample Proportion             0.005             0.995 \n          0.85498           0.81054           0.89763 \n\n\nNote that in the final interval, I asked the machine to round to five digits rather than the default of three. On my desktop (and probably yours), doing so results in this output:\nSample Proportion             0.005             0.995 \n          0.85498           0.81054           0.89763 \nI’ve got some setting wrong in my bookdown work so that this doesn’t show up above when the function is called. Sorry!\n\n25.3.8 The saifs.ci function in R\n\n`saifs.ci` &lt;- \n  function(x, n, conf.level=0.95, dig=3)\n  {\n    p.sample &lt;- round(x/n, digits=dig)\n    \n    p1 &lt;- x / (n+1)\n    p2 &lt;- (x+1) / (n+1)\n    \n    var1 &lt;- (p1*(1-p1))/n\n    se1 &lt;- sqrt(var1)\n    var2 &lt;- (p2*(1-p2))/n\n    se2 &lt;- sqrt(var2)\n    \n    lowq = (1 - conf.level)/2\n    tcut &lt;- qt(lowq, df=n-1, lower.tail=FALSE)\n    \n    lower.bound &lt;- round(p1 - tcut*se1, digits=dig)\n    upper.bound &lt;- round(p2 + tcut*se2, digits=dig)\n    res &lt;- c(p.sample, lower.bound, upper.bound)\n    names(res) &lt;- c('Sample Proportion',lowq, 1-lowq)\n    res\n  }",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Estimating a Population Proportion</span>"
    ]
  },
  {
    "objectID": "25-one_proportion.html#sec-ebola",
    "href": "25-one_proportion.html#sec-ebola",
    "title": "\n25  Estimating a Population Proportion\n",
    "section": "\n25.4 A Second Example: Ebola Mortality Rates through 9 Months of the Epidemic",
    "text": "25.4 A Second Example: Ebola Mortality Rates through 9 Months of the Epidemic\nThe World Health Organization’s Ebola Response Team published an article in the October 16, 2014 issue of the New England Journal of Medicine, which contained some data I will use in this example, focusing on materials from their Table 2.\nAs of September 14, 2014, a total of 4,507 confirmed and probable cases of Ebola virus disease (EVD) had been reported from West Africa. In our example, we will look at a set of 1,737 cases, with definitive outcomes, reported in Guinea, Liberia, Nigeria and Sierra Leone.\nAcross these 1,737 cases, a total of 1,229 cases led to death. Based on these sample data, what can be said about the case fatality rate in the population of EVD cases with definitive outcomes for this epidemic?\n\n25.4.1 Working through the Ebola Virus Disease Example\nWe have n = 1,737 subjects, of whom we observed death in 1,229, for a sample proportion of \\(p = \\frac{1229}{1737} = 0.708\\). The standard error of that sample proportion will be\n\\(SE(p) = \\sqrt{\\frac{p(1-p)}{n}} = \\sqrt{\\frac{0.708(1-0.708)}{1737}} = 0.011\\)\nAnd our 95% confidence interval (so that we’ll use \\(\\alpha\\) = 0.05) for the true population proportion, \\(\\pi\\), of EVD cases with definitive outcomes, who will die is \\(p \\pm Z_{.025} \\sqrt{\\frac{p(1-p)}{n}}\\), or 0.708 \\(\\pm\\) 1.96(0.011) = \\(0.708 \\pm 0.022\\), or (0.686, 0.730)\nNote that I simply recalled from our prior work that \\(Z_{0.025} = 1.96\\), but we can verify this:\n\nqnorm(0.025, lower.tail=FALSE)\n\n[1] 1.959964\n\n\nSince both np=(1737)(0.708)=1230 and n(1-p)=(1737)(1-0.708)=507 are substantially greater than 5, this should be a reasonably accurate confidence interval.\nWe have 95% confidence in an interval estimate for the true population proportion that falls between 0.686 and 0.730. Equivalently, we could say that we’re 95% confident that the true case fatality rate expressed as a percentage rather than a proportion, is between 68.6% and 73.0%.\n\n25.4.2 Using R to estimate the CI for our Ebola example\n\nebola_wald &lt;- binom.test(x = 1229, n = 1737, conf.level = 0.95,\n                                 ci.method = \"Wald\") |&gt;\n  tidy() |&gt; select(estimate, conf.low, conf.high) \nebola_clop &lt;- binom.test(x = 1229, n = 1737, conf.level = 0.95,\n                                 ci.method = \"Clopper-Pearson\") |&gt;\n  tidy() |&gt; select(estimate, conf.low, conf.high)\nebola_scor &lt;- binom.test(x = 1229, n = 1737, conf.level = 0.95,\n                                 ci.method = \"Score\") |&gt;\n  tidy() |&gt; select(estimate, conf.low, conf.high) \nebola_agco &lt;- binom.test(x = 1229, n = 1737, conf.level = 0.95,\n                                 ci.method = \"agresti-coull\") |&gt;\n  tidy() |&gt; select(estimate, conf.low, conf.high) \nebola_plus &lt;- binom.test(x = 1229, n = 1737, conf.level = 0.95,\n                                 ci.method = \"plus4\") |&gt;\n  tidy() |&gt; select(estimate, conf.low, conf.high) \n\nebola_res &lt;- bind_rows(ebola_wald, ebola_clop, ebola_scor, \n                       ebola_agco, ebola_plus) |&gt;\n  mutate(approach = c(\"Wald\", \"Clopper-Pearson\", \"Score\",\n                      \"Agresti-Coull\", \"Plus4\"))\n\nebola_res |&gt; kbl(digits = 6) |&gt; kable_classic(full_width = FALSE)\n\n\n\nestimate\nconf.low\nconf.high\napproach\n\n\n\n0.707542\n0.686150\n0.728934\nWald\n\n\n0.707542\n0.685524\n0.728856\nClopper-Pearson\n\n\n0.707542\n0.685710\n0.728457\nScore\n\n\n0.707542\n0.685705\n0.728462\nAgresti-Coull\n\n\n0.707542\n0.685687\n0.728443\nPlus4\n\n\n\n\n\nThis is way more precision than we can really justify, but I just want you to see that the five results are all (slightly) different.\n\n25.4.3 Plotting the Confidence Intervals for the Ebola Virus Disease Example\n\nggplot(ebola_res, aes(x = approach, y = estimate)) +\n    geom_point() +\n    geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +\n    coord_flip() +\n    labs(title = \"95% CIs for x = 1229, n = 1737\")\n\n\n\n\n\n\n\nSo in this case, it really doesn’t matter which one you choose. With a smaller sample, we may not come to the same conclusion about the relative merits of these different approaches.\n\n25.4.4 What about the saifs.ci() result?\n\nsaifs.ci(x = 1229, n = 1737, conf.level=0.95)\n\nSample Proportion             0.025             0.975 \n            0.708             0.686             0.729",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Estimating a Population Proportion</span>"
    ]
  },
  {
    "objectID": "25-one_proportion.html#can-the-choice-of-confidence-interval-method-matter",
    "href": "25-one_proportion.html#can-the-choice-of-confidence-interval-method-matter",
    "title": "\n25  Estimating a Population Proportion\n",
    "section": "\n25.5 Can the Choice of Confidence Interval Method Matter?",
    "text": "25.5 Can the Choice of Confidence Interval Method Matter?\nYes. This will especially be the case when we have a small sample size, and a probability of “success” that is close to either 0 or 1. For instance, suppose we run 10 trials, and obtain a single success, then use these data to estimate the true proportion of success, \\(\\pi\\).\nThe 90% confidence intervals under this circumstance are very different.\n\ntidy1 &lt;- binom.test(x = 1, n = 10, conf.level = 0.90,\n                            ci.method = \"Wald\") |&gt; tidy()\ntidy2 &lt;- binom.test(x = 1, n = 10, conf.level = 0.90, \n                            ci.method = \"Clopper-Pearson\") |&gt; tidy()\ntidy3 &lt;- binom.test(x = 1, n = 10, conf.level = 0.90, \n                            ci.method = \"Score\") |&gt; tidy()\ntidy4 &lt;- binom.test(x = 1, n = 10, conf.level = 0.90, \n                            ci.method = \"agresti-coull\") |&gt; tidy()\ntidy5 &lt;- binom.test(x = 1, n = 10, conf.level = 0.90, \n                            ci.method = \"plus4\") |&gt; tidy()\n\nres &lt;- bind_rows(tidy1, tidy2, tidy3, tidy4, tidy5) |&gt;\n  mutate(approach = c(\"Wald\", \"Clopper-Pearson\", \"Score\",\n                      \"Agresti-Coull\", \"Plus4\")) |&gt;\n  select(approach, estimate, conf.low, conf.high)\n\nres |&gt; kbl(digits = 3) |&gt; kable_paper(full_width = F)\n\n\n\napproach\nestimate\nconf.low\nconf.high\n\n\n\nWald\n0.1\n-0.056\n0.256\n\n\nClopper-Pearson\n0.1\n0.005\n0.394\n\n\nScore\n0.1\n0.023\n0.348\n\n\nAgresti-Coull\n0.1\n0.006\n0.364\n\n\nPlus4\n0.1\n0.034\n0.395\n\n\n\n\n\nNote that the Wald procedure doesn’t force the confidence interval to appear in the (0, 1) range.\n\nggplot(res, aes(x = approach, y = estimate)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + \n  geom_hline(aes(yintercept = 0), col = \"red\", lty = \"dashed\") +\n  coord_flip() +\n  labs(title = \"90% CIs for x = 1, n = 10\")\n\n\n\n\n\n\n\nNone of these three approaches is always better than any of the others. When we have a sample size below 100, or the sample proportion of success is either below 0.10 or above 0.90, caution is warranted, although in many cases, the various methods give similar responses.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Estimating a Population Proportion</span>"
    ]
  },
  {
    "objectID": "26-comparing_rates.html",
    "href": "26-comparing_rates.html",
    "title": "\n26  Comparing Proportions with Two Independent Samples\n",
    "section": "",
    "text": "26.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nsource(\"data/Love-boost.R\")\nlibrary(janitor)\nlibrary(mosaic)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Comparing Proportions with Two Independent Samples</span>"
    ]
  },
  {
    "objectID": "26-comparing_rates.html#a-first-example-ibuprofen-and-sepsis-trial",
    "href": "26-comparing_rates.html#a-first-example-ibuprofen-and-sepsis-trial",
    "title": "\n26  Comparing Proportions with Two Independent Samples\n",
    "section": "\n26.2 A First Example: Ibuprofen and Sepsis Trial",
    "text": "26.2 A First Example: Ibuprofen and Sepsis Trial\nAs we saw in Chapter 20, we are interested in comparing the percentage of patients in each arm of the trial (Ibuprofen vs. Placebo) that showed an improvement in their temperature (temp_drop &gt; 0). Our primary interest is in comparing the percentage of Ibuprofen patients whose temperature dropped to the percentage of Placebo patients whose temperature dropped.\n\nWe can summarize the data behind the two proportions we are comparing in a contingency table with two rows which identify the exposure or treatment of interest, and two columns to represent the outcomes of interest.\nIn this case, we are comparing two groups of subjects based on treatment (treat): those who received Ibuprofen and those who received a placebo. The outcome of interest is whether the subject’s temperature dropped (temp_drop &gt; 0), or not.\nIn the table, we place the frequency for each combination of a row and a column.\nThe rows need to be mutually exclusive and collectively exhaustive: each patient must either receive Ibuprofen or Placebo. Similarly, the columns must meet the same standard: every patient’s temperature either drops or does not drop.\n\nHere’s the contingency table.\n\nsepsis &lt;- read_csv(\"data/sepsis.csv\",\n                   show_col_types = FALSE) |&gt;\n    mutate(treat = factor(treat),\n           race = factor(race))\n\n\nsepsis &lt;- sepsis |&gt;\n    mutate(dropped = ifelse(temp_drop &gt; 0, \"Drop\", \"No Drop\"))\n\nsepsis |&gt; tabyl(treat, dropped) |&gt; \n    adorn_totals() |&gt;\n    adorn_percentages(denom = \"row\") |&gt;\n    adorn_pct_formatting(digits = 1) |&gt;\n    adorn_ns(position = \"front\")\n\n     treat        Drop     No Drop\n Ibuprofen 107 (71.3%)  43 (28.7%)\n   Placebo  80 (53.3%)  70 (46.7%)\n     Total 187 (62.3%) 113 (37.7%)\n\n\nIn our sample, we observe 71.3% of the 150 Ibuprofen subjects with a positive temp_drop as compared to 53.3% of the 150 Placebo subjects. We want to compare these two probabilities (represented using proportions as 0.713 vs. 0.533) to estimate the size of the difference between the proportions with a point estimate and 90% confidence interval.\nBut there are other comparisons we could make, too. The tricky part is that we have multiple ways to describe the relationship between treatment and outcome. We might compare outcome “risks” directly using the difference in probabilities, or the ratio of the two probabilities, or we might convert the risks to odds, and compare the ratio of those odds. In any case, we’ll get different point estimates and confidence intervals, all of which will help us make conclusions about the evidence available in this trial speaking to differences between Ibuprofen and Placebo.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Comparing Proportions with Two Independent Samples</span>"
    ]
  },
  {
    "objectID": "26-comparing_rates.html#relating-a-treatment-to-an-outcome",
    "href": "26-comparing_rates.html#relating-a-treatment-to-an-outcome",
    "title": "\n26  Comparing Proportions with Two Independent Samples\n",
    "section": "\n26.3 Relating a Treatment to an Outcome",
    "text": "26.3 Relating a Treatment to an Outcome\nThe question of interest is whether the percentage of subjects whose temperature dropped is different (and probably larger) in the subjects who received Ibuprofen than in those who received the Placebo.\n\n\n\n\n\n\n\n\n\nTreatment Arm\nDropped\nDid Not Drop\nTotal\nProportion who dropped\n\n\n\nIbuprofen\n107\n43\n150\n0.713\n\n\nPlacebo\n80\n70\n150\n0.533\n\n\n\nIn other words, what is the relationship between the treatment and the outcome?",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Comparing Proportions with Two Independent Samples</span>"
    ]
  },
  {
    "objectID": "26-comparing_rates.html#definitions-of-probability-and-odds",
    "href": "26-comparing_rates.html#definitions-of-probability-and-odds",
    "title": "\n26  Comparing Proportions with Two Independent Samples\n",
    "section": "\n26.4 Definitions of Probability and Odds",
    "text": "26.4 Definitions of Probability and Odds\n\nProportion = Probability = Risk of the trait = number with trait / total\nOdds of having the trait = (number with the trait / number without the trait) to 1\n\nIf p is the proportion of subjects with a trait, then the odds of having the trait are \\(\\frac{p}{1-p}\\) to 1.\nSo, the probability of a good result (temperature drop) in this case is \\(\\frac{107}{150} = 0.713\\) in the Ibuprofen group. The odds of a good result are thus \\(\\frac{0.713}{1 - 0.713} = 2.484\\) to 1 in the Ibuprofen group.\n\n\n\n\n\n\n\n\n\n\nTreatment Arm\nDropped\nDid Not Drop\nTotal\nPr(dropped)\nOdds(dropped)\n\n\n\nIbuprofen\n107\n43\n150\n0.713\n2.484\n\n\nPlacebo\n80\n70\n150\n0.533\n1.141",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Comparing Proportions with Two Independent Samples</span>"
    ]
  },
  {
    "objectID": "26-comparing_rates.html#defining-the-relative-risk",
    "href": "26-comparing_rates.html#defining-the-relative-risk",
    "title": "\n26  Comparing Proportions with Two Independent Samples\n",
    "section": "\n26.5 Defining the Relative Risk",
    "text": "26.5 Defining the Relative Risk\nAmong the Ibuprofen subjects, the risk of a good outcome (drop in temperature) is 71.3% or, stated as a proportion, 0.713. Among the Placebo subjects, the risk of a good outcome is 53.3% or, stated as a proportion, 0.533.\nOur “crude” estimate of the relative risk of a good outcome for Ibuprofen subjects as compared to Placebo subjects, is the ratio of these two risks, or 0.713/0.533 = 1.338\n\nThe fact that this relative risk is greater than 1 indicates that the probability of a good outcome is higher for Ibuprofen subjects than for Placebo subjects.\nA relative risk of 1 would indicate that the probability of a good outcome is the same for Ibuprofen subjects and for Placebo subjects.\nA relative risk less than 1 would indicate that the probability of a good outcome is lower for Ibuprofen subjects than for Placebo subjects.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Comparing Proportions with Two Independent Samples</span>"
    ]
  },
  {
    "objectID": "26-comparing_rates.html#defining-the-risk-difference",
    "href": "26-comparing_rates.html#defining-the-risk-difference",
    "title": "\n26  Comparing Proportions with Two Independent Samples\n",
    "section": "\n26.6 Defining the Risk Difference",
    "text": "26.6 Defining the Risk Difference\nOur “crude” estimate of the risk difference of a good outcome for Ibuprofen subjects as compared to Placebo subjects, is 0.713 - 0.533 = 0.180 or 18.0 percentage points.\n\nThe fact that this risk difference is greater than 0 indicates that the probability of a good outcome is higher for Ibuprofen subjects than for Placebo subjects.\nA risk difference of 0 would indicate that the probability of a good outcome is the same for Ibuprofen subjects and for Placebo subjects.\nA risk difference less than 0 would indicate that the probability of a good outcome is lower for Ibuprofen subjects than for Placebo subjects.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Comparing Proportions with Two Independent Samples</span>"
    ]
  },
  {
    "objectID": "26-comparing_rates.html#defining-the-odds-ratio-or-the-cross-product-ratio",
    "href": "26-comparing_rates.html#defining-the-odds-ratio-or-the-cross-product-ratio",
    "title": "\n26  Comparing Proportions with Two Independent Samples\n",
    "section": "\n26.7 Defining the Odds Ratio, or the Cross-Product Ratio",
    "text": "26.7 Defining the Odds Ratio, or the Cross-Product Ratio\nAmong the Ibuprofen subjects, the odds of a good outcome (temperature drop) are 2.484. Among the placebo subjects, the odds of a good outcome (temperature drop) are 1.141.\nSo our “crude” estimate of the odds ratio of a good outcome for Ibuprofen subjects as compared to placebo subjects, is 2.484 / 1.141 = 2.18\nAnother way to calculate this odds ratio is to calculate the cross-product ratio, which is equal to (a x d) / (b x c), for the 2 by 2 table with counts specified as shown:\n\n\nA Generic Table\nGood Outcome\nBad Outcome\n\n\n\nTreatment Group 1\na\nb\n\n\nTreatment Group 2\nc\nd\n\n\n\nSo, for our table, we have a = 107, b = 43, c = 80, and d = 70, so the cross-product ratio is \\(\\frac{107 x 70}{43 x 80} = \\frac{7490}{3440} = 2.18\\). As expected, this is the same as the “crude” odds ratio estimate.\n\nThe fact that this odds ratio risk is greater than 1 indicates that the odds of a good outcome are higher for Ibuprofen subjects than for Placebo subjects.\nAn odds ratio of 1 would indicate that the odds of a good outcome are the same for Ibuprofen subjects and for Placebo subjects.\nAn odds ratio less than 1 would indicate that the odds of a good outcome are lower for Ibuprofen subjects than for Placebo subjects.\n\nSo, we have several different ways to compare the outcomes across the treatments. Are these differences and ratios large enough to rule out chance?",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Comparing Proportions with Two Independent Samples</span>"
    ]
  },
  {
    "objectID": "26-comparing_rates.html#comparing-rates-in-a-2x2-table",
    "href": "26-comparing_rates.html#comparing-rates-in-a-2x2-table",
    "title": "\n26  Comparing Proportions with Two Independent Samples\n",
    "section": "\n26.8 Comparing Rates in a 2x2 Table",
    "text": "26.8 Comparing Rates in a 2x2 Table\nWhat is the relationship between the treatment (Ibuprofen vs. Placebo) and the outcome (drop in temperature) in the following two-by-two table?",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Comparing Proportions with Two Independent Samples</span>"
    ]
  },
  {
    "objectID": "26-comparing_rates.html#the-twobytwo-function-in-r",
    "href": "26-comparing_rates.html#the-twobytwo-function-in-r",
    "title": "\n26  Comparing Proportions with Two Independent Samples\n",
    "section": "\n26.9 The twobytwo function in R",
    "text": "26.9 The twobytwo function in R\nI built the twobytwo function in R (based on existing functions in the Epi library, which you need to have in your installed packages list in order for this to work) to do the work for this problem. All that is required is a single command, and a two-by-two table in standard epidemiological format (with the outcomes in the columns, and the treatments in the rows.)\n\n\nTreatment Arm\nDropped\nDid Not Drop\n\n\n\nIbuprofen\n107\n43\n\n\nPlacebo\n80\n70\n\n\n\nThe command just requires you to read off the cells of the table, followed by the labels for the two treatments, then the two outcomes, then a specification of the names of the rows (exposures) and columns (outcomes) from the table, and a specification of the confidence level you desire. We’ll use 90% here.\nThe resulting output follows.\n\ntwobytwo(107, 43, 80, 70, \n         \"Ibuprofen\", \"Placebo\", \"Dropped\", \"No Drop\",\n         conf.level = 0.90)\n\n2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : Dropped \nComparing : Ibuprofen vs. Placebo \n\n          Dropped No Drop    P(Dropped) 90% conf. interval\nIbuprofen     107      43        0.7133    0.6490   0.7701\nPlacebo        80      70        0.5333    0.4661   0.5993\n\n                                   90% conf. interval\n             Relative Risk: 1.3375    1.1492   1.5567\n         Sample Odds Ratio: 2.1773    1.4583   3.2509\nConditional MLE Odds Ratio: 2.1716    1.4177   3.3437\n    Probability difference: 0.1800    0.0881   0.2677\n\n             Exact P-value: 0.0019 \n        Asymptotic P-value: 0.0014 \n------------------------------------------------------\n\n\n\n26.9.1 Standard Epidemiological Format\nThis table is in standard epidemiological format, which means that:\n\nThe rows of the table describe the “treatment” (which we’ll take here to be treat). The more interesting (sometimes also the more common) “treatment” is placed in the top row. That’s Ibuprofen here.\nThe columns of the table describe the “outcome” (which we’ll take here to be whether the subject’s temperature dropped.) Typically, the more common “outcome” is placed to the left.\n\n26.9.2 Outcome Probabilities and Confidence Intervals Within the Treatment Groups\nThe twobytwo output starts with estimates of the probability (risk) of a “Dropped” outcome among subjects who fall into the two treatment groups (Ibuprofen or Placebo), along with 90% confidence intervals for each of these probabilities.\n2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : Dropped \nComparing : Ibuprofen vs. Placebo \n\n          Dropped No Drop    P(Dropped) 90% conf. interval\nIbuprofen     107      43        0.7133    0.6490   0.7701\nPlacebo        80      70        0.5333    0.4661   0.5993\nThe conditional probability of a temperature drop given that the subject is in the Ibuprofen group, is symbolized as Pr(Dropped | Ibuprofen) = 0.7133, and the 90% confidence interval around that proportion is (0.6490, 0.7701).\n\nNote that these two confidence intervals fail to overlap, and so we expect to see a fairly large difference in the estimated probability of a temperature drop when we compare Ibuprofen to Placebo.\n\n26.9.3 Relative Risk, Odds Ratio and Risk Difference, with Confidence Intervals\nThese elements are followed by estimates of the relative risk, odds ratio, and risk difference, each with associated 90% confidence intervals.\n                                   90% conf. interval\n             Relative Risk: 1.3375    1.1492   1.5567\n         Sample Odds Ratio: 2.1773    1.4583   3.2509\nConditional MLE Odds Ratio: 2.1716    1.4177   3.3437\n    Probability difference: 0.1800    0.0881   0.2677\n\nThe relative risk, or the ratio of P(Temperature Drop | Ibuprofen) to P(Temperature Drop | Placebo), is shown first. Note that the 90% confidence interval is entirely greater than 1.\nThe odds ratio is presented using two different definitions (the sample odds ratio is the cross-product ratio we mentioned earlier). Note that the 90% confidence interval using either approach is entirely greater than 1.\nThe probability (or risk) difference [P(Temperature Drop | Ibuprofen) - P(Temperature Drop | Placebo)] is presented last. Note that the 90% confidence interval is entirely greater than 0.\nNote carefully that if there had been no difference between Ibuprofen and Placebo, the relative risk and odds ratios would be 1, but the probability difference would be zero.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Comparing Proportions with Two Independent Samples</span>"
    ]
  },
  {
    "objectID": "26-comparing_rates.html#estimating-a-rate-more-accurately-use-x-2n-4-rather-than-xn",
    "href": "26-comparing_rates.html#estimating-a-rate-more-accurately-use-x-2n-4-rather-than-xn",
    "title": "\n26  Comparing Proportions with Two Independent Samples\n",
    "section": "\n26.10 Estimating a Rate More Accurately: Use (x + 2)/(n + 4) rather than x/n",
    "text": "26.10 Estimating a Rate More Accurately: Use (x + 2)/(n + 4) rather than x/n\nSuppose you have some data involving n independent tries, with x successes. A natural estimate of the “success rate” in the data is x / n. \nBut, strangely enough, it turns out this isn’t an entirely satisfying estimator. Alan Agresti provides substantial motivation for the (x + 2)/(n + 4) estimate as an alternative. This is sometimes called a Bayesian augmentation.\n\nThe big problem with x / n is that it estimates p = 0 or p = 1 when x = 0 or x = n. \nIt’s also tricky to compute confidence intervals at these extremes, since the usual standard error for a proportion, \\(\\sqrt{n p (1-p)}\\), gives zero, which isn’t quite right.\n(x + 2)/(n + 4) is much cleaner, especially when you build a confidence interval for the rate.\nThe only place where (x + 2)/(n + 4) will go wrong is if n is small and the true probability is very close to 0 or 1.\n\nFor example, if n = 10, and p is 1 in a million, then x will almost certainly be zero, and an estimate of 1/12 is much worse than the simple 0/10. However, how big a deal is this? If p might be 1 in a million, you’re not going to estimate it with a n = 10 experiment.\nApplying this method to our Ibuprofen and Sepsis Trial data, we would simply add two to each frequency in the main four cells in our 2x2 table.\nSo instead of using\n\ntwobytwo(107, 43, 80, 70, \n         \"Ibuprofen\", \"Placebo\", \"Dropped\", \"No Drop\",\n         conf.level = 0.90)\n\nthe Bayesian augmentation would encourage us to look at\n\ntwobytwo(109, 45, 82, 72, \n         \"Ibuprofen\", \"Placebo\", \"Dropped\", \"No Drop\",\n         conf.level = 0.90)\n\n2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : Dropped \nComparing : Ibuprofen vs. Placebo \n\n          Dropped No Drop    P(Dropped) 90% conf. interval\nIbuprofen     109      45        0.7078    0.6441   0.7643\nPlacebo        82      72        0.5325    0.4662   0.5977\n\n                                   90% conf. interval\n             Relative Risk: 1.3293    1.1434   1.5453\n         Sample Odds Ratio: 2.1268    1.4337   3.1550\nConditional MLE Odds Ratio: 2.1215    1.3950   3.2421\n    Probability difference: 0.1753    0.0845   0.2622\n\n             Exact P-value: 0.0022 \n        Asymptotic P-value: 0.0016 \n------------------------------------------------------\n\n\nAs you can see, the odds ratio and relative risk estimates are (a little) closer to 1, and the probability difference is also a little closer to 0. The Bayesian augmentation provides a slightly more conservative set of estimates of the impact of Ibuprofen as compared to Placebo.\nIt is likely that the augmented version is a more accurate estimate here, but the two estimates will be comparable, generally, so long as either (a) the sample size in each exposure group is more than, say, 30 subjects, and/or (b) the sample probability of the outcome is between 10% and 90% in each exposure group.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Comparing Proportions with Two Independent Samples</span>"
    ]
  },
  {
    "objectID": "26-comparing_rates.html#a-second-example-ebola-virus-disease-study-again",
    "href": "26-comparing_rates.html#a-second-example-ebola-virus-disease-study-again",
    "title": "\n26  Comparing Proportions with Two Independent Samples\n",
    "section": "\n26.11 A Second Example: Ebola Virus Disease Study, again",
    "text": "26.11 A Second Example: Ebola Virus Disease Study, again\nFor instance, recall the Ebola Virus Disease study from the New England Journal of Medicine that we described in Section 25.4. Suppose we want to compare the proportion of deaths among cases that had a definitive outcome who were hospitalized to the proportion of deaths among cases that had a definitive outcome who were not hospitalized.\nThe article suggests that of the 1,737 cases with a definitive outcome, there were 1,153 hospitalized cases. Across those 1,153 hospitalized cases, 741 people (64.3%) died, which means that across the remaining 584 non-hospitalized cases, 488 people (83.6%) died.\nHere is the initial contingency table, using only the numbers from the previous paragraph.\n\n\nInitial Ebola Table\nDeceased\nAlive\nTotal\n\n\n\nHospitalized\n741\n–\n1153\n\n\nNot Hospitalized\n488\n–\n584\n\n\nTotal\n\n\n1737\n\n\n\nNow, we can use arithmetic to complete the table, since the rows and the columns are each mutually exclusive and collectively exhaustive.\n\n\nEbola 2x2 Table\nDeceased\nAlive\nTotal\n\n\n\nHospitalized\n741\n412\n1153\n\n\nNot Hospitalized\n488\n96\n584\n\n\nTotal\n1229\n508\n1737\n\n\n\nWe want to compare the fatality risk (probability of being in the deceased column) for the population of people in the hospitalized row to the population of people in the not hospitalized row.\nWe can run these data through R, using the Bayesian augmentation (adding a death and a survival to the hospitalized and also to the not hospitalized groups.) We’ll use a 95% confidence level this time.\n\ntwobytwo(741+2, 412+2, 488+2, 96+2, \n         \"Hospitalized\", \"Not Hospitalized\", \"Deceased\", \"Alive\",\n         conf.level = 0.95)\n\n2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : Deceased \nComparing : Hospitalized vs. Not Hospitalized \n\n                 Deceased Alive    P(Deceased) 95% conf. interval\nHospitalized          743   414         0.6422    0.6141   0.6693\nNot Hospitalized      490    98         0.8333    0.8010   0.8613\n\n                                    95% conf. interval\n             Relative Risk:  0.7706    0.7285   0.8151\n         Sample Odds Ratio:  0.3589    0.2801   0.4599\nConditional MLE Odds Ratio:  0.3591    0.2772   0.4624\n    Probability difference: -0.1912   -0.2307  -0.1490\n\n             Exact P-value: 0.0000 \n        Asymptotic P-value: 0.0000 \n------------------------------------------------------\n\n\nI’ll leave it as an exercise for you to interpret these results and draw some conclusions.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Comparing Proportions with Two Independent Samples</span>"
    ]
  },
  {
    "objectID": "27-power_for_rates.html",
    "href": "27-power_for_rates.html",
    "title": "\n27  Power and Proportions\n",
    "section": "",
    "text": "27.1 Setup: Packages Used Here\nsource(\"data/Love-boost.R\")\n\nlibrary(pwr)\nWe’ll use the Love-boost.R script for the twobytwo function.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Power and Proportions</span>"
    ]
  },
  {
    "objectID": "27-power_for_rates.html#tuberculosis-prevalence-among-iv-drug-users",
    "href": "27-power_for_rates.html#tuberculosis-prevalence-among-iv-drug-users",
    "title": "\n27  Power and Proportions\n",
    "section": "\n27.2 Tuberculosis Prevalence Among IV Drug Users",
    "text": "27.2 Tuberculosis Prevalence Among IV Drug Users\nConsider a study to investigate factors affecting tuberculosis prevalence among intravenous drug users. The original data source is Graham NMH et al. (1992) Prevalence of Tuberculin Positivity and Skin Test Anergy in HIV-1-Seropositive and Seronegative Intravenous Drug Users. JAMA, 267, 369-373. Among 97 individuals who admit to sharing needles, 24 (24.7%) had a positive tuberculin skin test result; among 161 drug users who deny sharing needles, 28 (17.4%) had a positive test result. To start, we’ll test the null hypothesis that the proportions of intravenous drug users who have a positive tuberculin skin test result are identical for those who share needles and those who do not.\n\ntwobytwo(24, 73, 28, 133, \n         \"Sharing Needles\", \"Not Sharing\", \n         \"TB test+\", \"TB test-\")\n\n2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : TB test+ \nComparing : Sharing Needles vs. Not Sharing \n\n                TB test+ TB test-    P(TB test+) 95% conf. interval\nSharing Needles       24       73         0.2474    0.1717   0.3427\nNot Sharing           28      133         0.1739    0.1229   0.2404\n\n                                   95% conf. interval\n             Relative Risk: 1.4227    0.8772   2.3073\n         Sample Odds Ratio: 1.5616    0.8439   2.8898\nConditional MLE Odds Ratio: 1.5588    0.8014   3.0191\n    Probability difference: 0.0735   -0.0265   0.1807\n\n             Exact P-value: 0.1996 \n        Asymptotic P-value: 0.1557 \n------------------------------------------------------\n\n\nWhat conclusions should we draw?",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Power and Proportions</span>"
    ]
  },
  {
    "objectID": "27-power_for_rates.html#designing-a-new-tb-study",
    "href": "27-power_for_rates.html#designing-a-new-tb-study",
    "title": "\n27  Power and Proportions\n",
    "section": "\n27.3 Designing a New TB Study",
    "text": "27.3 Designing a New TB Study\nNow, suppose we wanted to design a new study with as many non-sharers as needle-sharers participating, and suppose that we wanted to detect any difference in the proportion of positive skin test results between the two groups that was identical to the data presented above or larger with at least 90% power, using a two-sided test and \\(\\alpha\\) = .05. What sample size would be required to accomplish these aims?",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Power and Proportions</span>"
    ]
  },
  {
    "objectID": "27-power_for_rates.html#using-power.prop.test-for-balanced-designs",
    "href": "27-power_for_rates.html#using-power.prop.test-for-balanced-designs",
    "title": "\n27  Power and Proportions\n",
    "section": "\n27.4 Using power.prop.test for Balanced Designs",
    "text": "27.4 Using power.prop.test for Balanced Designs\nOur constraints are that we want to find the sample size for a two-sample comparison of proportions using a balanced design, we will use \\(\\alpha\\) = .05, and power = .90, and that we estimate that the non-sharers will have a .174 proportion of positive tests, and we will try to detect a difference between this group and the needle sharers, who we estimate will have a proportion of .247, using a two-sided hypothesis test.\n\npower.prop.test(p1 = .174, p2  = .247, sig.level = 0.05, power = 0.90)\n\n\n     Two-sample comparison of proportions power calculation \n\n              n = 653.2876\n             p1 = 0.174\n             p2 = 0.247\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nSo, we’d need at least 654 non-sharing subjects, and 654 more who share needles to accomplish the aims of the study.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Power and Proportions</span>"
    ]
  },
  {
    "objectID": "27-power_for_rates.html#how-power.prop.test-works",
    "href": "27-power_for_rates.html#how-power.prop.test-works",
    "title": "\n27  Power and Proportions\n",
    "section": "\n27.5 How power.prop.test works",
    "text": "27.5 How power.prop.test works\npower.prop.test works much like the power.t.test we saw for means.\nAgain, we specify 4 of the following 5 elements of the comparison, and R calculates the fifth.\n\nThe sample size (interpreted as the # in each group, so half the total sample size)\nThe true probability in group 1\nThe true probability in group 2\nThe significance level (\\(\\alpha\\))\nThe power (1 - \\(\\beta\\))\n\nThe big weakness with the power.prop.test tool is that it doesn’t allow you to work with unbalanced designs.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Power and Proportions</span>"
    ]
  },
  {
    "objectID": "27-power_for_rates.html#a-revised-scenario",
    "href": "27-power_for_rates.html#a-revised-scenario",
    "title": "\n27  Power and Proportions\n",
    "section": "\n27.6 A Revised Scenario",
    "text": "27.6 A Revised Scenario\nSuppose we can get exactly 800 subjects in total (400 sharing and 400 non-sharing). How much power would we have to detect a difference in the proportion of positive skin test results between the two groups that was identical to the data presented above or larger, using a one-sided test, with \\(\\alpha\\) = .10?\n\npower.prop.test(n=400, p1=.174, p2=.247, sig.level = 0.10,\n                alternative=\"one.sided\")\n\n\n     Two-sample comparison of proportions power calculation \n\n              n = 400\n             p1 = 0.174\n             p2 = 0.247\n      sig.level = 0.1\n          power = 0.8954262\n    alternative = one.sided\n\nNOTE: n is number in *each* group\n\n\nWe would have just under 90% power to detect such an effect.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Power and Proportions</span>"
    ]
  },
  {
    "objectID": "27-power_for_rates.html#using-the-pwr-package-for-unbalanced-designs",
    "href": "27-power_for_rates.html#using-the-pwr-package-for-unbalanced-designs",
    "title": "\n27  Power and Proportions\n",
    "section": "\n27.7 Using the pwr package for Unbalanced Designs",
    "text": "27.7 Using the pwr package for Unbalanced Designs\nThe pwr.2p2n.test function in the pwr package can help assess the power of a test to determine a particular effect size using an unbalanced design, where \\(n_1\\) is not equal to \\(n_2\\).\nAs before, we specify four of the following five elements of the comparison, and R calculates the fifth.\n\n\nn1 = The sample size in group 1\n\nn2 = The sample size in group 2\n\nsig.level = The significance level (\\(\\alpha\\))\n\npower = The power (1 - \\(\\beta\\))\n\nh = the effect size h, which can be calculated separately in R based on the two proportions being compared: p1 and p2.\n\n\n27.7.1 Calculating the Effect Size h\n\nTo calculate the effect size for a given set of proportions, just use ES.h(p1, p2) which is available in the pwr package.\nFor instance, in our comparison, we have the following effect size.\n\nES.h(p1 = .174, p2 = .247)\n\n[1] -0.1796783",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Power and Proportions</span>"
    ]
  },
  {
    "objectID": "27-power_for_rates.html#using-pwr.2p2n.test-in-r",
    "href": "27-power_for_rates.html#using-pwr.2p2n.test-in-r",
    "title": "\n27  Power and Proportions\n",
    "section": "\n27.8 Using pwr.2p2n.test in R",
    "text": "27.8 Using pwr.2p2n.test in R\nSuppose we can have 700 samples in group 1 (the not sharing group) but only half that many in group 2 (the group of users who share needles). How much power would we have to detect this same difference (\\(p_1\\) = .174, \\(p_2\\) = .247) with a 5% significance level in a two-sided test?\n\npwr.2p2n.test(h = ES.h(p1 = .174, p2 = .247), \n              n1 = 700, n2 = 350,\n              sig.level = 0.05)\n\n\n     difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.1796783\n             n1 = 700\n             n2 = 350\n      sig.level = 0.05\n          power = 0.7836768\n    alternative = two.sided\n\nNOTE: different sample sizes\n\n\nNote that the headline for this output actually reads:\ndifference of proportion power calculation for binomial distribution \n(arcsine transformation) \nIt appears we will have about 78% power under these circumstances.\n\n27.8.1 Comparison to Balanced Design\nHow does this compare to the results with a balanced design using only 1000 drug users in total, so that we have 500 patients in each group?\n\npwr.2p2n.test(h = ES.h(p1 = .174, p2 = .247), \n              n1 = 500, n2 = 500, sig.level = 0.05)\n\n\n     difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.1796783\n             n1 = 500\n             n2 = 500\n      sig.level = 0.05\n          power = 0.8108416\n    alternative = two.sided\n\nNOTE: different sample sizes\n\n\nor we could instead have used…\n\npower.prop.test(p1 = .174, p2 = .247, \n                sig.level = 0.05, n = 500)\n\n\n     Two-sample comparison of proportions power calculation \n\n              n = 500\n             p1 = 0.174\n             p2 = 0.247\n      sig.level = 0.05\n          power = 0.8091808\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nNote that these two sample size estimation approaches are approximations, and use slightly different approaches, so it’s not surprising that the answers are similar, but not completely identical.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Power and Proportions</span>"
    ]
  },
  {
    "objectID": "28-contingency_tables.html",
    "href": "28-contingency_tables.html",
    "title": "\n28  Larger Contingency Tables\n",
    "section": "",
    "text": "28.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nsource(\"data/Love-boost.R\")\nlibrary(Epi)\nlibrary(janitor)\nlibrary(vcd)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Larger Contingency Tables</span>"
    ]
  },
  {
    "objectID": "28-contingency_tables.html#a-2x3-table-comparing-response-to-active-vs.-placebo",
    "href": "28-contingency_tables.html#a-2x3-table-comparing-response-to-active-vs.-placebo",
    "title": "\n28  Larger Contingency Tables\n",
    "section": "\n28.2 A 2x3 Table: Comparing Response to Active vs. Placebo",
    "text": "28.2 A 2x3 Table: Comparing Response to Active vs. Placebo\nThe table below, containing 2 rows and 3 columns of data (ignoring the marginal totals) specifies the number of patients who show complete, partial, or no response after treatment with either active medication or a placebo.\n\n\nGroup\nNone\nPartial\nComplete\n\n\n\nActive\n16\n26\n29\n\n\nPlacebo\n24\n26\n18\n\n\n\nIs there a statistically significant association here? That is to say, is there a statistically significant difference between the treatment groups in the distribution of responses?\n\n28.2.1 Getting the Table into R\nTo answer this, we’ll have to get the data from this contingency table into a matrix in R. Here’s one approach…\n\nT1 &lt;- matrix(c(16,26,29,24,26,18), ncol=3, nrow=2, byrow=TRUE)\nrownames(T1) &lt;- c(\"Active\", \"Placebo\")\ncolnames(T1) &lt;- c(\"None\", \"Partial\", \"Complete\")\n\nT1\n\n        None Partial Complete\nActive    16      26       29\nPlacebo   24      26       18\n\n\n\n28.2.2 Manipulating the Table’s presentation\nWe can add margins to the matrix to get a table including row and column totals.\n\naddmargins(T1)\n\n        None Partial Complete Sum\nActive    16      26       29  71\nPlacebo   24      26       18  68\nSum       40      52       47 139\n\n\nInstead of the counts, we can tabulate the proportion of all patients within each cell.\n\nprop.table(T1)\n\n             None   Partial  Complete\nActive  0.1151079 0.1870504 0.2086331\nPlacebo 0.1726619 0.1870504 0.1294964\n\n\nNow, to actually obtain a p value and perform the significance test with \\(H_0\\): rows and columns are independent vs. \\(H_A\\): rows and columns are associated, we simply run a Pearson chi-square test on T1 …\n\nchisq.test(T1)\n\n\n    Pearson's Chi-squared test\n\ndata:  T1\nX-squared = 4.1116, df = 2, p-value = 0.128\n\n\nThanks to a p-value of about 0.13 (using the Pearson chi-square test) our conclusion would be to retain the null hypothesis of independence in this setting.\nWe could have run a Fisher’s exact test, too, if we needed it.\n\nfisher.test(T1)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  T1\np-value = 0.1346\nalternative hypothesis: two.sided\n\n\nThe Fisher exact test p value is also 0.13. Either way, there is insufficient evidence to conclude that there is a (true) difference in the distributions of responses.",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Larger Contingency Tables</span>"
    ]
  },
  {
    "objectID": "28-contingency_tables.html#accuracy-of-death-certificates-a-6x3-table",
    "href": "28-contingency_tables.html#accuracy-of-death-certificates-a-6x3-table",
    "title": "\n28  Larger Contingency Tables\n",
    "section": "\n28.3 Accuracy of Death Certificates (A 6x3 Table)",
    "text": "28.3 Accuracy of Death Certificates (A 6x3 Table)\nThe table below compiles data from six studies designed to investigate the accuracy of death certificates. The original citation is Kircher T, Nelson J, Burdo H (1985) The autopsy as a measure of accuracy of the death certificate. NEJM, 313, 1263-1269. 5373 autopsies were compared to the causes of death listed on the certificates. Of those, 3726 were confirmed to be accurate, 783 either lacked information or contained inaccuracies but did not require recoding of the underlying cause of death, and 864 were incorrect and required recoding. Do the results across studies appear consistent?\n\n\n\n\n\n\n\n\n\nDate of Study\n[Confirmed] Accurate\n[Inaccurate] No Change\n[Incorrect] Recoding\nTotal\n\n\n\n1955-1965\n2040\n367\n327\n2734\n\n\n1970\n149\n60\n48\n257\n\n\n1970-1971\n288\n25\n70\n383\n\n\n1975-1977\n703\n197\n252\n1152\n\n\n1977-1978\n425\n62\n88\n575\n\n\n1980\n121\n72\n79\n272\n\n\nTotal\n3726\n783\n864\n5373",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Larger Contingency Tables</span>"
    ]
  },
  {
    "objectID": "28-contingency_tables.html#the-pearson-chi-square-test-of-independence",
    "href": "28-contingency_tables.html#the-pearson-chi-square-test-of-independence",
    "title": "\n28  Larger Contingency Tables\n",
    "section": "\n28.4 The Pearson Chi-Square Test of Independence",
    "text": "28.4 The Pearson Chi-Square Test of Independence\nWe can assess the homogeneity of the confirmation results (columns) we observe in the table using a Pearson chi-squared test of independence.\n\nThe null hypothesis is that the rows and columns are independent.\nThe alternative hypothesis is that there is an association between the rows and the columns.\n\n\ndeath_table &lt;- matrix(c(2040,367,327,149,60,48,288,25,70,703,\n                      197,252,425,62,88,121,72,79), byrow=TRUE, nrow=6)\nrownames(death_table) &lt;- c(\"1955-65\", \"1970\", \"1970-71\", \"1975-77\", \"1977-78\",\n                 \"1980\")\ncolnames(death_table) &lt;- c(\"Confirmed\", \"Inaccurate\", \"Incorrect\")\n \naddmargins(death_table)\n\n        Confirmed Inaccurate Incorrect  Sum\n1955-65      2040        367       327 2734\n1970          149         60        48  257\n1970-71       288         25        70  383\n1975-77       703        197       252 1152\n1977-78       425         62        88  575\n1980          121         72        79  272\nSum          3726        783       864 5373\n\n\nTo see the potential heterogeneity across rows in these data, we should perhaps also look at the proportions of autopsies in each of the three accuracy categories for each study.\n\naddmargins(round_half_up(100*prop.table(death_table,1),1),2) \n\n        Confirmed Inaccurate Incorrect Sum\n1955-65      74.6       13.4      12.0 100\n1970         58.0       23.3      18.7 100\n1970-71      75.2        6.5      18.3 100\n1975-77      61.0       17.1      21.9 100\n1977-78      73.9       10.8      15.3 100\n1980         44.5       26.5      29.0 100\n\n\nIn three of the studies, approximately 3/4 of the results were confirmed. In the other three, 45%, 58% and 61% were confirmed. It looks like there’s a fair amount of variation in results across studies. To see if this is true, formally, we run Pearson’s chi-square test of independence, where the null hypothesis is that the rows and columns are independent, and the alternative hypothesis is that there is an association between the rows and the columns.\n\nchisq.test(death_table)\n\n\n    Pearson's Chi-squared test\n\ndata:  death_table\nX-squared = 209.09, df = 10, p-value &lt; 2.2e-16\n\n\nThe chi-square test statistic is 200 on 10 degrees of freedom, yielding p &lt; 0.0001.\nAutopsies are not performed at random; in fact, many are done because the cause of death listed on the certificate is uncertain. What problems may arise if you attempt to use the results of these studies to make inference about the population as a whole?",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Larger Contingency Tables</span>"
    ]
  },
  {
    "objectID": "28-contingency_tables.html#three-way-tables-a-2x2xk-table-and-a-mantel-haenszel-analysis",
    "href": "28-contingency_tables.html#three-way-tables-a-2x2xk-table-and-a-mantel-haenszel-analysis",
    "title": "\n28  Larger Contingency Tables\n",
    "section": "\n28.5 Three-Way Tables: A 2x2xK Table and a Mantel-Haenszel Analysis",
    "text": "28.5 Three-Way Tables: A 2x2xK Table and a Mantel-Haenszel Analysis\nThe material I discuss in this section is attributable to Jeff Simonoff and his book Analyzing Categorical Data. The example is taken from Section 8.1 of that book.\nA three-dimensional or three-way table of counts often reflects a situation where the rows and columns refer to variables whose association is of primary interest to us, and the third factor (a layer, or strata) describes a control variable, whose effect on our primary association is something we are controlling for in the analysis.\n\n28.5.1 Smoking and Mortality in the UK\nIn the early 1970s and then again 20 years later, in Whickham, United Kingdom, surveys yielded the following relationship between whether a person was a smoker at the time of the original survey and whether they were still alive 20 years later.\n\nwhickham1 &lt;- matrix(c(502, 230, 443, 139), byrow=TRUE, nrow=2)\nrownames(whickham1) &lt;- c(\"Non-Smoker\", \"Smoker\")\ncolnames(whickham1) &lt;- c(\"Alive\", \"Dead\")\naddmargins(whickham1)\n\n           Alive Dead  Sum\nNon-Smoker   502  230  732\nSmoker       443  139  582\nSum          945  369 1314\n\n\nHere’s the two-by-two table analysis.\n\ntwoby2(whickham1)\n\n2 by 2 table analysis: \n------------------------------------------------------ \nOutcome   : Alive \nComparing : Non-Smoker vs. Smoker \n\n           Alive Dead    P(Alive) 95% conf. interval\nNon-Smoker   502  230      0.6858    0.6512   0.7184\nSmoker       443  139      0.7612    0.7248   0.7941\n\n                                    95% conf. interval\n             Relative Risk:  0.9010    0.8427   0.9633\n         Sample Odds Ratio:  0.6848    0.5353   0.8761\nConditional MLE Odds Ratio:  0.6850    0.5307   0.8822\n    Probability difference: -0.0754   -0.1230  -0.0266\n\n             Exact P-value: 0.0030 \n        Asymptotic P-value: 0.0026 \n------------------------------------------------------\n\n\nThere is a detectable association between smoking and mortality, but it isn’t the one you might expect.\n\nThe odds ratio is 0.68, implying that the odds of having lived were only 68% as large for non-smokers as for smokers.\nDoes that mean that smoking is good for you?\n\nNot likely. There is a key “lurking” variable here - a variable that is related to both smoking and mortality that is obscuring the actual relationship - namely, age.\n\n28.5.2 The whickham data with age, too\nThe table below gives the mortality experience separated into subtables by initial age group.\n\nage &lt;- c(rep(\"18-24\", 4), rep(\"25-34\", 4), \n         rep(\"35-44\", 4), rep(\"45-54\", 4), \n         rep(\"55-64\", 4), rep(\"65-74\", 4), \n         rep(\"75+\", 4))\nsmoking &lt;- c(rep(c(\"Smoker\", \"Smoker\", \"Non-Smoker\", \"Non-Smoker\"), 7))\nstatus &lt;- c(rep(c(\"Alive\", \"Dead\"), 14))\ncounts &lt;- c(53, 2, 61, 1, 121, 3, 152, 5,\n            95, 14, 114, 7, 103, 27, 66, 12,\n            64, 51, 81, 40, 7, 29, 28, 101,\n            0, 13, 0, 64)\n\nwhickham2 &lt;- tibble(smoking, status, age, counts) |&gt;\n    mutate(smoking = factor(smoking),\n           status = factor(status),\n           age = factor(age))\n\n\nwhickham2\n\n# A tibble: 28 × 4\n   smoking    status age   counts\n   &lt;fct&gt;      &lt;fct&gt;  &lt;fct&gt;  &lt;dbl&gt;\n 1 Smoker     Alive  18-24     53\n 2 Smoker     Dead   18-24      2\n 3 Non-Smoker Alive  18-24     61\n 4 Non-Smoker Dead   18-24      1\n 5 Smoker     Alive  25-34    121\n 6 Smoker     Dead   25-34      3\n 7 Non-Smoker Alive  25-34    152\n 8 Non-Smoker Dead   25-34      5\n 9 Smoker     Alive  35-44     95\n10 Smoker     Dead   35-44     14\n# ℹ 18 more rows\n\n\n\nwhick_t2 &lt;- \n    xtabs(counts ~ smoking + status + age, data = whickham2)\n\nwhick_t2\n\n, , age = 18-24\n\n            status\nsmoking      Alive Dead\n  Non-Smoker    61    1\n  Smoker        53    2\n\n, , age = 25-34\n\n            status\nsmoking      Alive Dead\n  Non-Smoker   152    5\n  Smoker       121    3\n\n, , age = 35-44\n\n            status\nsmoking      Alive Dead\n  Non-Smoker   114    7\n  Smoker        95   14\n\n, , age = 45-54\n\n            status\nsmoking      Alive Dead\n  Non-Smoker    66   12\n  Smoker       103   27\n\n, , age = 55-64\n\n            status\nsmoking      Alive Dead\n  Non-Smoker    81   40\n  Smoker        64   51\n\n, , age = 65-74\n\n            status\nsmoking      Alive Dead\n  Non-Smoker    28  101\n  Smoker         7   29\n\n, , age = 75+\n\n            status\nsmoking      Alive Dead\n  Non-Smoker     0   64\n  Smoker         0   13\n\n\nThe sample odds ratios for remaining Alive comparing Non-Smokers to Smokers for each of these subtables (except the last one, where the odds ratio is undefined because of the zero cells) are calculated using the usual cross-product ratio approach, which yields the following results.\n\n\nAge Group\nOdds Ratio\n\n\n\n18-24\n2.30\n\n\n25-34\n0.75\n\n\n35-44\n2.40\n\n\n45-54\n1.44\n\n\n55-64\n1.61\n\n\n65-74\n1.15\n\n\n75+\nUndefined\n\n\n\nThus, for all age groups except 25-34 year olds, not smoking appears to be associated with higher odds of remaining alive.\nWhy? Not surprisingly, there is a strong association between age and mortality, with mortality rates being very low for young people (2.5% for 18-24 year olds) and increasing to 100% for 75+ year olds.\nThere is also an association between age and smoking, with smoking rates peaking in the 45-54 year old range and then falling off rapidly. In particular, respondents who were 65 and older at the time of the first survey had very low smoking rates (25.4%) but very high mortality rates (85.5%). Smoking was hardly the cause, however, since even among the 65-74 year olds mortality was higher among smokers (80.6%) than it was among non-smokers (78.3%). A flat version of the table (ftable in R) can help us with these calculations.\n\nftable(whick_t2)\n\n                  age 18-24 25-34 35-44 45-54 55-64 65-74 75+\nsmoking    status                                            \nNon-Smoker Alive         61   152   114    66    81    28   0\n           Dead           1     5     7    12    40   101  64\nSmoker     Alive         53   121    95   103    64     7   0\n           Dead           2     3    14    27    51    29  13\n\n\n\n28.5.3 Checking Assumptions: The Woolf test\nWe can also obtain a test (using the woolf_test function, in the vcd library) to see if the common odds ratio estimated in the Mantel-Haenszel procedure is reasonable for all age groups. In other words, the Woolf test is a test of the assumption of homogeneous odds ratios across the six age groups.\nIf the Woolf test is significant, it suggests that the Cochran-Mantel-Haenszel test is not appropriate, since the odds ratios for smoking and mortality vary too much in the sub-tables by age group. Here, we have the following log odds ratios (estimated using conditional maximum likelihood, rather than cross-product ratios) and the associated Woolf test.\n\n## Next two results use the vcd package\noddsratio(whick_t2, log = TRUE)\n\nlog odds ratios for smoking and status by age \n\n      18-24       25-34       35-44       45-54       55-64       65-74 \n 0.65018114 -0.22473479  0.84069420  0.34608770  0.47421763  0.09933253 \n        75+ \n-1.56397554 \n\nwoolf_test(whick_t2)\n\n\n    Woolf-test on Homogeneity of Odds Ratios (no 3-Way assoc.)\n\ndata:  whick_t2\nX-squared = 3.2061, df = 6, p-value = 0.7826\n\n\nAs you can see, the Woolf test is not close to our usual standards for statistically detectable results, implying the common odds ratio is at least potentially reasonable for all age groups (or at least the ones under ages 75, where some data are available.)\n\n28.5.4 The Cochran-Mantel-Haenszel Test\nSo, the marginal table looking at smoking and mortality combining all age groups isn’t the most meaningful summary of the relationship between smoking and mortality. Instead, we need to look at the conditional association of smoking and mortality, given age, to address our interests.\nThe null hypothesis would be that, in the population, smoking and mortality are independent within strata formed by age group. In other words, \\(H_0\\) requires that smoking be of no value in predicting mortality once age has been accounted for.\nThe alternative hypothesis would be that, in the population, smoking and mortality are associated within the strata formed by age group. In other words, \\(H_A\\) requires that smoking be of at least some value in predicting mortality even after age has been accounted for.\nWe can consider the evidence that helps us choose between these two hypotheses with a Cochran-Mantel-Haenszel test, which is obtained in R through the mantelhaen.test function. This test requires us to assume that, in the population and within each age group, the smoking-mortality odds ratio is the same. Essentially, this means that the association of smoking with mortality is the same for older and younger people.\n\nmantelhaen.test(whick_t2, conf.level  = 0.90)\n\n\n    Mantel-Haenszel chi-squared test with continuity correction\n\ndata:  whick_t2\nMantel-Haenszel X-squared = 5.435, df = 1, p-value = 0.01974\nalternative hypothesis: true common odds ratio is not equal to 1\n90 percent confidence interval:\n 1.143198 2.041872\nsample estimates:\ncommon odds ratio \n          1.52783 \n\n\n\nThe Cochran-Mantel-Haenszel test statistic is a bit larger than 5 (after a continuity correction) leading to a p value of 0.02, indicating strong rejection of the null hypothesis of conditional independence of smoking and survival given age.\nThe estimated common conditional odds ratio is 1.53. This implies that (adjusting for age) being a non-smoker is associated with 53% higher odds of being alive 20 years later than being a smoker.\nA 90% confidence interval for that common odds ratio is (1.14, 2.04), reinforcing rejection of the null hypothesis of conditional independence (where the odds ratio would be 1).\n\n28.5.5 Without the Continuity Correction\nBy default, R presents the Mantel-Haenszel test with a continuity correction, when used for a 2x2xK table. In virtually all cases, go ahead and do this, but as you can see below, the difference it makes in this case is modest.\n\nmantelhaen.test(whick_t2, correct=FALSE, conf.level = 0.90)\n\n\n    Mantel-Haenszel chi-squared test without continuity correction\n\ndata:  whick_t2\nMantel-Haenszel X-squared = 5.8443, df = 1, p-value = 0.01563\nalternative hypothesis: true common odds ratio is not equal to 1\n90 percent confidence interval:\n 1.143198 2.041872\nsample estimates:\ncommon odds ratio \n          1.52783",
    "crumbs": [
      "Part B. Comparing Summaries",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Larger Contingency Tables</span>"
    ]
  },
  {
    "objectID": "29-mult_reg_intro.html",
    "href": "29-mult_reg_intro.html",
    "title": "29  Multiple Regression: Introduction",
    "section": "",
    "text": "29.1 Reminders of a few Key Concepts",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Multiple Regression: Introduction</span>"
    ]
  },
  {
    "objectID": "29-mult_reg_intro.html#reminders-of-a-few-key-concepts",
    "href": "29-mult_reg_intro.html#reminders-of-a-few-key-concepts",
    "title": "29  Multiple Regression: Introduction",
    "section": "",
    "text": "Scatterplots We have often accompanied our scatterplots with regression lines estimated by the method of least squares, and by loess smooths which permit local polynomial functions to display curved relationships, and occasionally presented in the form of a scatterplot matrix to enable simultaneous comparisons of multiple two-way associations.\nMeasures of Correlation/Association By far the most commonly used is the Pearson correlation, which is a unitless (scale-free) measure of bivariate linear association for the variables X and Y, symbolized by r, and ranging from -1 to +1. The Pearson correlation is a function of the slope of the least squares regression line, divided by the product of the standard deviations of X and Y. We have also mentioned the Spearman rank correlation coefficient, which is obtained by using the usual formula for a Pearson correlation, but on the ranks (1 = minimum, n = maximum, with average ranks are applied to the ties) of the X and Y values. This approach (running a correlation of the orderings of the data) substantially reduces the effect of outliers. The result still ranges from -1 to +1, with 0 indicating no linear association.\nFitting Linear Models We have fit several styles of linear model to date, including both simple regressions, where our outcome Y is modeled as a linear function of a single predictor X, and multiple regression models, where more than one predictor is used. Functions from the broom package can be used to yield several crucial results, in addition to those we can obtain with a summary of the model. These include:\n\n\n(from tidy) the estimated coefficients (intercept and slope(s)) of the fitted model, and\n(from glance) the \\(R^2\\) or coefficient of determination, which specifies the proportion of variation in our outcome accounted for by the linear model, and various other summaries of the model’s quality of fit\n(from augment) fitted values, residuals and other summaries related to individual points used to fit the model, or individual predictions made by the model, which will be helpful for assessing predictive accuracy and for developing diagnostic tools for assessing the assumptions of multiple regression.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Multiple Regression: Introduction</span>"
    ]
  },
  {
    "objectID": "29-mult_reg_intro.html#what-is-important-in-431",
    "href": "29-mult_reg_intro.html#what-is-important-in-431",
    "title": "29  Multiple Regression: Introduction",
    "section": "29.2 What is important in 431?",
    "text": "29.2 What is important in 431?\nIn 431, my primary goal is to immerse you in several cases, which will demonstrate good statistical practice in the analysis of data using multiple regression models. Often, we will leave gaps for 432, but the principal goal is to get you to the point where you can do a solid (if not quite complete) analysis of data for the modeling part (Study 2) of Project B.\nKey topics regarding multiple regression we cover in 431 include:\n\nDescribing the multivariate relationship - Scatterplots and smoothing - Correlation coefficients, Correlation matrices\nTransformations and Re-expression - The need for transformation - Using a Box-Cox method to help identify effective transformation choices - Measuring and addressing collinearity\nTesting the significance of a multiple regression model - T tests for individual predictors as last predictor in - Global F tests based on ANOVA to assess overall predictive significance - Incremental and Sequential testing of groups of predictors\nInterpreting the predictive value of a model - \\(R^2\\) and Adjusted \\(R^2\\), along with AIC and BIC - Residual standard deviation and RMSE\nChecking model assumptions - Residual Analysis including studentized residuals, and the major plots - Identifying points with high Leverage - Assessing Influence numerically and graphically\nModel Selection - The importance of parsimony - Stepwise regression\nAssessing Predictive Accuracy through Cross-Validation - Summaries of predictive error - Plotting predictions across multiple models\nSummarizing the Key Findings of the Model, briefly and accurately - Making the distinction between causal findings and associations - The importance of logic, theory and empirical evidence. (LTE)",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Multiple Regression: Introduction</span>"
    ]
  },
  {
    "objectID": "30-regr_diagnostics.html",
    "href": "30-regr_diagnostics.html",
    "title": "30  Regression Diagnostics",
    "section": "",
    "text": "30.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(broom)\nlibrary(car)\nlibrary(GGally)\nlibrary(ggrepel)\nlibrary(janitor)\nlibrary(kableExtra)\nlibrary(mosaic)\nlibrary(patchwork)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "30-regr_diagnostics.html#introduction",
    "href": "30-regr_diagnostics.html#introduction",
    "title": "30  Regression Diagnostics",
    "section": "\n30.2 Introduction",
    "text": "30.2 Introduction\nSome of this discussion comes from Bock, Velleman, and De Veaux (2004).\nMultiple linear regression (also called ordinary least squares, or OLS) has four main assumptions that are derived from its model.\nFor a simple regression, the model underlying the regression line is \\(E(Y) = \\beta_0 + \\beta_1 x\\)\n\nwhere E(Y) = the expectation (mean) of the outcome Y,\n\n\\(\\beta_0\\) is the intercept, and\n\n\\(\\beta_1\\) is the slope\n\nNow, if we have a multiple regression with three predictors \\(x_1, x_2\\) and \\(x_3\\), then the model becomes \\(E(Y) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\).\nAlternatively, we can write the model to relate individual y’s to the x’s by adding an individual error term: \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon\\)\nOf course, the multiple regression model is not limited to three predictors. We will often use k to represent the number of coefficients (slopes plus intercept) in the model, and will also use p to represent the number of predictors (usually p = k - 1).",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "30-regr_diagnostics.html#building-an-example-500-subjects-from-wcgs",
    "href": "30-regr_diagnostics.html#building-an-example-500-subjects-from-wcgs",
    "title": "30  Regression Diagnostics",
    "section": "\n30.3 Building an Example: 500 subjects from WCGS",
    "text": "30.3 Building an Example: 500 subjects from WCGS\nTo fix ideas, suppose we return to the Western Collaborative Group Study (WCGS) data we studied in Chapter 17, which described 3154 men ages 39-59 who were free of heart disease and were employees of one of 10 California companies in 1960-1961.\nWe are going to select a random sample of 500 of those men, with complete data on our outcome of interest, which is total cholesterol in mg/dl (chol) and on our three predictors of interest, which are systolic blood pressure in mm Hg (sbp), body mass index (bmi) and behavioral pattern (dibpat), which is a two-category variable with levels “Type A” or “Type B”.\n\nIndividuals exhibiting a Type A behavior pattern show evidence of being competitive and self-critical without feeling joy in their efforts or accomplishments, and are more likely to have high work involvement, important life imbalance, and high blood pressure.\nIndividuals exhibiting a Type B behavior pattern show evidence of being more reflective, more relaxed, less competitive and less anxious then Type A individuals.\nThere is substantial evidence to discredit the Type A Behavior Pattern theory in some areas, and some scholars definitely argue that Type A behavior is not a good predictor of coronary heart disease.\n\nBelow we create this sample of 500 subjects with complete data on our outcome and each of our three predictors.\n\nwcgs &lt;- read_csv(\"data/wcgs.csv\", show_col_types = FALSE)\n\nset.seed(12345)\n\nwcgs_500 &lt;- wcgs |&gt; \n    select(id, sbp, bmi, dibpat, chol) |&gt;\n    filter(complete.cases(id, sbp, bmi, dibpat, chol)) |&gt;\n    slice_sample(n = 500, replace = FALSE)\n\nNext, we’ll look at a scatterplot matrix using ggpairs from the GGally package, placing our outcome last, as usual.\n\nggpairs(wcgs_500 |&gt; select(sbp, bmi, dibpat, chol),\n        lower = list(combo = wrap(\"facethist\", binwidth = 0.5)), \n        title = \"Scatterplot Matrix for 500 WCGS subjects\")\n\n\n\n\n\n\n\nNext, we create our planned linear model, and evaluate the coefficients and fit quality.\n\nchol_m1 &lt;- lm(chol ~ sbp + bmi + dibpat, data = wcgs_500)\n\nsummary(chol_m1)\n\n\nCall:\nlm(formula = chol ~ sbp + bmi + dibpat, data = wcgs_500)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-113.133  -25.689   -1.932   24.036  186.390 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  214.5185    19.8825  10.789  &lt; 2e-16 ***\nsbp            0.2361     0.1125   2.099  0.03634 *  \nbmi           -0.5894     0.7290  -0.808  0.41921    \ndibpatType B -11.0408     3.5643  -3.098  0.00206 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39.66 on 496 degrees of freedom\nMultiple R-squared:  0.02954,   Adjusted R-squared:  0.02367 \nF-statistic: 5.033 on 3 and 496 DF,  p-value: 0.001915\n\n\nAs usual, we’ll use tidy and glance from the broom package (part of tidymodels) to evaluate the coefficients in more detail, and summarize the quality of fit for our model.\n\ntidy(chol_m1, conf.int = TRUE, conf.level = 0.90) |&gt;\n    select(term, estimate, std.error, conf.low, conf.high, p.value) |&gt;\n    kbl(digits = 3) |&gt; kable_paper()\n\n\n\nterm\nestimate\nstd.error\nconf.low\nconf.high\np.value\n\n\n\n(Intercept)\n214.519\n19.883\n181.753\n247.284\n0.000\n\n\nsbp\n0.236\n0.112\n0.051\n0.421\n0.036\n\n\nbmi\n-0.589\n0.729\n-1.791\n0.612\n0.419\n\n\ndibpatType B\n-11.041\n3.564\n-16.915\n-5.167\n0.002\n\n\n\n\n\n\nglance(chol_m1) |&gt;\n    select(r.squared, adj.r.squared, sigma, AIC, BIC, nobs) |&gt;\n    kbl(digits = c(3, 3, 1, 1, 1, 0)) |&gt; kable_paper()\n\n\n\nr.squared\nadj.r.squared\nsigma\nAIC\nBIC\nnobs\n\n\n0.03\n0.024\n39.7\n5105.3\n5126.4\n500\n\n\n\n\nThis isn’t a terrific model, accounting for only 3% (according to \\(R^2\\)) of the variation in our outcome, although some of the individual predictors show effects that meet our usual standards for “detectable” results. Our goal, though, is to use this model as an example for studying residual diagnostics to search for possible violations of regression assumptions.\nTo assess regression assumptions, we’ll have to look at residuals and fits for each of the individual points, which we can obtain use the augment function from the broom package.\n\naug_chol1 &lt;- augment(chol_m1, data = wcgs_500)\n\nThis augments the original data with several summaries of the quality of fit for our individual rows in the wcgs_500 data. Here are the first few values for some of the elements of augment.\n\nhead(aug_chol1 |&gt; select(id:.resid)) |&gt; \n  kbl(digits = 3) |&gt; kable_minimal()\n\n\n\nid\nsbp\nbmi\ndibpat\nchol\n.fitted\n.resid\n\n\n\n10155\n120\n18.162\nType B\n294\n221.104\n72.896\n\n\n3495\n140\n24.807\nType A\n301\n232.950\n68.050\n\n\n12634\n124\n21.616\nType A\n196\n231.054\n-35.054\n\n\n21354\n114\n25.381\nType A\n176\n226.474\n-50.474\n\n\n3085\n190\n22.349\nType A\n241\n246.203\n-5.203\n\n\n3589\n140\n23.054\nType B\n268\n222.943\n45.057\n\n\n\n\n\nThis includes the original data, plus the model’s predicted value for chol, stored in .fitted and the residual (actual chol - .fitted) stored in .resid. Here are the other elements of the augment result.\n\nhead(aug_chol1 |&gt; select(id, .std.resid:.cooksd)) |&gt; \n  kbl(digits = 3) |&gt; kable_minimal()\n\n\n\nid\n.std.resid\n.cooksd\n\n\n\n10155\n1.854\n0.015\n\n\n3495\n1.720\n0.003\n\n\n12634\n-0.887\n0.001\n\n\n21354\n-1.277\n0.003\n\n\n3085\n-0.134\n0.000\n\n\n3589\n1.140\n0.002\n\n\n\n\n\nHere, we see:\n\n\n.std.resid the standardized residual, which is a measure of how poorly fit the point is, scaled to have standard deviation 1 (the raw residuals, as well as these, already have mean 0)\n\n.hat, the hat matrix value, which is a measurement of the leverage this point has on the model. Leverage is, essentially, a measure of how unusual the point is in terms of the predictors.\n\n.sigma, the value of the residual standard error .sigma that would emerge if this point was deleted from the chol_m1 model, and\n\n.cooksd, Cook’s distance, or Cook’s d, which measures the influence of this point on the model. A highly influential point (one with .cooksd greater than 0.5, for instance) would be a point that, if removed from the chol_m1 model, would substantially change the fit of the model or the coefficient estimates.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "30-regr_diagnostics.html#the-four-key-regression-assumptions",
    "href": "30-regr_diagnostics.html#the-four-key-regression-assumptions",
    "title": "30  Regression Diagnostics",
    "section": "\n30.4 The Four Key Regression Assumptions",
    "text": "30.4 The Four Key Regression Assumptions\nThe assumptions and conditions for a multiple regression model are nearly the same as those for simple regression. The key assumptions that must be checked in a multiple regression model are:\n\nLinearity Assumption\nIndependence Assumption\nEqual Variance (Constant Variance / Homoscedasticity) Assumption\nNormality Assumption\n\nHappily, R is well suited to provide us with multiple diagnostic tools to generate plots and other summaries that will let us look more closely at each of these assumptions.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "30-regr_diagnostics.html#the-linearity-assumption",
    "href": "30-regr_diagnostics.html#the-linearity-assumption",
    "title": "30  Regression Diagnostics",
    "section": "\n30.5 The Linearity Assumption",
    "text": "30.5 The Linearity Assumption\nWe are fitting a linear model.\n\nBy linear, we mean that each predictor value, x, appears simply multiplied by its coefficient and added to the model.\nNo x appears in an exponent or some other more complicated function.\nIf the regression model is true, then the outcome y is linearly related to each of the x’s.\n\nUnfortunately, assuming the model is true is not sufficient to prove that the linear model fits, so we check what Bock, Velleman, and De Veaux (2004) call the “Straight Enough Condition”\n\n30.5.1 Initial Scatterplots for the “Straight Enough” Condition\n\nScatterplots of y against each of the predictors are reasonably straight.\nThe scatterplots need not show a strong (or any!) slope; we just check that there isn’t a bend or other nonlinearity.\nAny substantial curve is indicative of a potential problem.\nModest bends are not usually worthy of serious attention.\n\nFor example, in the wcgs_500 data, here are the relevant scatterplots for the quantitative predictors (in practice, I would simply look at the scatterplot matrix shown earlier) as well as a boxplot with violin for the categorical predictor (dibpat).\n\np1 &lt;- ggplot(wcgs_500, aes(x = sbp, y = chol)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", se = FALSE, span = 2, formula = y ~ x) +\n    geom_smooth(method = \"lm\", se = FALSE, col = \"tomato\", formula = y ~ x)\n\np2 &lt;- ggplot(wcgs_500, aes(x = bmi, y = chol)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", se = FALSE, span = 2, formula = y ~ x) +\n    geom_smooth(method = \"lm\", se = FALSE, col = \"tomato\", formula = y ~ x)\n\np3 &lt;- ggplot(wcgs_500, aes(x = dibpat, y = chol)) +\n    geom_violin(aes(fill = dibpat)) +\n    geom_boxplot(width = 0.3) +\n    coord_flip() + guides(fill = \"none\")\n\n(p1 + p2) / p3 + \n    plot_layout(heights = c(2,1)) +\n    plot_annotation(title = \"WCGS_500 Cholesterol vs. Predictors\")\n\n\n\n\n\n\n\nHere, I’ve simply placed recov.score on the vertical (Y) axis, plotted against each of the predictors, in turn. I’ve added a straight line (OLS) fit [in tomato red] and a loess smooth [in blue] to each plot to guide your assessment of the “straight enough” condition. Note the use here of span = 2 in the loess smooths to produce lines that are less wiggly than they would be using the default span = 0.75.\n\nEach of these is “straight enough” for our purposes, in initially fitting the data.\nIf one of these was not, we might consider a transformation of the relevant predictor, or, if all were problematic, we might transform the outcome Y.\n\n30.5.2 Residuals vs. Predicted Values to Check for Non-Linearity\nThe residuals should appear to have no pattern (no curve, for instance) with respect to the predicted (fitted) values. It is a very good idea to plot the residuals against the fitted values to check for patterns, especially bends or other indications of non-linearity. For a multiple regression, the fitted values are a combination of the x’s given by the regression equation, so they combine the effects of the x’s in a way that makes sense for our particular regression model. That makes them a good choice to plot against. We’ll check for other things in this plot, as well.\nWhen you ask R to plot the result of a linear model, it will produce up to five separate plots: the first of which is a plot of residuals vs. fitted values.\n\n30.5.2.1 Using plot(model, which = 1)\n\nTo obtain this plot for the model including sbp, bmi and dibpat to predict chol we have two options. The simpler approach uses the following code to indicate plot 1 using the which command within the plot function.\n\nplot(chol_m1, which=1)\n\n\n\n\n\n\n\nA smooth is again added (in red) to help you identify serious non-linearity. In this case, I would conclude that there were no serious problems with linearity in these data.\nThe plot also, by default, identifies the three values with the largest (in absolute value) residuals.\n\nHere, these are rows 29, 113 and 158, each of which has a positive residual (i.e. they represent under-predictions by the model.) To identify these points in the data, use the slice function.\n\n\nslice(wcgs_500, c(29, 113, 158))\n\n# A tibble: 3 × 5\n     id   sbp   bmi dibpat  chol\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1  3604   124  29.4 Type A   346\n2  2280   120  25.9 Type A   414\n3 10360   140  23.4 Type B   349\n\n\nThese turn out to be the observations with id = 3604, 2280 and 10360, respectively.\n\n30.5.2.2 Using ggplot2 to plot Residuals vs. Predicted Values\n\nggplot(aug_chol1, aes(x = .fitted, y = .resid)) +\n  geom_point() + \n  geom_point(data = aug_chol1 |&gt; \n               slice_max(abs(.resid), n = 5),\n             col = \"red\", size = 2) +\n  geom_text_repel(data = aug_chol1 |&gt; \n               slice_max(abs(.resid), n = 5),\n               aes(label = id), col = \"red\") +\n  geom_abline(intercept = 0, slope = 0, lty = \"dashed\") +\n  geom_smooth(method = \"loess\", formula = y ~ x, se = F) +\n  labs(title = \"Residuals vs. Fitted Values from chol_m1\",\n       caption = \"5 largest |residuals| highlighted in red.\",\n       x = \"Fitted Total Cholesterol\", y = \"Residual\") +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\n30.5.3 Residuals vs. Predictors To Further Check for Non-Linearity\nIf we do see evidence of non-linearity in the plot of residuals against fitted values, I usually then proceed to look at residual plots against each of the individual predictors, in turn, to try to identify the specific predictor (or predictors) where we may need to use a transformation.\nThe appeal of such plots (as compared to the initial scatterplots we looked at of the outcome against each predictor) is that they eliminate the distraction of the linear fit, and let us look more closely at the non-linear part of the relationship between the outcome and each quantitative predictor. We might also look at a boxplot of the relationship between the outcome and a categorical predictor, although this is primarily for the purpose of assessing the potential for non-constant variance, rather than non-linearity.\nAlthough I don’t think you need them here, here are these plots for the residuals from our chol_m1 model.\n\np1 &lt;- ggplot(aug_chol1, aes(x = sbp, y = chol)) +\n    geom_point() + \n    geom_smooth(method = \"loess\", formula = y ~ x, span = 2, se = FALSE) +\n    geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE, col = \"tomato\")\np2 &lt;- ggplot(aug_chol1, aes(x = bmi, y = chol)) +\n    geom_point() + \n    geom_smooth(method = \"loess\", formula = y ~ x, span = 2, se = FALSE) +\n    geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE, col = \"tomato\")\np3 &lt;- ggplot(aug_chol1, aes(x = dibpat, y = chol)) +\n    geom_violin(aes(fill = dibpat)) + geom_boxplot(width = 0.3) + \n    coord_flip() + guides(fill = \"none\")\n\n(p1 + p2) / p3 + \n    plot_layout(heights = c(2,1)) +\n    plot_annotation(title = \"Residuals vs. Predictors in chol_m1 model\")\n\n\n\n\n\n\n\nAgain, I see no particularly problematic issues with the assumption of linearity in either of the scatterplots here.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "30-regr_diagnostics.html#the-independence-assumption",
    "href": "30-regr_diagnostics.html#the-independence-assumption",
    "title": "30  Regression Diagnostics",
    "section": "\n30.6 The Independence Assumption",
    "text": "30.6 The Independence Assumption\nThe errors in the true underlying regression model must be mutually independent, but there is no way to be sure that the independence assumption is true. Fortunately, although there can be many predictor variables, there is only one outcome variable and only one set of errors. The independence assumption concerns the errors, so we check the residuals.\nRandomization condition. The data should arise from a random sample, or from a randomized experiment.\n\nThe residuals should appear to be randomly scattered and show no patterns, trends or clumps when plotted against the predicted values.\nIn the special case when an x-variable is related to time, make sure that the residuals do not have a pattern when plotted against time.\n\n\n30.6.1 Residuals vs. Fitted Values to Check for Dependence\nThe wcgs_500 men were not related in any way to each other except that they were drawn from the same 10 employers, and the data are cross-sectional here, so we can be pretty sure that their measurements are independent. The residuals vs. fitted values plots we’ve already seen show no clear trend, cycle or pattern which concerns us.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "30-regr_diagnostics.html#the-constant-variance-assumption",
    "href": "30-regr_diagnostics.html#the-constant-variance-assumption",
    "title": "30  Regression Diagnostics",
    "section": "\n30.7 The Constant Variance Assumption",
    "text": "30.7 The Constant Variance Assumption\nThe variability of our outcome, y, should be about the same for all values of every predictor x. Of course, we can’t check every combination of x values, so we look at scatterplots and check to see if the plot shows a “fan” shape - in essence, the Does the plot thicken? Condition.\n\nScatterplots of residuals against each x or against the predicted values, offer a visual check.\nBe alert for a “fan” or a “funnel” shape showing growing/shrinking variability in one part of the plot.\n\nReviewing the same plots we have previously seen, I can find no evidence of substantial “plot thickening” in either the plot of residuals vs. fitted values or in any of the plots of the residuals against each predictor separately.\n\n30.7.1 The Scale-Location Plot to Check for Non-Constant Variance\nR does provide an additional plot to help assess this issue as linear model diagnostic plot 3. This one looks at the square root of the standardized residual plotted against the fitted values. You want the loess smooth in this plot to be flat, rather than sloped.\n\nplot(chol_m1, which=3)\n\n\n\n\n\n\n\nWe can replicate this plot using ggplot2 as follows. With a loess smooth with span = 2, there’s no suggestion of a meaningful trend up or down in this plot.\n\nggplot(aug_chol1, aes(x = .fitted, y = sqrt(abs(.std.resid)))) +\n  geom_point() + \n  geom_point(data = aug_chol1 |&gt; \n               slice_max(sqrt(abs(.std.resid)), n = 3),\n             col = \"red\", size = 1) +\n  geom_text_repel(data = aug_chol1 |&gt; \n               slice_max(sqrt(abs(.std.resid)), n = 3),\n               aes(label = id), col = \"red\") +\n  geom_smooth(method = \"loess\", formula = y ~ x, span = 2, se = F) +\n  labs(title = \"Scale-Location Plot for chol_m1 model\",\n       caption = \"3 largest |Standardized Residual| in red.\",\n       x = \"Fitted Value of Total Cholesterol\", \n       y = \"Square Root of |Standardized Residual|\") +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\n30.7.2 What does trouble look like?\nIt’s helpful to see how this works in practice. Here are three sets of plots for two settings, where the left plots in each pair show simulated data that are homoscedastic (variance is constant across the levels of the predictor) and the right plots show data that are heteroscedastic (variance is not constant across predictor levels.) Source: This example is modified from http://goo.gl/weMI0U [from stats.stackexchange.com]\nHere, I’ll create a tibble containing fake data on a predictor (x) and two outcomes (y1 or y2). The relationship between y1 and x is meant to show no problems with the assumption of homoscedasticity. The relationship between y2 and x, on the other hand, should show severe problems with this assumption.\n\nset.seed(5)\n\nN  = 250    ## original n = 500\nb0 = 3\nb1 = 0.4\n\ns2 = 5\ng1 = 1.5     ## original g1 = 1.5\ng2 = 0.02    ## original g2 = 0.015\n\nx        = runif(N, min=0, max=100)\ny1   = b0 + b1*x + rnorm(N, mean=0, sd=sqrt(s2            ))\ny2 = b0 + b1*x + rnorm(N, mean=0, sd=sqrt(exp(g1 + g2*x)))\n\nfake_1 &lt;- tibble(id = 1:250, x, y1, y2)\n\nFirst, we’ll fit our two models, and obtain augmented values.\n\nfake_m1 &lt;- lm(y1 ~ x, data = fake_1) # no problems\nfake_m2 &lt;- lm(y2 ~ x, data = fake_1) # non-constant variance\n\naug_fake1 &lt;- augment(fake_m1, data = fake_1)\naug_fake2 &lt;- augment(fake_m2, data = fake_1)\n\nNow, let’s look at the plots, developed first using ggplot2.\nFirst, here are the two scatterplots, for our fake_1 model with constant variance on the left, and our fake_2 model with non-constant variance on the right.\n\np1a &lt;- ggplot(fake_1, aes(x = x, y = y1)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x, span = 2, se = F) +\n    geom_smooth(method = \"lm\", formula = y ~ x, se = F, col = \"tomato\") +\n    labs(title = \"Constant Variance (fake_m1)\",\n         subtitle = \"Scatterplot of y1 vs. x\")\n\np1b &lt;- ggplot(fake_1, aes(x = x, y = y2)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x, span = 2, se = F) +\n    geom_smooth(method = \"lm\", formula = y ~ x, se = F, col = \"tomato\") +\n    labs(title = \"Non-Constant Variance (fake_m2)\",\n         subtitle = \"Scatterplot of y2 vs. x\")\n\np1a + p1b \n\n\n\n\n\n\n\nSecond, here are the two plots of residuals vs. fitted values, for our fake_1 model with constant variance on the left, and our fake_2 model with non-constant variance on the right.\n\np2a &lt;- ggplot(aug_fake1, aes(x = .fitted, y = .resid)) +\n    geom_point() + \n    geom_point(data = aug_fake1 |&gt; \n                   slice_max(abs(.resid), n = 5),\n               col = \"red\", size = 2) +\n    geom_text_repel(data = aug_fake1 |&gt; \n                        slice_max(abs(.resid), n = 5),\n                    aes(label = id), col = \"red\") +\n    geom_abline(intercept = 0, slope = 0, lty = \"dashed\") +\n    geom_smooth(method = \"loess\", formula = y ~ x, se = F) +\n    labs(title = \"Constant Variance (fake_m1)\",\n         subtitle = \"Residuals vs. Fitted Values\",\n         caption = \"5 largest |residuals| highlighted in red.\",\n         x = \"Fitted Value of y1\", y = \"Residual\") +\n    theme(aspect.ratio = 1)\n\np2b &lt;- ggplot(aug_fake2, aes(x = .fitted, y = .resid)) +\n    geom_point() + \n    geom_point(data = aug_fake2 |&gt; \n                   slice_max(abs(.resid), n = 5),\n               col = \"red\", size = 2) +\n    geom_text_repel(data = aug_fake2 |&gt; \n                        slice_max(abs(.resid), n = 5),\n                    aes(label = id), col = \"red\") +\n    geom_abline(intercept = 0, slope = 0, lty = \"dashed\") +\n    geom_smooth(method = \"loess\", formula = y ~ x, se = F) +\n    labs(title = \"Non-Constant Variance (fake_m2)\",\n         subtitle = \"Residuals vs. Fitted Values\",\n         caption = \"5 largest |residuals| highlighted in red.\",\n         x = \"Fitted Value of y1\", y = \"Residual\") +\n    theme(aspect.ratio = 1)\n\np2a + p2b \n\n\n\n\n\n\n\nFinally, here is the scale-location plot for each scenario.\n\np3a &lt;- ggplot(aug_fake1, aes(x = .fitted, y = sqrt(abs(.std.resid)))) +\n  geom_point() + \n  geom_point(data = aug_fake1 |&gt; \n               slice_max(sqrt(abs(.std.resid)), n = 3),\n             col = \"red\", size = 1) +\n  geom_text_repel(data = aug_fake1 |&gt; \n               slice_max(sqrt(abs(.std.resid)), n = 3),\n               aes(label = id), col = \"red\") +\n  geom_smooth(method = \"loess\", formula = y ~ x, span = 2, se = F) +\n  labs(title = \"Constant Variance (fake_m1)\",\n       subtitle = \"Scale-Location Plot\",\n       caption = \"3 largest |Standardized Residual| in red.\",\n       x = \"Fitted Value\", \n       y = \"Square Root of |Standardized Residual|\") +\n  theme(aspect.ratio = 1)\n\np3b &lt;- ggplot(aug_fake2, aes(x = .fitted, y = sqrt(abs(.std.resid)))) +\n  geom_point() + \n  geom_point(data = aug_fake2 |&gt; \n               slice_max(sqrt(abs(.std.resid)), n = 3),\n             col = \"red\", size = 1) +\n  geom_text_repel(data = aug_fake2 |&gt; \n               slice_max(sqrt(abs(.std.resid)), n = 3),\n               aes(label = id), col = \"red\") +\n  geom_smooth(method = \"loess\", formula = y ~ x, span = 2, se = F) +\n  labs(title = \"Non-Constant Variance (fake_m2)\",\n       subtitle = \"Scale-Location Plot\",\n       caption = \"3 largest |Standardized Residual| in red.\",\n       x = \"Fitted Value\", \n       y = \"Square Root of |Standardized Residual|\") +\n  theme(aspect.ratio = 1)\n\np3a + p3b\n\n\n\n\n\n\n\nNote the funnel shape for the upper two heteroscedastic (non-constant variance) plots, and the upward sloping loess line in the last one.\nHere are the relevant plots for the same scenario using base R.\n\npar(mfrow=c(3,2))\nplot(y1 ~ x, data = fake_1, main=\"Constant Variance\")\nplot(y2 ~ x, data = fake_1, main=\"Non-Constant Variance\")\nplot(fake_m1, which=1)\nplot(fake_m2, which=1)\nplot(fake_m1, which=3)\nplot(fake_m2, which=3)\n\n\n\n\n\n\npar(mfrow=c(1,1))",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "30-regr_diagnostics.html#the-normality-assumption",
    "href": "30-regr_diagnostics.html#the-normality-assumption",
    "title": "30  Regression Diagnostics",
    "section": "\n30.8 The Normality Assumption",
    "text": "30.8 The Normality Assumption\nIf the plot is straight enough, the data are independent, and the plots don’t thicken, you can now move on to the final assumption, that of Normality.\nWe assume that the errors around the idealized regression model at any specified values of the x-variables follow a Normal model. We need this assumption so that we can use a Student’s t-model for inference. As with other times when we’ve used Student’s t, we’ll settle for the residuals satisfying the Nearly Normal condition. To assess this, we simply look at a histogram or Normal probability plot of the residuals. Note that the Normality Assumption also becomes less important as the sample size grows.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "30-regr_diagnostics.html#outlier-diagnostics-points-with-unusual-residuals",
    "href": "30-regr_diagnostics.html#outlier-diagnostics-points-with-unusual-residuals",
    "title": "30  Regression Diagnostics",
    "section": "\n30.9 Outlier Diagnostics: Points with Unusual Residuals",
    "text": "30.9 Outlier Diagnostics: Points with Unusual Residuals\nA multiple regression model will always have a point which displays the most unusual residual (that is, the residual furthest from zero in either a positive or negative direction.) As part of our assessment of the normality assumption, we will often try to decide whether the residuals follow a Normal distribution by creating a Q-Q plot of standardized residuals.\n\n30.9.1 Standardized Residuals\nStandardized residuals are scaled to have mean zero and a constant standard deviation of 1, as a result of dividing the original (raw) residuals by an estimate of the standard deviation that uses all of the data in the data set.\nThe augment function, when applied to a linear regression model, will generate the standardized residuals and store them in the .std.resid column.\nIf multiple regression assumptions hold, then the standardized residuals (in addition to following a Normal distribution in general) will also have mean zero and standard deviation 1, with approximately 95% of values between -2 and +2, and approximately 99.74% of values between -3 and +3.\nA natural check, therefore, will be to identify the most extreme/unusual standardized residual in our data set, and see whether its value is within the general bounds implied by the Empirical Rule associated with the Normal distribution. If, for instance, we see a standardized residual below -3 or above +3, this is likely to be a very poorly fit point (we would expect to see a point like this about 2.6 times for every 1,000 observations.)\nA very poorly fitting point, especially if it also has high influence on the model (a concept we’ll discuss shortly), may be sufficiently different from the rest of the points in the data set to merit its removal before modeling. If we did remove such a point, we’d have to have a good reason for this exclusion, and include that explanation in our report.\nR’s general plot set for a linear model includes (as plot 2) a Normal Q-Q plot of the standardized residuals from the model.\n\n30.9.2 Checking the Normality Assumption with a Plot\n\nplot(chol_m1, which=2)\n\n\n\n\n\n\n\nor we can use the ggplot2 version:\n\np1 &lt;- ggplot(aug_chol1, aes(sample = .std.resid)) +\n  geom_qq() + \n  geom_qq_line(col = \"red\") +\n  labs(title = \"Normal Q-Q plot\",\n       y = \"Standardized Residual from chol_m1\", \n       x = \"Standard Normal Quantiles\") +\n  theme(aspect.ratio = 1)\n\np2 &lt;- ggplot(aug_chol1, aes(y = .std.resid, x = \"\")) +\n  geom_violin(fill = \"dodgerblue\") +\n  geom_boxplot(width = 0.3) +\n  labs(title = \"Box and Violin Plot\",\n       y = \"Standardized Residual from chol_m1\",\n       x = \"mod_1\")\n\np1 + p2 + \n  plot_layout(widths = c(2, 1)) +\n  plot_annotation(\n    title = \"Normality of Standardized Residuals from mod_1\",\n    caption = paste0(\"n = \", \n                     nrow(aug_chol1 |&gt; select(.std.resid)),\n                     \" residual values are plotted here.\"))\n\n\n\n\n\n\n\nThe Q-Q plot of standardized residuals shows one point (in row 113, according to the plot) that is far away from our expectations under the Normal model. Let’s look at that point a little more closely.\n\naug_chol1 |&gt; slice(113) |&gt; select(id:.std.resid) |&gt; \n  kbl(digits = 2) |&gt; kable_classic_2()\n\n\n\nid\nsbp\nbmi\ndibpat\nchol\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n2280\n120\n25.86\nType A\n414\n227.61\n186.39\n0.01\n38.8\n0.03\n4.71\n\n\n\n\nThe standardized residual here is 4.71. That seems excessively large, in a sample of 250 observations.\n\n30.9.3 Assessing Standardized Residuals with an Outlier Test\nIs a standardized residual of 4.71 particularly unusual in a sample of 250 such standardized residuals, given that we are trying to assume that these are well modeled by a Normal distribution, supposedly with mean 0 and standard deviation 1?\nYes. The car library has a test called outlierTest which can be applied to a linear model to see if the most unusual studentized residual in absolute value is a surprise given the sample size (the studentized residual is very similar to the standardized residual - it simply uses a different estimate for the standard deviation for every point, in each case excluding the point it is currently studying)\n\noutlierTest(chol_m1)\n\n    rstudent unadjusted p-value Bonferroni p\n113 4.816993         1.9404e-06   0.00097019\n\n\nOur conclusion from the Bonferroni p value here is that there’s evidence to support the belief that we could have a serious problem with Normality, in that a studentized residual of 4.82 is out of the range we might reasonably expect to see given a Normal distribution of errors and 250 observations.\nWhich value is that? It’s the subject with id = 2280.\n\naug_chol1 |&gt; slice(113) |&gt;\n    select(id:.std.resid) |&gt; kbl(digits = 2) |&gt; kable_classic_2()\n\n\n\nid\nsbp\nbmi\ndibpat\nchol\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n2280\n120\n25.86\nType A\n414\n227.61\n186.39\n0.01\n38.8\n0.03\n4.71\n\n\n\n\nIn terms of solutions, we could consider dropping the point, but this would leave us with the vexing problem of how, exactly, to justify that choice. One question we might ask is whether dropping that point (row 113, or id = 2280) from our chol_m1 model have a big impact?",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "30-regr_diagnostics.html#outlier-diagnostics-identifying-points-with-unusually-high-leverage",
    "href": "30-regr_diagnostics.html#outlier-diagnostics-identifying-points-with-unusually-high-leverage",
    "title": "30  Regression Diagnostics",
    "section": "\n30.10 Outlier Diagnostics: Identifying Points with Unusually High Leverage",
    "text": "30.10 Outlier Diagnostics: Identifying Points with Unusually High Leverage\nAn observation can be an outlier not just in terms of having an unusual residual, but also in terms of having an unusual combination of predictor values. Such an observation is described as having high leverage in a data set.\n\nThe augment function stores the leverage for each point in the .hat variable.\nThe average leverage value is equal to k/n, where n is the number of observations included in the regression model, and k is the number of coefficients (slopes + intercept).\nAny point with a leverage value greater than 3 times the average leverage of a point in the data set is one that should be investigated closely.\n\nFor instance, in the wcgs_500 data, we have 500 observations, so the average leverage will be 4/500 = 0.008 and a high leverage point would have leverage \\(\\geq\\) 12/500 or .024. We can check this out directly, with a sorted list of leverage values.\n\naug_chol1 |&gt; select(id:chol, .hat) |&gt; \n    filter(.hat &gt; 0.024) |&gt;\n    arrange(desc(.hat))\n\n# A tibble: 12 × 6\n      id   sbp   bmi dibpat  chol   .hat\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 19078   230  24.4 Type B   254 0.0884\n 2 10214   170  37.7 Type B   153 0.0600\n 3 22025   210  26.5 Type B   187 0.0548\n 4 10075   150  35.3 Type B   224 0.0401\n 5 16031   200  28.1 Type A   256 0.0392\n 6 11432   196  27.3 Type B   178 0.0384\n 7  3085   190  22.3 Type A   241 0.0383\n 8 13389   120  33.0 Type A   232 0.0303\n 9 12976   106  15.7 Type B   207 0.0287\n10  3182   120  32.3 Type A   218 0.0262\n11 19099   104  30.5 Type A   234 0.0252\n12  3235   184  26.6 Type A   274 0.0248\n\n\nTwelve of our 500 subjects meet the standard for large leverage values (.hat values exceeding three times the average .hat value) but none of these are the outlier (id = 2280) we identified as having an especially poor fit (large standardized residual.)\n\naug_chol1 |&gt;\n    filter(id == 2280) |&gt;\n    select(id, .std.resid, .hat)\n\n# A tibble: 1 × 3\n     id .std.resid    .hat\n  &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n1  2280       4.71 0.00555\n\n\nAnother way to understand the impact of leverage is to look at a plot of residuals vs. leverage, which is diagnostic plot 5 for a linear model. Here’s the base R version for our chol_m1 model.\n\nplot(chol_m1, which=5)\n\n\n\n\n\n\n\nHere is the ggplot2 version.\n\nggplot(aug_chol1, aes(x = .hat, y = .std.resid)) +\n  geom_point() + \n  geom_point(data = aug_chol1 |&gt; filter(.cooksd &gt;= 0.5),\n             col = \"red\", size = 2) +\n  geom_text_repel(data = aug_chol1 |&gt; filter(.cooksd &gt;= 0.5),\n               aes(label = id), col = \"red\") +\n  geom_smooth(method = \"loess\", formula = y ~ x, span = 2, se = F) +\n  geom_vline(aes(xintercept = 3*mean(.hat)), lty = \"dashed\") +\n  labs(title = \"Residuals vs. Leverage\",\n       x = \"Leverage\", y = \"Standardized Residual\") \n\n\n\n\n\n\n\nWe see that the most highly leveraged points are shown furthest to the right in this plot (in fact, I’ve inserted a dotted vertical line at the cutpoint of 3*mean(.hat)), but our problematic standardized residual has a very typical leverage value (at the top left of this plot.)\nLet’s look at our highest leveraged point (id = 19078 from our list above) and remember that this just means that this point is unusual in terms of its predictor values.\n\naug_chol1 |&gt; filter(id == 19078) |&gt;\n    select(id, sbp, bmi, dibpat, .hat)\n\n# A tibble: 1 × 5\n     id   sbp   bmi dibpat   .hat\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 19078   230  24.4 Type B 0.0884\n\n\nCompare these findings to what we see in the wcgs_500 data overall. Let’s focus on the quantitative variables involved in our model, sbp and bmi, and use the df_stats() function from the mosaic package.\n\ndf_stats(~ sbp + bmi, data = wcgs_500)\n\n  response      min        Q1   median       Q3       max      mean        sd\n1      sbp 100.0000 120.00000 126.0000 140.0000 230.00000 129.39800 16.548453\n2      bmi  15.6605  23.03123  24.5412  26.1554  37.65281  24.65342  2.546486\n    n missing\n1 500       0\n2 500       0\n\n\nNote that our highly leveraged point has the largest sbp of all 500 subjects, although a rather typical bmi.\nIf you’re wondering about the rest of our chol_m1 model, there are only two possible values of dibpat and so a point cannot really be an outlier on that variable.\n\nwcgs_500 |&gt; tabyl(dibpat)\n\n dibpat   n percent\n Type A 259   0.518\n Type B 241   0.482\n\n\nRemember that 12 of the points in our chol_m1 model showed up with leverage values more than three times higher than the mean leverage.\n\naug_chol1 |&gt; \n    filter(.hat &gt; 3 * mean(.hat)) |&gt; nrow()\n\n[1] 12\n\n\nSo, let’s look at a plot of the two quantitative predictors in our data to see which 12 points show up as those with “high” leverage.\n\nggplot(aug_chol1, aes(x = sbp, y = bmi)) +\n    geom_point(col = \"gray50\") + \n    geom_point(data = aug_chol1 |&gt; \n                   slice_max(.hat, n = 12),\n               col = \"red\", size = 2) + \n    labs(x = \"Systolic BP\", y = \"Body Mass Index\",\n         title = \"Where are the highly leveraged points in our model?\",\n         subtitle = \"wcgs_500 model chol_1, 12 highest leverage points in red\")\n\n\n\n\n\n\n\nThe points with high leverage (large values of .hat) are simply those with an unusual combination of the two quantitative predictors.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "30-regr_diagnostics.html#outlier-diagnostics-identifying-points-with-high-influence-on-the-model",
    "href": "30-regr_diagnostics.html#outlier-diagnostics-identifying-points-with-high-influence-on-the-model",
    "title": "30  Regression Diagnostics",
    "section": "\n30.11 Outlier Diagnostics: Identifying Points with High Influence on the Model",
    "text": "30.11 Outlier Diagnostics: Identifying Points with High Influence on the Model\nA point in a regression model has high influence if the inclusion (or exclusion) of that point will have substantial impact on the model, in terms of the estimated coefficients, and the summaries of fit. The measure I routinely use to describe the level of influence a point has on the model is called Cook’s distance.\nAs a general rule, a Cook’s distance greater than 1 indicates a point likely to have substantial influence on the model, while a point in the 0.5 to 1.0 range is only occasionally worthy of investigation. Observations with Cook’s distance below 0.5 are unlikely to influence the model in any substantial way.\n\n30.11.1 Assessing the Value of Cook’s Distance\nYou can obtain information on Cook’s distance in several ways in R.\n\nMy favorite is model diagnostic plot 5 (the residuals vs. leverage plot) which we’ve seen before and which I repeat below. This plot uses red contours to indicate the value of Cook’s distance.\n\nThis is possible because influence, as measured by Cook’s distance, is a function of both the observation’s standardized residual and its leverage.\n\n\n\n\nplot(chol_m1, which=5)\n\n\n\n\n\n\n\nNo points in this plot indicate substantial influence on our model. To do so, they’d have to be outside the Cook’s distance contour shown at the top right of this plot (there is a similar arc at the bottom right, but it’s too far away from the data to show up in the plot.)\n\n30.11.2 Index Plot of Cook’s Distance\nModel Diagnostic Plot 4 for a linear model is an index plot of Cook’s distance.\n\nplot(chol_m1, which=4)\n\n\n\n\n\n\n\nOr, if you like, run the ggplot2 version:\n\naug_chol1_ex &lt;- aug_chol1 |&gt; \n  mutate(obsnum = 1:nrow(aug_chol1 |&gt; select(.cooksd)))\n\nggplot(aug_chol1_ex, aes(x = obsnum, y = .cooksd)) + \n    geom_point() + \n    geom_segment(aes(x = obsnum, xend = obsnum, y = 0, yend = .cooksd)) +\n    geom_text_repel(data = aug_chol1_ex |&gt; \n               slice_max(.cooksd, n = 3),\n               aes(label = id)) +\n    labs(x = \"Row Number\",\n         y = \"Cook's Distance\",\n         title = \"Cook's distance Index plot for chol_m1\",\n         subtitle = \"Subjects with the 3 largest Cook's d values are identified.\")\n\n\n\n\n\n\n\nIt is clear from this plot that the largest Cook’s distance (somewhere between 0.05 and 0.06) is for the observation in subject 10214 (or row 131 of the data set, depending on which version you build. All of the Cook’s distance values are stored in the .cooksd variable in our augmented results. Let’s look at the six largest Cook’s distance values.\n\naug_chol1 |&gt; select(id, .std.resid, .hat, .cooksd) |&gt;\n    arrange(desc(.cooksd)) |&gt; head() |&gt; kbl(digits = 3) |&gt; kable_classic()\n\n\n\nid\n.std.resid\n.hat\n.cooksd\n\n\n\n10214\n-1.779\n0.060\n0.051\n\n\n2280\n4.713\n0.006\n0.031\n\n\n3604\n3.033\n0.012\n0.029\n\n\n22025\n-1.308\n0.055\n0.025\n\n\n11432\n-1.432\n0.038\n0.020\n\n\n12392\n-2.661\n0.010\n0.017\n\n\n\n\n\nRemember that we’d need a Cook’s distance value to be at least 0.50 for us to worry about it in a serious way. Here, the largest we see (0.05 for id = 10214) is still far away from that standard.\nWhat is the story on point 10214? Why is it unusual?\n\naug_chol1 |&gt; filter(id == 10214) |&gt;\n    select(id:.fitted, .std.resid, .hat, .cooksd) |&gt; \n    kbl(digits = 2) |&gt; kable_classic()\n\n\n\nid\nsbp\nbmi\ndibpat\nchol\n.fitted\n.std.resid\n.hat\n.cooksd\n\n\n10214\n170\n37.65\nType B\n153\n221.42\n-1.78\n0.06\n0.05\n\n\n\n\nThis turns out to be the subject with the largest BMI in the data, hence some leverage, and a somewhat large (-1.78) but not enormous standardized residual. Hence the combination suggests some influence on the model.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "30-regr_diagnostics.html#running-a-regression-model-while-excluding-a-point",
    "href": "30-regr_diagnostics.html#running-a-regression-model-while-excluding-a-point",
    "title": "30  Regression Diagnostics",
    "section": "\n30.12 Running a Regression Model While Excluding A Point",
    "text": "30.12 Running a Regression Model While Excluding A Point\nSuppose that we wanted to remove the point with the most influence over the model. We could fit a model to the data without id = 10214 to see the impact of this change. Let’s try it.\n\nsummary(lm(chol ~ sbp + bmi + dibpat, data = wcgs_500 |&gt; filter(id != 10214)))\n\n\nCall:\nlm(formula = chol ~ sbp + bmi + dibpat, data = filter(wcgs_500, \n    id != 10214))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-113.062  -25.453   -2.469   23.954  186.200 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  206.3207    20.3647  10.131  &lt; 2e-16 ***\nsbp            0.2464     0.1124   2.192  0.02883 *  \nbmi           -0.3127     0.7438  -0.420  0.67433    \ndibpatType B -10.6424     3.5635  -2.986  0.00296 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39.57 on 495 degrees of freedom\nMultiple R-squared:  0.02943,   Adjusted R-squared:  0.02355 \nF-statistic: 5.003 on 3 and 495 DF,  p-value: 0.001996\n\n\nCompare these results to those obtained with the full data set, shown below.\n\nHow is the model affected by removing subject 10214?\nWhat is the impact on the slopes?\nOn the summary measures? Residuals?\n\n\nsummary(lm(chol ~ sbp + bmi + dibpat, data = wcgs_500))\n\n\nCall:\nlm(formula = chol ~ sbp + bmi + dibpat, data = wcgs_500)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-113.133  -25.689   -1.932   24.036  186.390 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  214.5185    19.8825  10.789  &lt; 2e-16 ***\nsbp            0.2361     0.1125   2.099  0.03634 *  \nbmi           -0.5894     0.7290  -0.808  0.41921    \ndibpatType B -11.0408     3.5643  -3.098  0.00206 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39.66 on 496 degrees of freedom\nMultiple R-squared:  0.02954,   Adjusted R-squared:  0.02367 \nF-statistic: 5.033 on 3 and 496 DF,  p-value: 0.001915\n\n\nWhile it is true that we can sometimes improve the performance of the model in some ways by removing this point, there’s no good reason to do so. We can’t just remove a point from the data set without a good reason (and, to be clear, “I ran my model and it doesn’t fit this point well” is NOT a good reason). Good reasons would include:\n\nThis observation was included in the sample in error, for instance, because the subject was not eligible.\nAn error was made in transcribing the value of this observation to the final data set.\nAnd, sometimes, even “This observation is part of a meaningful subgroup of patients that I had always intended to model separately…” assuming that’s true.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "30-regr_diagnostics.html#summarizing-regression-diagnostics-for-431",
    "href": "30-regr_diagnostics.html#summarizing-regression-diagnostics-for-431",
    "title": "30  Regression Diagnostics",
    "section": "\n30.13 Summarizing Regression Diagnostics for 431",
    "text": "30.13 Summarizing Regression Diagnostics for 431\n\nCheck the “straight enough” condition with scatterplots of the y variable (outcome) against each x-variable (predictor), usually via the top row of a scatterplot matrix.\n\nIf the data are straight enough (that is, if it looks like the regression model is plausible), fit a regression model, to obtain residuals and influence measures.\n\nIf not, consider using the Box-Cox approach to identify a possible transformation for the outcome variable, and then recheck the straight enough condition.\n\n\n\nThe plot function for a fitted linear model builds five diagnostic plots.\n\n\n[Plot 1] A scatterplot of the residuals against the fitted values.\n\nThis plot should look patternless. Check in particular for any bend (which would suggest that the data weren’t all that straight after all) and for any thickening, which would indicate non-constant variance.\nIf the data are measured over time, check especially for evidence of patterns that might suggest they are not independent. For example, plot the residuals against time to look for patterns.\nThese values are stored as .resid and .fitted by the augment() function applied to a linear model.\n\n\n[Plot 3] A scale-location plot of the square root of the standardized residuals against the fitted values to look for a non-flat loess smooth, which indicates non-constant variance. Standardized residuals are obtained via augment and stored in the .std.resid variable.\n[Plot 2] If the plots above look OK, then consider a Normal Q-Q plot of the standardized residuals to check the nearly Normal condition.\n\n[Plot 5] The final plot we often look at is the plot of residuals vs. leverage, with influence contours. Sometimes, we’ll also look at [Plot 4] the index plot of the Cook’s distance for each observation in the data set.\n\nTo look for points with substantial leverage on the model by virtue of having unusual values of the predictors - look for points whose leverage is at least 3 times as large as the average leverage value.\n\nThe average leverage is always k/n, where k is the number of coefficients fit by the model (including the slopes and intercept), and n is the number of observations in the model.\nTo obtain the leverage values, use the augment() function which stores them in .hat.\n\n\nTo look for points with substantial influence on the model, that is, removing them from the model would change it substantially, consider the Cook’s distance, plotted in contours in Plot 5, or in an index plot in Plot 4.\n\nAny Cook’s d &gt; 1 will likely have a substantial impact on the model.\nEven points with Cook’s d &gt; 0.5 may merit further investigation.\nFind the Cook’s distances using the augment() function, which stores them in .cooksd.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "30-regr_diagnostics.html#violated-assumptions-problems-with-linearity",
    "href": "30-regr_diagnostics.html#violated-assumptions-problems-with-linearity",
    "title": "30  Regression Diagnostics",
    "section": "\n30.14 Violated Assumptions: Problems with Linearity",
    "text": "30.14 Violated Assumptions: Problems with Linearity\nSo what do serious assumption violations look like, and what can we do about them?\nHere is a simulated example that shows a clear problem with non-linearity.\n\nset.seed(4311); x1 &lt;- rnorm(n = 100, mean = 15, sd = 5)\nset.seed(4312); x2 &lt;- rnorm(n = 100, mean = 10, sd = 5)\nset.seed(4313); e1 &lt;- rnorm(n = 100, mean = 0, sd = 15)\ny &lt;- 15 + x1 + x2^2 + e1\nviol1 &lt;- data.frame(outcome = y, pred1 = x1, pred2 = x2) |&gt; tibble()\nmodel_1 &lt;- lm(outcome ~ pred1 + pred2, data = viol1)\nplot(model_1, which = 1)\n\n\n\n\n\n\n\nIn light of this, I would be looking for a potential transformation of outcome. Does the Box-Cox plot make any useful suggestions?\n\nboxCox(model_1); powerTransform(model_1)\n\n\n\n\n\n\n\nEstimated transformation parameter \n       Y1 \n0.4414775 \n\n\nNote that if the outcome was negative, we would have to add some constant value to every outcome in order to get every outcome value to be positive, and Box-Cox to run. This suggests fitting a new model, using the square root of the outcome.\n\nmodel_2 &lt;- lm(sqrt(outcome) ~ pred1 + pred2, data = viol1)\nplot(model_2, which = 1)\n\n\n\n\n\n\n\nThis is meaningfully better in terms of curve, but now looks a bit fan-shaped, indicating a potential problem with heteroscedasticity. Let’s look at the scale-location plot for this model.\n\nplot(model_2, which = 3)\n\n\n\n\n\n\n\nThis definitely looks like there’s a trend down in this plot. So the square root transformation, by itself, probably hasn’t resolved assumptions sufficiently well. We’ll have to be very careful about our interpretation of the model.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "30-regr_diagnostics.html#problems-with-non-normality-an-influential-point",
    "href": "30-regr_diagnostics.html#problems-with-non-normality-an-influential-point",
    "title": "30  Regression Diagnostics",
    "section": "\n30.15 Problems with Non-Normality: An Influential Point",
    "text": "30.15 Problems with Non-Normality: An Influential Point\nWith 100 observations, a single value with a standardized residual above 3 is very surprising. In our initial model_1 here, we have a standardized residual value as large as 6, so we clearly have a problem with that outlier.\n\nplot(model_1, which = 2)\n\n\n\n\n\n\n\nShould we, perhaps, remove point 72, and try again? Only if we have a reason beyond “it was poorly fit” to drop that point. Is point 72 highly leveraged or influential?\n\nplot(model_1, which = 5)\n\n\n\n\n\n\n\nWhat if we drop this point (72) and fit our linear model again. Does this resolve our difficulty with the assumption of linearity?\n\nmodel_1_no72 &lt;- lm(outcome ~ pred1 + pred2, data = viol1[-72,])\npar(mfrow=c(1,2))\nplot(model_1, which = 1, caption = \"With Point 72\")\nplot(model_1_no72, which = 1, caption = \"Without Point 72\")\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nNo, it doesn’t. But what if we combine our outcome transformation with dropping point 72?\n\nmodel_2_no72 &lt;- lm(sqrt(outcome) ~ pred1 + pred2, data = viol1[-72,])\npar(mfrow=c(1,2))\nplot(model_2_no72, which = c(1,3))\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nNope. That still doesn’t alleviate the problem of heteroscedasticity very well. At least, we no longer have any especially influential points, nor do we have substantial non-Normality.\n\npar(mfrow=c(1,2))\nplot(model_2_no72, which = c(2,5))\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nAt this point, I would be considering potential transformations of the predictors, quite possibly fitting some sort of polynomial term or cubic spline term in the predictors, but I’ll leave that for discussion in 432.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "30-regr_diagnostics.html#problems-with-non-normality-skew",
    "href": "30-regr_diagnostics.html#problems-with-non-normality-skew",
    "title": "30  Regression Diagnostics",
    "section": "\n30.16 Problems with Non-Normality: Skew",
    "text": "30.16 Problems with Non-Normality: Skew\n\nset.seed(4314); x1 &lt;- rnorm(n = 100, mean = 15, sd = 5)\nset.seed(4315); x2 &lt;- rnorm(n = 100, mean = 10, sd = 5)\nset.seed(4316); e2 &lt;- rnorm(n = 100, mean = 3, sd = 5)\ny2 &lt;- 50 + x1 + x2 + e2^2\nviol2 &lt;- data.frame(outcome = y2, pred1 = x1, pred2 = x2) |&gt; tibble()\nmodel.3 &lt;- lm(outcome ~ pred1 + pred2, data = viol2)\nplot(model.3, which = 2)\n\n\n\n\n\n\n\nSkewed residuals often show up in strange patterns in the plot of residuals vs. fitted values, too, as in this case.\n\nplot(model.3, which = 1)\n\n\n\n\n\n\n\nClearly, we have some larger residuals on the positive side, but not on the negative side. Would an outcome transformation be suggested by Box-Cox?\n\nboxCox(model.3); powerTransform(model.3)\n\n\n\n\n\n\n\nEstimated transformation parameter \n       Y1 \n-1.438747 \n\n\nThe suggested transformation looks like the inverse of our outcome.\n\nmodel.4 &lt;- lm(1/outcome ~ pred1 + pred2, data = viol2)\nplot(model.4, which = 2)\n\n\n\n\n\n\n\nOK. That’s something of an improvement. How about the other residual plots with this transformation?\n\nplot(model.4, which = 1)\n\n\n\n\n\n\n\nThe impact of the skew is reduced, at least. I might well be satisfied enough with this, in practice.\n\n\n\n\nBock, David E., Paul F. Velleman, and Richard D. De Veaux. 2004. Stats: Modelling the World. Boston MA: Pearson Addison-Wesley.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Regression Diagnostics</span>"
    ]
  },
  {
    "objectID": "31-prediction_wcgs.html",
    "href": "31-prediction_wcgs.html",
    "title": "\n31  Building Prediction Models for wcgs\n",
    "section": "",
    "text": "31.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(broom)\nlibrary(car)\nlibrary(GGally)\nlibrary(ggrepel)\nlibrary(janitor)\nlibrary(kableExtra)\nlibrary(patchwork)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\nWe’ll also use some functions from the Hmisc and rms packages.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Building Prediction Models for `wcgs`</span>"
    ]
  },
  {
    "objectID": "31-prediction_wcgs.html#predicting-cholesterol-level-in-wcgs-again",
    "href": "31-prediction_wcgs.html#predicting-cholesterol-level-in-wcgs-again",
    "title": "\n31  Building Prediction Models for wcgs\n",
    "section": "\n31.2 Predicting Cholesterol Level in WCGS, Again",
    "text": "31.2 Predicting Cholesterol Level in WCGS, Again\nTo address these issues, I’ll again look at the wcgs data (Western Collaborative Group Study), described in Chapter 17, then again in Chapter 30.\n\nwcgs &lt;- read_csv(\"data/wcgs.csv\", show_col_types = FALSE)\n\nThis time, we’ll try to predict the variable chol on the basis of some subset of the following six predictors: age, bmi, sbp, dbp, smoke and dibpat.\nThe steps we’ll take are as follows.\n\nCheck the wcgs data for missing or out-of-range values in the variables under study, so we don’t regret it later. Make a decision about how to handle issues here.\nPartition the wcgs data into a training (development) sample of 2000 observations, and a test (holdout) sample of the remaining observations.\nUsing only the model development sample, fit three candidate models.\n\nModel A will predict chol using all six predictors.\nModel B will predict chol using five predictors, specifically age, bmi, dbp, dibpat and smoke.\nModel C will predict chol using only three predictors, specifically age, dbp and smoke\n\n\n\nCompare the fit quality of these models in the development sample to see if any of them is superior in terms of in-sample predictive quality.\nAssess regression assumptions in each of these models in the development sample.\nFinally moving to the holdout sample, compare the quality of predictions made by the models in terms of several criteria to see if any of the models (A, B or C) is clearly superior in terms of out-of-sample prediction.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Building Prediction Models for `wcgs`</span>"
    ]
  },
  {
    "objectID": "31-prediction_wcgs.html#checking-for-missing-or-problematic-values",
    "href": "31-prediction_wcgs.html#checking-for-missing-or-problematic-values",
    "title": "\n31  Building Prediction Models for wcgs\n",
    "section": "\n31.3 Checking for Missing or Problematic Values",
    "text": "31.3 Checking for Missing or Problematic Values\nSuppose that after consulting with clinical experts, we want to ensure that:\n\nall age values are between 39 and 59 years\nall bmi are between 15 and 50\nall sbp are between 80 and 250 mm Hg\nall dbp are between 50 and 200 mm Hg\nall values of sbp-dbp are at least 10 and no more than 90 mm Hg\nall values of chol are between 100 and 400 mg/dl\n\n\nHmisc::describe(wcgs |&gt; select(age, bmi, sbp, dbp, smoke, dibpat, chol))\n\nselect(wcgs, age, bmi, sbp, dbp, smoke, dibpat, chol) \n\n 7  Variables      3154  Observations\n--------------------------------------------------------------------------------\nage \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3154        0       21    0.996    46.28    6.256       39       40 \n     .25      .50      .75      .90      .95 \n      42       45       50       55       57 \n\nlowest : 39 40 41 42 43, highest: 55 56 57 58 59\n--------------------------------------------------------------------------------\nbmi \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3154        0      679        1    24.52    2.803    20.59    21.52 \n     .25      .50      .75      .90      .95 \n   22.96    24.39    25.84    27.45    28.73 \n\nlowest : 11.1906 15.6605 16.872  17.2163 17.2224\nhighest: 36.0425 37.2297 37.2481 37.6528 38.9474\n--------------------------------------------------------------------------------\nsbp \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3154        0       62    0.996    128.6    16.25      110      112 \n     .25      .50      .75      .90      .95 \n     120      126      136      148      156 \n\nlowest :  98 100 102 104 106, highest: 200 208 210 212 230\n--------------------------------------------------------------------------------\ndbp \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3154        0       42    0.992    82.02    10.51       68       70 \n     .25      .50      .75      .90      .95 \n      76       80       86       94      100 \n\nlowest :  58  60  62  64  66, highest: 125 129 130 136 150\n--------------------------------------------------------------------------------\nsmoke \n       n  missing distinct \n    3154        0        2 \n                      \nValue         No   Yes\nFrequency   1652  1502\nProportion 0.524 0.476\n--------------------------------------------------------------------------------\ndibpat \n       n  missing distinct \n    3154        0        2 \n                        \nValue      Type A Type B\nFrequency    1589   1565\nProportion  0.504  0.496\n--------------------------------------------------------------------------------\nchol \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n    3142       12      237        1    226.4    47.99    161.1    175.0 \n     .25      .50      .75      .90      .95 \n   197.2    223.0    253.0    280.0    302.0 \n\nlowest : 103 110 111 112 113, highest: 386 390 400 414 645\n--------------------------------------------------------------------------------\n\n\nHere are the issues I see:\n\nThe (chol) total cholesterol levels include 12 missing values (so we need to decide what to do about them) and values of 414 and 645 that I don’t think are plausible. Since chol is our outcome, I’m inclined to delete those 14 subjects from our analyses.\nThe bmi value of 11.9 is much smaller than all other values, and seems implausible, so I’m inclined to delete that subject, as well.\nAll of the age values are between 39 and 59, as they should be.\nThe dibpat and smoke binary variables each appear to be reasonable given the timing of the data collection (the early 1960s).\nThe blood pressures, individually, for sbp and dbp appear reasonable in terms of their ranges and have no missing data. It’s worth it to check to see that everyone has a sbp meaningfully larger than their dbp.\n\n\nwcgs |&gt; mutate(bp_diff = sbp - dbp) |&gt;\n    select(id, sbp, dbp, bp_diff) |&gt;\n    slice_min(bp_diff, n = 3)\n\n# A tibble: 3 × 4\n     id   sbp   dbp bp_diff\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1  3822   120   100      20\n2 11406   104    80      24\n3 10204   110    86      24\n\n\nThis looks fine. Are any combinations out of line in the other direction, with a difference of 90 or more?\n\nwcgs |&gt; mutate(bp_diff = sbp - dbp) |&gt;\n    select(id, sbp, dbp, bp_diff) |&gt;\n    filter(bp_diff &gt;= 90)\n\n# A tibble: 8 × 4\n     id   sbp   dbp bp_diff\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1  2172   208   102     106\n2 12453   196    86     110\n3 12280   190    98      92\n4 13268   170    80      90\n5  3546   188    90      98\n6  2078   188    86     102\n7 19078   230   118     112\n8 22025   210   108     102\n\n\nThis sbp - dbp value is called the pulse pressure. Given that these men did not (at enrollment) have heart disease, it is surprising to see so many values separated by more than what would be typical (about 40-60 mm Hg.)\nI’ll be a little aggressive in my cleaning and also drop these 8 subjects with sbp - dbp of 90 or more mm Hg. So, we need to deal with the missingness in chol, the strange bmi value and the strange chol value in addition to dropping subjects with sbp - dbp to get to a “clean” data set here.\n\nwcgs_ms &lt;- wcgs |&gt;\n    select(id, age, bmi, sbp, dbp, smoke, dibpat, chol) |&gt;\n    filter(complete.cases(id, age, bmi, sbp, \n                          dbp, smoke, dibpat, chol)) |&gt; # drops 12 subjects\n    filter(bmi &gt; 15) |&gt; # drops 1 subject\n    filter(sbp - dbp &lt; 90) |&gt; # drops 7 more subjects\n    filter(chol &lt;= 400) # drops 2 more subjects\n\nnrow(wcgs); nrow(wcgs_ms)\n\n[1] 3154\n\n\n[1] 3132\n\n\nSo we have dropped a total of 22 subjects from the data set. We could certainly consider imputing missing values of chol, but I am uncomfortable doing that for an outcome.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Building Prediction Models for `wcgs`</span>"
    ]
  },
  {
    "objectID": "31-prediction_wcgs.html#partitioning-the-wcgs_ms-sample",
    "href": "31-prediction_wcgs.html#partitioning-the-wcgs_ms-sample",
    "title": "\n31  Building Prediction Models for wcgs\n",
    "section": "\n31.4 Partitioning the wcgs_ms sample",
    "text": "31.4 Partitioning the wcgs_ms sample\n\nPartition the wcgs_ms tibble into a training (development) sample of 2000 observations, and a test (holdout) sample of the remaining observations.\n\nBefore we partition, it’s always a good idea to ensure that each of the subject identifiers (here, id) are unique.\nWe could simply count the number of rows in the data set and make sure it matches the number of distinct id values.\n\nc(nrow(wcgs_ms), n_distinct(wcgs_ms |&gt; select(id)))\n\n[1] 3132 3132\n\n\nLooks good. If you prefer, check directly with the identical() function, which should return TRUE.\n\nidentical(nrow(wcgs_ms), n_distinct(wcgs_ms |&gt; select(id)))\n\n[1] TRUE\n\n\nRemember to set a seed so that you can replicate the selection.\n\nset.seed(431)\n\nwcgs_ms_train &lt;- slice_sample(wcgs_ms, n = 2000)\n\nwcgs_ms_test &lt;- anti_join(wcgs_ms, wcgs_ms_train, by = \"id\")\n\nc(nrow(wcgs_ms), nrow(wcgs_ms_train), nrow(wcgs_ms_test))\n\n[1] 3132 2000 1132\n\n\nOK. This looks good. We have 2000 observations in the training sample, and the rest in the test sample.\nGiven a large sample size (at least 500 observations in the full data set) I would usually think about holding out somewhere between 20% and 30% of the data in this manner, but it’s OK to stray a bit from those bounds, as we have here, with 1132/3132 = 36% held out for the test sample.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Building Prediction Models for `wcgs`</span>"
    ]
  },
  {
    "objectID": "31-prediction_wcgs.html#should-we-transform-our-outcome",
    "href": "31-prediction_wcgs.html#should-we-transform-our-outcome",
    "title": "\n31  Building Prediction Models for wcgs\n",
    "section": "\n31.5 Should we transform our outcome?",
    "text": "31.5 Should we transform our outcome?\nConsider the Box-Cox approach, which we’ll check in our complete (model C) model.\n\nboxCox(lm(chol ~ age + bmi + sbp + dbp + smoke + dibpat, \n             data = wcgs_ms_train))\n\n\n\n\n\n\n\nIt looks like we might want to consider using the square root of the cholesterol level as our outcome. We’ll try that.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Building Prediction Models for `wcgs`</span>"
    ]
  },
  {
    "objectID": "31-prediction_wcgs.html#scatterplot-matrix-and-assessment-of-collinearity",
    "href": "31-prediction_wcgs.html#scatterplot-matrix-and-assessment-of-collinearity",
    "title": "\n31  Building Prediction Models for wcgs\n",
    "section": "\n31.6 Scatterplot Matrix and Assessment of Collinearity",
    "text": "31.6 Scatterplot Matrix and Assessment of Collinearity\n\nwcgs_ms_train &lt;- wcgs_ms_train |&gt; \n    mutate(sqrtchol = sqrt(chol))\n\nggpairs(wcgs_ms_train |&gt; select(age, bmi, sbp, dbp, smoke, dibpat, sqrtchol),\n        lower = list(combo = wrap(\"facethist\", binwidth = 0.5)), \n        title = \"2000 subjects in `wcgs_ms_train`\")\n\n\n\n\n\n\n\nWith so many observations and more than just a few predictors, it’s not always easy to parse this plot. We might have considered splitting it into two parts.\nLet’s check for collinearity. Unsurprisingly, sbp and dbp are highly correlated with each other. Does this give us problems with variance inflation? Again, we’ll consider the largest model we’ll fit.\n\nvif(lm(chol ~ age + bmi + sbp + dbp + smoke + dibpat, \n             data = wcgs_ms_train))\n\n     age      bmi      sbp      dbp    smoke   dibpat \n1.038393 1.157091 2.536938 2.578828 1.035868 1.016123 \n\n\nThe VIF is largest for sbp and dbp, suggesting that perhaps we won’t need them both.\nThere is also a vif function within the rms package.\n\nrms::vif(lm(chol ~ age + bmi + sbp + dbp + smoke + dibpat, \n             data = wcgs_ms_train))\n\n         age          bmi          sbp          dbp     smokeYes dibpatType B \n    1.038393     1.157091     2.536938     2.578828     1.035868     1.016123 \n\n\nSince we have candidate models using only dbp, let’s see how they fare.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Building Prediction Models for `wcgs`</span>"
    ]
  },
  {
    "objectID": "31-prediction_wcgs.html#fit-our-three-candidate-models",
    "href": "31-prediction_wcgs.html#fit-our-three-candidate-models",
    "title": "\n31  Building Prediction Models for wcgs\n",
    "section": "\n31.7 Fit our Three Candidate Models",
    "text": "31.7 Fit our Three Candidate Models\nDue to our choice to transform our outcome, we will now fit the following candidate models:\n\nModel A will predict sqrt(chol) using all six predictors.\nModel B will predict sqrt(chol) using five predictors, specifically age, bmi, dbp, dibpat and smoke.\nModel C will predict sqrt(chol) using only two predictors, specifically dbp and smoke\n\n\n\n31.7.1 Three Candidate Models\n\nmodelA &lt;- lm(sqrt(chol) ~ age + bmi + sbp + dbp + smoke + dibpat, \n             data = wcgs_ms_train)\nmodelB &lt;- lm(sqrt(chol) ~ age + bmi + dbp + smoke + dibpat, \n             data = wcgs_ms_train)\nmodelC &lt;- lm(sqrt(chol) ~ age + dbp + smoke, \n             data = wcgs_ms_train)\n\n\n31.7.2 Could we have fit other models?\nSure. One approach would have been to consider an automated variable selection tool, even though they’re all pretty terrible. Stepwise regression is a common, if poor, choice for this task. The problem is that there’s no good tool for this task, essentially, as we’ll discuss further in 432.\n\nA stepwise regression applied to modelA in the training sample with backwards elimination suggests our modelB.\n\n\nstep(modelA)\n\nStart:  AIC=1357.64\nsqrt(chol) ~ age + bmi + sbp + dbp + smoke + dibpat\n\n         Df Sum of Sq    RSS    AIC\n- sbp     1     0.359 3916.0 1355.8\n&lt;none&gt;                3915.6 1357.6\n- bmi     1     5.044 3920.6 1358.2\n- dibpat  1    10.321 3925.9 1360.9\n- age     1    22.217 3937.8 1367.0\n- dbp     1    28.014 3943.6 1369.9\n- smoke   1    46.322 3961.9 1379.2\n\nStep:  AIC=1355.82\nsqrt(chol) ~ age + bmi + dbp + smoke + dibpat\n\n         Df Sum of Sq    RSS    AIC\n&lt;none&gt;                3916.0 1355.8\n- bmi     1     4.858 3920.8 1356.3\n- dibpat  1    10.204 3926.2 1359.0\n- age     1    21.913 3937.9 1365.0\n- smoke   1    46.021 3962.0 1377.2\n- dbp     1    52.228 3968.2 1380.3\n\n\n\nCall:\nlm(formula = sqrt(chol) ~ age + bmi + dbp + smoke + dibpat, data = wcgs_ms_train)\n\nCoefficients:\n (Intercept)           age           bmi           dbp      smokeYes  \n    12.03138       0.01930       0.02074       0.01835       0.30687  \ndibpatType B  \n    -0.14393  \n\n\n\nA stepwise regression with forward selection from the intercept only model moving towards modelA in the training sample also suggests modelB.\n\n\nmin_model &lt;- lm(sqrt(chol) ~ 1, data = wcgs_ms_train)\nbiggest &lt;- formula(lm(sqrt(chol) ~ age + bmi + sbp + dbp + smoke + dibpat, \n             data = wcgs_ms_train))\nfwd_step &lt;- stats::step(min_model, direction = 'forward', scope = biggest)\n\nStart:  AIC=1430.36\nsqrt(chol) ~ 1\n\n         Df Sum of Sq    RSS    AIC\n+ dbp     1    83.336 4001.7 1391.1\n+ sbp     1    57.718 4027.3 1403.9\n+ smoke   1    39.557 4045.5 1412.9\n+ age     1    39.060 4046.0 1413.1\n+ dibpat  1    20.740 4064.3 1422.2\n+ bmi     1    19.788 4065.2 1422.7\n&lt;none&gt;                4085.0 1430.4\n\nStep:  AIC=1391.13\nsqrt(chol) ~ dbp\n\n         Df Sum of Sq    RSS    AIC\n+ smoke   1    45.953 3955.7 1370.0\n+ age     1    24.171 3977.5 1381.0\n+ dibpat  1    16.675 3985.0 1384.8\n&lt;none&gt;                4001.7 1391.1\n+ bmi     1     1.964 3999.7 1392.2\n+ sbp     1     0.766 4000.9 1392.8\n\nStep:  AIC=1370.04\nsqrt(chol) ~ dbp + smoke\n\n         Df Sum of Sq    RSS    AIC\n+ age     1   24.3512 3931.4 1359.7\n+ dibpat  1   13.4535 3942.3 1365.2\n+ bmi     1    4.7652 3951.0 1369.6\n&lt;none&gt;                3955.7 1370.0\n+ sbp     1    0.0080 3955.7 1372.0\n\nStep:  AIC=1359.69\nsqrt(chol) ~ dbp + smoke + age\n\n         Df Sum of Sq    RSS    AIC\n+ dibpat  1   10.5635 3920.8 1356.3\n+ bmi     1    5.2180 3926.2 1359.0\n&lt;none&gt;                3931.4 1359.7\n+ sbp     1    0.0906 3931.3 1361.6\n\nStep:  AIC=1356.3\nsqrt(chol) ~ dbp + smoke + age + dibpat\n\n       Df Sum of Sq    RSS    AIC\n+ bmi   1    4.8581 3916.0 1355.8\n&lt;none&gt;              3920.8 1356.3\n+ sbp   1    0.1735 3920.6 1358.2\n\nStep:  AIC=1355.82\nsqrt(chol) ~ dbp + smoke + age + dibpat + bmi\n\n       Df Sum of Sq    RSS    AIC\n&lt;none&gt;              3916.0 1355.8\n+ sbp   1   0.35901 3915.6 1357.6\n\n\nIt is tempting to conclude that modelB is somehow a good model, since both forwards and stepwise regression like modelB in our training sample given the predictors we fed to the algorithm. Resist that temptation. As you’ll see, even with the pretty simple validation strategy we’re going to use here, we won’t wind up selecting model B.\n\n31.7.3 Coefficients of our 3 models with tidy\n\nGiven the large sample size, let’s look at some 99% confidence intervals for our model coefficients. We’ll use tidy from the broom package.\n\ntidy(modelA, conf.int = TRUE, conf.level = 0.99) |&gt;\n    select(term, estimate, conf.low, conf.high, p.value) |&gt;\n    kbl(digits = 3) |&gt; kable_classic(full_width = F)\n\n\n\nterm\nestimate\nconf.low\nconf.high\np.value\n\n\n\n(Intercept)\n12.058\n10.933\n13.183\n0.000\n\n\nage\n0.019\n0.005\n0.034\n0.001\n\n\nbmi\n0.021\n-0.013\n0.055\n0.109\n\n\nsbp\n-0.001\n-0.010\n0.007\n0.669\n\n\ndbp\n0.020\n0.006\n0.034\n0.000\n\n\nsmokeYes\n0.310\n0.146\n0.475\n0.000\n\n\ndibpatType B\n-0.145\n-0.308\n0.018\n0.022\n\n\n\n\n\n\ntidy(modelB, conf.int = TRUE, conf.level = 0.99) |&gt;\n    select(term, estimate, conf.low, conf.high, p.value) |&gt;\n    kbl(digits = 3) |&gt; kable_classic(full_width = F)\n\n\n\nterm\nestimate\nconf.low\nconf.high\np.value\n\n\n\n(Intercept)\n12.031\n10.918\n13.145\n0.000\n\n\nage\n0.019\n0.004\n0.034\n0.001\n\n\nbmi\n0.021\n-0.013\n0.055\n0.116\n\n\ndbp\n0.018\n0.009\n0.028\n0.000\n\n\nsmokeYes\n0.307\n0.143\n0.470\n0.000\n\n\ndibpatType B\n-0.144\n-0.307\n0.019\n0.023\n\n\n\n\n\n\ntidy(modelC, conf.int = TRUE, conf.level = 0.99) |&gt;\n    select(term, estimate, conf.low, conf.high, p.value) |&gt;\n    kbl(digits = 3) |&gt; kable_classic(full_width = F)\n\n\n\nterm\nestimate\nconf.low\nconf.high\np.value\n\n\n\n(Intercept)\n12.243\n11.325\n13.160\n0\n\n\nage\n0.020\n0.005\n0.035\n0\n\n\ndbp\n0.021\n0.012\n0.029\n0\n\n\nsmokeYes\n0.305\n0.142\n0.467\n0\n\n\n\n\n\n\n31.7.4 ANOVA comparison of the 3 models\nSince model C is a subset of model B which is a subset of model A, we can compare these models with ANOVA tests.\n\nanova(modelC, modelB, modelA)\n\nAnalysis of Variance Table\n\nModel 1: sqrt(chol) ~ age + dbp + smoke\nModel 2: sqrt(chol) ~ age + bmi + dbp + smoke + dibpat\nModel 3: sqrt(chol) ~ age + bmi + sbp + dbp + smoke + dibpat\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)  \n1   1996 3931.4                             \n2   1994 3916.0  2    15.422 3.9247 0.0199 *\n3   1993 3915.6  1     0.359 0.1827 0.6691  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThere appears to be a detectable improvement (using \\(\\alpha = 0.05\\)) for model B as compared to model C, but not if we use \\(\\alpha = 0.01\\).\nThe improvement from model B to model A doesn’t appear to meet the standard for a statistically detectable impact based on this ANOVA comparison.\n\n31.7.5 Assessing Fit Quality of our 3 models with glance\n\n\nrepA &lt;- glance(modelA) |&gt; mutate(name = \"modelA\")\nrepB &lt;- glance(modelB) |&gt; mutate(name = \"modelB\")\nrepC &lt;- glance(modelC) |&gt; mutate(name = \"modelC\")\n\nfit_report &lt;- bind_rows(repA, repB, repC)\n\nfit_report |&gt; \n    select(name, r2 = r.squared, adjr2 = adj.r.squared, sigma, \n           AIC, BIC, nobs, df, df.res = df.residual) |&gt;\n    kbl(digits = c(0,4,4,3,0,0,0,0,0)) |&gt; kable_minimal()\n\n\n\nname\nr2\nadjr2\nsigma\nAIC\nBIC\nnobs\ndf\ndf.res\n\n\n\nmodelA\n0.0415\n0.0386\n1.402\n7035\n7080\n2000\n6\n1993\n\n\nmodelB\n0.0414\n0.0390\n1.401\n7034\n7073\n2000\n5\n1994\n\n\nmodelC\n0.0376\n0.0362\n1.403\n7037\n7065\n2000\n3\n1996\n\n\n\n\n\nOur conclusions are:\n\nModel A has the strongest \\(R^2\\) value, as it must because it contains the predictors in the other models, and \\(R^2\\) is greedy.\nModel B has a slightly stronger adjusted \\(R^2\\) and \\(\\sigma\\) than the other models, and also has the best performance according to AIC.\nModel C (the smallest of these models) shows the best in-sample BIC result.\nNone of the differences we observe between these models are particularly large, and none of the models are especially effective, as judged by \\(R^2\\).",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Building Prediction Models for `wcgs`</span>"
    ]
  },
  {
    "objectID": "31-prediction_wcgs.html#develop-residual-plots",
    "href": "31-prediction_wcgs.html#develop-residual-plots",
    "title": "\n31  Building Prediction Models for wcgs\n",
    "section": "\n31.8 Develop Residual Plots",
    "text": "31.8 Develop Residual Plots\nWe’ll use augment from the broom package to calculate our summaries of the quality of fit at the level of the individual observations, and the fact that our fitted values refer to the square root of cholesterol level is fine for assessing assumptions in-sample.\n\naug_A &lt;- augment(modelA, data = wcgs_ms_train)\naug_B &lt;- augment(modelB, data = wcgs_ms_train)\naug_C &lt;- augment(modelC, data = wcgs_ms_train)\n\n\n31.8.1 First Set of Residual Diagnostics (3 models)\nFor each model, we’ll start with the pair of plots that show us:\n\nthe residuals vs. the fitted values, to check for non-linearity and non-constant variance, as well as\n\nNormal Q-Q plots of the standardized residuals to check for important non-Normality.\n\n\np1a &lt;- ggplot(aug_A, aes(x = .fitted, y = .resid)) +\n    geom_point(alpha = 0.2) + \n    geom_point(data = aug_A |&gt; \n                   slice_max(abs(.resid), n = 5),\n               col = \"red\", size = 2) +\n    geom_text_repel(data = aug_A |&gt; \n                        slice_max(abs(.resid), n = 5),\n                    aes(label = id), col = \"red\") +\n    geom_abline(intercept = 0, slope = 0, lty = \"dashed\") +\n    geom_smooth(method = \"loess\", formula = y ~ x, span = 2, se = F) +\n    labs(title = \"Model A\",\n         x = \"Fitted sqrt(cholesterol)\", y = \"Residual\") \n\np1b &lt;- ggplot(aug_B, aes(x = .fitted, y = .resid)) +\n    geom_point(alpha = 0.2) + \n    geom_point(data = aug_B |&gt; \n                   slice_max(abs(.resid), n = 5),\n               col = \"red\", size = 2) +\n    geom_text_repel(data = aug_B |&gt; \n                        slice_max(abs(.resid), n = 5),\n                    aes(label = id), col = \"red\") +\n    geom_abline(intercept = 0, slope = 0, lty = \"dashed\") +\n    geom_smooth(method = \"loess\", formula = y ~ x, span = 2, se = F) +\n    labs(title = \"Model B\",\n         x = \"Fitted sqrt(cholesterol)\", y = \"Residual\")\n\np1c &lt;- ggplot(aug_C, aes(x = .fitted, y = .resid)) +\n    geom_point(alpha = 0.2) + \n    geom_point(data = aug_C |&gt; \n                   slice_max(abs(.resid), n = 5),\n               col = \"red\", size = 2) +\n    geom_text_repel(data = aug_C |&gt; \n                        slice_max(abs(.resid), n = 5),\n                    aes(label = id), col = \"red\") +\n    geom_abline(intercept = 0, slope = 0, lty = \"dashed\") +\n    geom_smooth(method = \"loess\", formula = y ~ x, span = 2, se = F) +\n    labs(title = \"Model C\",\n         caption = \"In each plot, 5 largest |residuals| highlighted in red.\",\n         x = \"Fitted sqrt(cholesterol)\", y = \"Residual\")\n\np2a &lt;- ggplot(aug_A, aes(sample = .std.resid)) +\n    geom_qq() + \n    geom_qq_line(col = \"red\") +\n    labs(title = \"Model A: Normal Q-Q Plot\",\n         y = \"Model A Standardized Residual\", \n         x = \"Standard Normal Quantiles\")\n\np2b &lt;- ggplot(aug_B, aes(sample = .std.resid)) +\n    geom_qq() + \n    geom_qq_line(col = \"red\") +\n    labs(title = \"Model B: Normal Q-Q Plot\",\n         y = \"Model B Standardized Residual\", \n         x = \"Standard Normal Quantiles\")\n\np2c &lt;- ggplot(aug_C, aes(sample = .std.resid)) +\n    geom_qq() + \n    geom_qq_line(col = \"red\") +\n    labs(title = \"Model C: Normal Q-Q Plot\",\n         y = \"Model C Standardized Residual\", \n         x = \"Standard Normal Quantiles\")\n\n(p1a + p2a) / (p1b + p2b) / (p1c + p2c) +\n    plot_annotation(\n        title = \"Residual Diagnostics: Set 1 for Models A, B, C\")\n\n\n\n\n\n\n\nIn each of these plots, I see no clear signs of substantial non-linearity or any substantial problems with the assumptions of constant variance or Normality. No standardized residuals are especially unusual, and there’s no sign of a substantial curve or fan shape in the plots of residuals vs. fitted values.\n\n31.8.2 Second Set of Residual Diagnostics (3 models)\nAs a second check on the assumption of non-constant variance, we’ll draw scale-location plots. As a second check on the potential for poorly fit or highly leveraged points to influence our results, we’ll also run the residuals vs. leverage plots.\n\np3a &lt;- ggplot(aug_A, aes(x = .fitted, y = sqrt(abs(.std.resid)))) +\n  geom_point() + \n  geom_smooth(method = \"loess\", formula = y ~ x, span = 2, se = F) +\n  labs(title = \"Model A Scale-Location Plot\",\n       x = \"Fitted sqrt(cholesterol)\", \n       y = \"Square Root of |Standardized Residual|\")\n\np3b &lt;- ggplot(aug_B, aes(x = .fitted, y = sqrt(abs(.std.resid)))) +\n  geom_point() + \n  geom_smooth(method = \"loess\", formula = y ~ x, span = 2, se = F) +\n  labs(title = \"Model B Scale-Location Plot\",\n       x = \"Fitted sqrt(cholesterol)\", \n       y = \"Square Root of |Standardized Residual|\")\n\np3c &lt;- ggplot(aug_C, aes(x = .fitted, y = sqrt(abs(.std.resid)))) +\n  geom_point() + \n  geom_smooth(method = \"loess\", formula = y ~ x, span = 2, se = F) +\n  labs(title = \"Model C Scale-Location Plot\",\n       x = \"Fitted sqrt(cholesterol)\", \n       y = \"Square Root of |Standardized Residual|\")\n\n\np4a &lt;- ggplot(aug_A, aes(x = .hat, y = .std.resid)) +\n  geom_point() + \n  geom_point(data = aug_A |&gt; filter(.cooksd &gt;= 0.5),\n             col = \"red\", size = 2) +\n  geom_text_repel(data = aug_A |&gt; filter(.cooksd &gt;= 0.5),\n               aes(label = id), col = \"red\") +\n  geom_smooth(method = \"loess\", formula = y ~ x, span = 2, se = F) +\n  geom_vline(aes(xintercept = 3*mean(.hat)), lty = \"dashed\") +\n  labs(title = \"Model A Residuals vs. Leverage\",\n       caption = \"Red points indicate Cook's d at least 0.5\",\n       x = \"Leverage\", y = \"Standardized Residual\")\n\np4b &lt;- ggplot(aug_B, aes(x = .hat, y = .std.resid)) +\n  geom_point() + \n  geom_point(data = aug_B |&gt; filter(.cooksd &gt;= 0.5),\n             col = \"red\", size = 2) +\n  geom_text_repel(data = aug_B |&gt; filter(.cooksd &gt;= 0.5),\n               aes(label = id), col = \"red\") +\n  geom_smooth(method = \"loess\", formula = y ~ x, span = 2, se = F) +\n  geom_vline(aes(xintercept = 3*mean(.hat)), lty = \"dashed\") +\n  labs(title = \"Model B Residuals vs. Leverage\",\n       caption = \"Red points indicate Cook's d at least 0.5\",\n       x = \"Leverage\", y = \"Standardized Residual\")\n\np4c &lt;- ggplot(aug_C, aes(x = .hat, y = .std.resid)) +\n  geom_point() + \n  geom_point(data = aug_C |&gt; filter(.cooksd &gt;= 0.5),\n             col = \"red\", size = 2) +\n  geom_text_repel(data = aug_C |&gt; filter(.cooksd &gt;= 0.5),\n               aes(label = id), col = \"red\") +\n  geom_smooth(method = \"loess\", formula = y ~ x, span = 2, se = F) +\n  geom_vline(aes(xintercept = 3*mean(.hat)), lty = \"dashed\") +\n  labs(title = \"Model C Residuals vs. Leverage\",\n       caption = \"Red points indicate Cook's d at least 0.5\",\n       x = \"Leverage\", y = \"Standardized Residual\")\n\n(p3a + p4a) / (p3b + p4b) / (p3c + p4c) +\n    plot_annotation(\n        title = \"Residual Diagnostics: Set 2 for Models A, B, C\")\n\n\n\n\n\n\n\nAgain, I don’t see any signs of powerful trends in the scale-location plot for any of the three models, so there’s no clear sign of non-constant variance.\nFor each of the three models, the residuals vs. leverage plot shows no points highlighted in red (which would indicate substantial influence - a Cook’s distance value above 5.) So I don’t see anything to worry about.\n\n31.8.3 Numerical Summaries of these measures (all 3 models)\nWe could summarize the largest standardized residual and the largest Cook’s distance, for instance, for each model, with something like this.\n\naug_A |&gt; slice_max(abs(.std.resid), n = 1) |&gt;\n    select(id, sqrtchol, .fitted, .resid, .std.resid)\n\n# A tibble: 1 × 5\n     id sqrtchol .fitted .resid .std.resid\n  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 10340     19.7    14.7   5.08       3.63\n\n\n\naug_A |&gt; slice_max(.cooksd, n = 1) |&gt;\n    select(id, sqrtchol, .fitted, .resid, .hat, .cooksd)\n\n# A tibble: 1 × 6\n     id sqrtchol .fitted .resid    .hat .cooksd\n  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 10072     10.6    15.5  -4.90 0.00938  0.0167",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Building Prediction Models for `wcgs`</span>"
    ]
  },
  {
    "objectID": "31-prediction_wcgs.html#test-sample-comparisons-for-our-3-models",
    "href": "31-prediction_wcgs.html#test-sample-comparisons-for-our-3-models",
    "title": "\n31  Building Prediction Models for wcgs\n",
    "section": "\n31.9 Test Sample Comparisons for our 3 Models",
    "text": "31.9 Test Sample Comparisons for our 3 Models\nFinally, we’ll use our three candidate models (A, B and C) to predict the results in our holdout sample of 1132 observations not used to fit these models to see which model performs better in these new data.\nOnce again, we’ll be using augment to do this work. To start, this is straightforward - we just need to specify the new data we want to predict with newdata.\n\ntest_A &lt;- augment(modelA, newdata = wcgs_ms_test) |&gt; mutate(mod_n = \"Model A\")\ntest_B &lt;- augment(modelB, newdata = wcgs_ms_test) |&gt; mutate(mod_n = \"Model B\")\ntest_C &lt;- augment(modelC, newdata = wcgs_ms_test) |&gt; mutate(mod_n = \"Model C\")\n\nLet’s look at the first two results using Model A.\n\ntest_A |&gt; \n    mutate(sqrtchol = sqrt(chol)) |&gt;\n    select(chol, sqrtchol, .fitted, .resid) |&gt; head(2)\n\n# A tibble: 2 × 4\n   chol sqrtchol .fitted .resid\n  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1   194     13.9    15.2 -1.28 \n2   258     16.1    15.5  0.592\n\n\nAs we can see, the .fitted value is trying to predict the square root of chol and not chol. When we are doing validation in our test sample, it is more helpful to get back to the scale of our original outcome. So here, we’d square the .fitted value (to back out of our square root transformation) and get the actual prediction our model makes for chol. Then we can take the observed chol value and subtract the prediction of chol to get a new residual on the scale of our original chol values.\n\ntest_res &lt;- bind_rows(test_A, test_B, test_C) |&gt;\n    mutate(fit_chol = .fitted^2, res_chol = chol - fit_chol) |&gt;\n    select(mod_n, id, chol, fit_chol, res_chol, everything()) |&gt;\n    arrange(id, mod_n)\n\ntest_res |&gt; head()\n\n# A tibble: 6 × 13\n  mod_n      id  chol fit_chol res_chol   age   bmi   sbp   dbp smoke dibpat\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Model A  2003   181     214.    -33.2    42  23.6   110    78 No    Type B\n2 Model B  2003   181     214.    -32.7    42  23.6   110    78 No    Type B\n3 Model C  2003   181     216.    -35.0    42  23.6   110    78 No    Type B\n4 Model A  2004   132     222.    -89.9    41  23.1   124    78 Yes   Type B\n5 Model B  2004   132     222.    -89.9    41  23.1   124    78 Yes   Type B\n6 Model C  2004   132     224.    -92.5    41  23.1   124    78 Yes   Type B\n# ℹ 2 more variables: .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;\n\n\nNow, we can summarize the quality of the predictions across each of the models with four summary statistics calculated across the 1132 observations in the test sample.\n\nthe mean absolute prediction error, or MAPE\nthe median absolute prediction error, or medAPE\nthe maximum absolute prediction error, or maxAPE\nthe square root of the mean squared prediction error, or RMSPE\n\nNo one of these dominates the other, but we might be interested in which model gives us the best (smallest) result for each of these summaries. Let’s run them.\n\ntest_res |&gt; \n    group_by(mod_n) |&gt;\n    summarise(MAPE = mean(abs(res_chol)),\n              medAPE = median(abs(res_chol)),\n              maxAPE = max(abs(res_chol)),\n              RMSPE = sqrt(mean(res_chol^2)))\n\n# A tibble: 3 × 5\n  mod_n    MAPE medAPE maxAPE RMSPE\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Model A  32.2   27.1   151.  40.8\n2 Model B  32.2   27.1   150.  40.8\n3 Model C  32.1   26.5   147.  40.8\n\n\nWhich model has the best performance in the testing sample?\n\nModel C has the smallest (best) MAPE, median APE, maximum APE and RMSPE.\nThere are no enormous differences between the models on most of these summaries, but Model C (the smallest model) appears a bit better.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Building Prediction Models for `wcgs`</span>"
    ]
  },
  {
    "objectID": "31-prediction_wcgs.html#putting-it-together---which-model-do-we-like-best",
    "href": "31-prediction_wcgs.html#putting-it-together---which-model-do-we-like-best",
    "title": "\n31  Building Prediction Models for wcgs\n",
    "section": "\n31.10 Putting it Together - which model do we like best?",
    "text": "31.10 Putting it Together - which model do we like best?\nIt’s worth remembering that none of the three models was particularly strong. Even Model A (with the full set of predictors) had an \\(R^2\\) of only about 4.15%.\n\nAll 3 models had reasonable residual plots, so we don’t have much to choose from there.\nWithin the development sample, Model B had slightly better adjusted \\(R^2\\), \\(\\sigma\\) and AIC results than the other models, while Model C had the best BIC.\nModel C had the best results in the test sample, across all four summaries we examined.\n\nIn this case, I’d pick Model C from among these options, based mainly on the stronger test sample results and the fact that it’s a smaller model (with fewer predictors) and if none of the models are going to do very well in terms of predicting chol, there’s not much to choose from anyway.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Building Prediction Models for `wcgs`</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html",
    "href": "32-galapagos.html",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "",
    "text": "32.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(broom)\nlibrary(car)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\nWe’ll also use a function from the arm package, and from the Hmisc package.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#a-little-background",
    "href": "32-galapagos.html#a-little-background",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.2 A Little Background",
    "text": "32.2 A Little Background\nThe gala data describe describe features of the 30 Galapagos Islands.\n\ngala &lt;- read_csv(\"data/gala.csv\", show_col_types = FALSE)\n\nThe Galapagos Islands are found about 900 km west of South America: specifically the continental part of Ecuador. The Islands form a province of Ecuador and serve as a national park and marine reserve. They are noted for their vast numbers of unique (or endemic) species and were studied by Charles Darwin during the voyage of the Beagle.\n\n\ngalapic.jpg\n\n\n32.2.1 Sources\nThe data were initially presented by Johnson M and Raven P (1973) Species number and endemism: the Galapagos Archipelago revisited. Science 179: 893-895 and also appear in several regression texts, including my source: Faraway (2015). Note that Faraway filled in some missing data to simplify things a bit. A similar version of the data is available as part of the faraway library in R, but I encourage you to use the version I supply on our web site.\n\n32.2.2 Variables in the gala data frame\n\n\nid = island identification code\n\nisland = island name\n\nspecies = our outcome, the number of species found on the island\n\narea = the area of the island, in square kilometers\n\nelevation = the highest elevation of the island, in meters\n\nnearest = the distance from the nearest island, in kilometers\n\nscruz = the distance from Santa Cruz Island, in kilometers. Santa Cruz is the home to the largest human population in the Islands, and to the town of Puerto Ayora.\n\nadjacent = the area of the adjacent island, in square kilometers\n\n\ngala\n\n# A tibble: 30 × 8\n      id island       species  area elevation nearest scruz adjacent\n   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     1 Baltra            58 25.1        346     0.6   0.6     1.84\n 2     2 Bartolome         31  1.24       109     0.6  26.3   572.  \n 3     3 Caldwell           3  0.21       114     2.8  58.7     0.78\n 4     4 Champion          25  0.1         46     1.9  47.4     0.18\n 5     5 Coamano            2  0.05        77     1.9   1.9   904.  \n 6     6 Daphne.Major      18  0.34       119     8     8       1.84\n 7     7 Daphne.Minor      24  0.08        93     6    12       0.34\n 8     8 Darwin            10  2.33       168    34.1 290.      2.85\n 9     9 Eden               8  0.03        71     0.4   0.4    18.0 \n10    10 Enderby            2  0.18       112     2.6  50.2     0.1 \n# ℹ 20 more rows\n\nHmisc::describe(gala) # check for missing and inexplicable values\n\ngala \n\n 8  Variables      30  Observations\n--------------------------------------------------------------------------------\nid \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n      30        0       30        1     15.5    10.33     2.45     3.90 \n     .25      .50      .75      .90      .95 \n    8.25    15.50    22.75    27.10    28.55 \n\nlowest :  1  2  3  4  5, highest: 26 27 28 29 30\n--------------------------------------------------------------------------------\nisland \n       n  missing distinct \n      30        0       30 \n\nlowest : Baltra     Bartolome  Caldwell   Champion   Coamano   \nhighest: SantaFe    SantaMaria Seymour    Tortuga    Wolf      \n--------------------------------------------------------------------------------\nspecies \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n      30        0       27    0.999    85.23    109.5      2.0      2.9 \n     .25      .50      .75      .90      .95 \n    13.0     42.0     96.0    280.5    319.1 \n\nlowest :   2   3   5   8  10, highest: 237 280 285 347 444\n--------------------------------------------------------------------------------\narea \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n      30        0       29        1    261.7    478.6   0.0390   0.0770 \n     .25      .50      .75      .90      .95 \n  0.2575   2.5900  59.2375 578.5460 782.6215 \n                                                          \nValue          0    50   100   150   550   600   900  4650\nFrequency     21     2     1     1     2     1     1     1\nProportion 0.700 0.067 0.033 0.033 0.067 0.033 0.033 0.033\n\nFor the frequency table, variable is rounded to the nearest 50\n--------------------------------------------------------------------------------\nelevation \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n      30        0       30        1      368    411.1    47.35    68.80 \n     .25      .50      .75      .90      .95 \n   97.75   192.00   435.25   868.20  1229.40 \n\nlowest :   25   46   49   71   76, highest:  777  864  906 1494 1707\n--------------------------------------------------------------------------------\nnearest \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n      30        0       22    0.997    10.06    13.73    0.445    0.590 \n     .25      .50      .75      .90      .95 \n   0.800    3.050   10.025   34.100   40.205 \n\nlowest : 0.2  0.4  0.5  0.6  0.7 , highest: 16.5 29.1 34.1 45.2 47.4\n--------------------------------------------------------------------------------\nscruz \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n      30        0       29        1    56.98    65.17     0.49     0.60 \n     .25      .50      .75      .90      .95 \n   11.02    46.65    81.08    97.73   193.90 \n\nlowest : 0     0.4   0.6   1.9   8    , highest: 93.1  95.3  119.6 254.7 290.2\n--------------------------------------------------------------------------------\nadjacent \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n      30        0       21    0.998    261.1    477.8     0.10     0.10 \n     .25      .50      .75      .90      .95 \n    0.52     2.59    59.24   578.55   782.62 \n                                                    \nValue          0    50   100   550   600   900  4650\nFrequency     21     2     2     2     1     1     1\nProportion 0.700 0.067 0.067 0.067 0.033 0.033 0.033\n\nFor the frequency table, variable is rounded to the nearest 50\n--------------------------------------------------------------------------------",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#dtdp-a-scatterplot-matrix",
    "href": "32-galapagos.html#dtdp-a-scatterplot-matrix",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.3 DTDP: A Scatterplot Matrix",
    "text": "32.3 DTDP: A Scatterplot Matrix\nAfter missingness and range checks, the first step in any data analysis problem is to draw the picture. The most useful picture for me in thinking about a regression problem with a reasonably small number of predictors is a scatterplot matrix.\nOur outcome, that we are predicting here is the number of species.\nWe’ll use five predictors:\n\narea\nelevation\nnearest\n\nscruz and\n\nadjacent.\n\n\nggpairs(gala |&gt; select(area, elevation, nearest,\n                        scruz, adjacent, species))\n\n\n\n\n\n\n\n\n32.3.1 Questions about the Scatterplot Matrix\nIn this Chapter, I will provide you with nearly 100 questions that you should be able to answer in light of the output provided. Here are the first few. If you have questions about how to interpret the output in light of these questions, please ask about them on Piazza or in TA office hours.\n\nWhat are we looking for in the scatterplots in the bottom row?\nWhat can we learn from the Pearson correlations in the right column?\nHow do the density plots help increase our understanding of the data?\nWhat about the scatterplots that are not in the top row?\nWhat can we learn from the Pearson correlations that compare predictors?",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#fitting-a-kitchen-sink-linear-regression-model",
    "href": "32-galapagos.html#fitting-a-kitchen-sink-linear-regression-model",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.4 Fitting A “Kitchen Sink” Linear Regression model",
    "text": "32.4 Fitting A “Kitchen Sink” Linear Regression model\nNext, we’ll fit a multiple linear regression model to predict the number of species based on the five predictors included in the gala data frame (and scatterplot matrix above.) We use the lm command to fit the linear model, and use what is called Wilkinson-Rogers notation to specify the model.\n\nmodel1 &lt;- lm(species ~ area + elevation + nearest + scruz + \n               adjacent, data=gala)\n\nHere are the results of running tidy() and glance() from the broom package on model1.\n\ntidy(model1) |&gt;\n    kable(digits = 3)\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n7.068\n19.154\n0.369\n0.715\n\n\narea\n-0.024\n0.022\n-1.068\n0.296\n\n\nelevation\n0.319\n0.054\n5.953\n0.000\n\n\nnearest\n0.009\n1.054\n0.009\n0.993\n\n\nscruz\n-0.241\n0.215\n-1.117\n0.275\n\n\nadjacent\n-0.075\n0.018\n-4.226\n0.000\n\n\n\n\nglance(model1) |&gt;\n    select(r.squared, adj.r.squared, sigma, statistic, \n           p.value, df, df.residual, nobs, AIC, BIC) |&gt;\n    kable(digits = c(3, 3, 1, 1, 3, 0, 0, 0, 1, 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\ndf.residual\nnobs\nAIC\nBIC\n\n\n0.766\n0.717\n61\n15.7\n0\n5\n24\n30\n339.1\n348.9\n\n\n\n\nHere is the result of running summary on model1.\n\nsummary(model1)\n\n\nCall:\nlm(formula = species ~ area + elevation + nearest + scruz + adjacent, \n    data = gala)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-111.679  -34.898   -7.862   33.460  182.584 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.068221  19.154198   0.369 0.715351    \narea        -0.023938   0.022422  -1.068 0.296318    \nelevation    0.319465   0.053663   5.953 3.82e-06 ***\nnearest      0.009144   1.054136   0.009 0.993151    \nscruz       -0.240524   0.215402  -1.117 0.275208    \nadjacent    -0.074805   0.017700  -4.226 0.000297 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 60.98 on 24 degrees of freedom\nMultiple R-squared:  0.7658,    Adjusted R-squared:  0.7171 \nF-statistic:  15.7 on 5 and 24 DF,  p-value: 6.838e-07\n\n\n\n32.4.1 Questions about the Kitchen Sink Model Summaries\nWhat conclusions can we draw from the summary output for this model? Specifically …\n\nWhat is being predicted? What is the prediction equation?\nHow do we interpret the elevation estimate of 0.32?\nHow do we interpret the area estimate of -0.02?\nHow do we interpret the intercept estimate of 7.07?\nOverall, does our model1 add detectable predictive value (say, at the 5% significance level) over the simplest possible model for this outcome (the model using the intercept term alone)?\nWhat proportion of the variation in species counts does this model account for?\nWhat does the residual standard error mean in this context?\nWhat can we learn from the standard errors in the coefficient output?\nWhat can we learn from the t values and Pr(&gt;|t|) values in the summary output?\nHow should we interpret the meaning of the Adjusted R-squared value?",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#finding-confidence-intervals-for-our-coefficient-estimates",
    "href": "32-galapagos.html#finding-confidence-intervals-for-our-coefficient-estimates",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.5 Finding Confidence Intervals for our Coefficient Estimates",
    "text": "32.5 Finding Confidence Intervals for our Coefficient Estimates\nOf course, we can find these intervals in several ways. One approach uses tidy() from the broom package.\n\ntidy(model1, conf.int = TRUE, conf.level = 0.95) |&gt;\n    select(term, estimate, std.error, conf.low, conf.high) |&gt;\n    kable(digits = 3)\n\n\n\nterm\nestimate\nstd.error\nconf.low\nconf.high\n\n\n\n(Intercept)\n7.068\n19.154\n-32.464\n46.601\n\n\narea\n-0.024\n0.022\n-0.070\n0.022\n\n\nelevation\n0.319\n0.054\n0.209\n0.430\n\n\nnearest\n0.009\n1.054\n-2.166\n2.185\n\n\nscruz\n-0.241\n0.215\n-0.685\n0.204\n\n\nadjacent\n-0.075\n0.018\n-0.111\n-0.038\n\n\n\n\n\nAnother approach would be:\n\nconfint(model1, level = 0.95)\n\n                  2.5 %      97.5 %\n(Intercept) -32.4641006 46.60054205\narea         -0.0702158  0.02233912\nelevation     0.2087102  0.43021935\nnearest      -2.1664857  2.18477363\nscruz        -0.6850926  0.20404416\nadjacent     -0.1113362 -0.03827344\n\n\n\n32.5.1 Questions about the Confidence Intervals\n\nWhat can we learn from the provided confidence interval for elevation?\nHow do the confidence interval results here compare to the t tests we saw previously?",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#measuring-collinearity---the-variance-inflation-factor",
    "href": "32-galapagos.html#measuring-collinearity---the-variance-inflation-factor",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.6 Measuring Collinearity - the Variance Inflation Factor",
    "text": "32.6 Measuring Collinearity - the Variance Inflation Factor\nThe variance inflation factor (abbreviated VIF) can be used to quantify the impact of multicollinearity in a linear regression model.\nThe VIF is sometimes interpreted by taking its square root, and then interpreting the result as telling you how much larger the standard error for that coefficient is, as compared to what it would be if that variable were uncorrelated with the other predictors.\nIn R, the vif function from the car library, when applied to a linear regression model, specifies the variance inflation factors for each of the model’s coefficients, as follows.\n\nvif(model1)\n\n     area elevation   nearest     scruz  adjacent \n 2.928145  3.992545  1.766099  1.675031  1.826403 \n\n\nSo, for instance, the VIF of 3.99 for elevation implies that the standard error of the elevation coefficient is approximately 2 times larger than it would be if elevation was uncorrelated with the other predictors.\nI will look closely at any VIF value that is greater than 5, although some people use a cutoff of 10.\n\nAnother collinearity measure called tolerance is simply 1/VIF.\nFor example, the tolerance for elevation would be 0.25, and the cutoff for a potentially problematic tolerance is either 0.2 or lower, or 0.1 or lower.\n\nTo calculate the VIF for a predictor \\(x_1\\), use all of the other predictors to predict \\(x_1\\) and find the multiple \\(R^2\\) value.\n\n\nVIF for \\(x_1\\) = 1 / (1 - \\(R^2_{x_1 | others}\\)), and tolerance = (1 - \\(R^2_{x_1 | others}\\)).",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#global-f-testing-of-overall-significance",
    "href": "32-galapagos.html#global-f-testing-of-overall-significance",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.7 Global (F) Testing of Overall Significance",
    "text": "32.7 Global (F) Testing of Overall Significance\nOur Galapagos Islands species count regression model (called model1) predicts the count of an island’s species using area, elevation, nearest, scruz and adjacent.\n\nnullmodel &lt;- lm(species ~ 1, data=gala)\nsummary(nullmodel)\n\n\nCall:\nlm(formula = species ~ 1, data = gala)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-83.23 -72.23 -43.23  10.77 358.77 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    85.23      20.93   4.072 0.000328 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 114.6 on 29 degrees of freedom\n\nanova(model1, nullmodel)\n\nAnalysis of Variance Table\n\nModel 1: species ~ area + elevation + nearest + scruz + adjacent\nModel 2: species ~ 1\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     24  89231                                  \n2     29 381081 -5   -291850 15.699 6.838e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n32.7.1 Questions about the Global Test via ANOVA\n\nHow do we interpret the null model fit above?\nWhat are the hypotheses being tested by this ANOVA output?\nWhat conclusions can we draw from the ANOVA output presented here?\nWhere do we find information regarding the result for the previous question in the summary output for the linear model?\nHow would we set up an ANOVA model to test whether the “kitchen sink” model’s predictive value would be meaningfully impacted by removing the adjacent predictor from the model?\nWhere do we find information regarding these result for the previous question in the summary output for the linear model?\nHow would we set an ANOVA model to test whether a model with area only would be a detectable improvement over the null model?",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#sequential-testing-in-a-regression-model-with-anova",
    "href": "32-galapagos.html#sequential-testing-in-a-regression-model-with-anova",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.8 Sequential Testing in a Regression Model with ANOVA",
    "text": "32.8 Sequential Testing in a Regression Model with ANOVA\n\nanova(model1)\n\nAnalysis of Variance Table\n\nResponse: species\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \narea       1 145470  145470 39.1262 1.826e-06 ***\nelevation  1  65664   65664 17.6613 0.0003155 ***\nnearest    1     29      29  0.0079 0.9300674    \nscruz      1  14280   14280  3.8408 0.0617324 .  \nadjacent   1  66406   66406 17.8609 0.0002971 ***\nResiduals 24  89231    3718                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n32.8.1 Questions about Sequential Testing and ANOVA\n\nWhat conclusions can we draw from the area row in the output above?\nWhat conclusions can we draw from the elevation row?\nDoes nearest add statistically detectable predictive value to the model including area and elevation, but none of the other predictors?\nDoes adjacent add detectable predictive value as last predictor into the model?\nWhere else in the regression output can we find the answer to the previous question?\nHow does the mean square of the residuals (3718) relate to the residual standard error?\nWhat percentage of the variation in the species counts is accounted for by area alone?\nWhat percentage of the variation explained by the kitchen sink model would also be accounted for in a two-predictor regression model including area and elevation alone?\nHow could we use the original linear model output to whether a model using the four predictors that appear most promising here would be statistically meaningfully worse than the full model with all five predictors?\nWhat does the following output do differently than the output above, and why is that potentially useful here? Why is the p value for scruz so different?\n\n\nanova(lm(species ~ area + elevation + adjacent + \n             scruz + nearest, data=gala))\n\nAnalysis of Variance Table\n\nResponse: species\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \narea       1 145470  145470 39.1262 1.826e-06 ***\nelevation  1  65664   65664 17.6613 0.0003155 ***\nadjacent   1  73171   73171 19.6804 0.0001742 ***\nscruz      1   7544    7544  2.0291 0.1671885    \nnearest    1      0       0  0.0001 0.9931506    \nResiduals 24  89231    3718                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nConsider the ANOVA below, run on a new model with elevation after adjacent. What happens? Why?\n\n\nanova(lm(species ~ area + adjacent + elevation + \n             scruz + nearest, data=gala))\n\nAnalysis of Variance Table\n\nResponse: species\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \narea       1 145470  145470 39.1262 1.826e-06 ***\nadjacent   1   2850    2850  0.7666    0.3900    \nelevation  1 135985  135985 36.5751 3.030e-06 ***\nscruz      1   7544    7544  2.0291    0.1672    \nnearest    1      0       0  0.0001    0.9932    \nResiduals 24  89231    3718                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#an-anova-table-for-the-model-as-a-whole",
    "href": "32-galapagos.html#an-anova-table-for-the-model-as-a-whole",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.9 An ANOVA table for the Model as a Whole",
    "text": "32.9 An ANOVA table for the Model as a Whole\nIt’s probably also worthwhile to compute a completed ANOVA table for the model as a whole. All elements are in the ANOVA tables above, or the model summary.\n\n\nGroup\nDF\nSS\nMS\nF\nP\n\n\n\nRegression\n5\n291849\n58369.8\n15.7\n6.838e-07\n\n\nResiduals\n24\n89231\n3718.0\n\n\n\n\nTotal\n29\n381080\n\n\n\n\n\n\n\nHow did I determine the Mean Square for the Regression model?\nWhat conclusions can we draw from this ANOVA table?",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#assumption-checking-for-our-galapagos-islands-models",
    "href": "32-galapagos.html#assumption-checking-for-our-galapagos-islands-models",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.10 Assumption Checking for our Galapagos Islands models",
    "text": "32.10 Assumption Checking for our Galapagos Islands models\nRemember that the key assumptions of multiple linear regression are:\n\n[Linearity] We have also assumed that the structural part of the model is correctly specified (we’ve included all the predictors that need to be in the model, and expressed them in the most appropriate manner, and we’ve left out any predictors that don’t need to be in the model.)\n[Normality] The regression makes errors that come from a Normal distribution\n[Homoscedasticity = Constant Variance] The regression makes errors that come from a distribution with constant variance at all predictor levels.\n[Independence] The regression errors are independent of each other.\n\nIn addition, we need to realize that sometimes a few observations may be particularly problematic. For instance:\n\nAn observation may simply not fit the model well (i.e. it creates a large residual)\nAn observation may have high leverage over the fit of the model (this happens with observations that have an unusual combination of predictor values, in particular)\nAn observation may actually have high influence on the model (in the sense that whether they are included or excluded has a large impact on the model’s fit, and the value of its parameter estimates.)\nOr any combination of high residual, leverage and influence may occur.\n\nSo it is important to check the assumptions that we can with the data we have. Our most important tools are plots and other summaries of residuals, and what are called influence statistics.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#my-first-plot-studentized-residuals-vs.-fitted-values",
    "href": "32-galapagos.html#my-first-plot-studentized-residuals-vs.-fitted-values",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.11 My First Plot: Studentized Residuals vs. Fitted Values",
    "text": "32.11 My First Plot: Studentized Residuals vs. Fitted Values\nThe first diagnostic plot I usually draw for a multiple regression is a scatterplot of the model’s studentized residuals (on the vertical axis) vs. the model’s fitted values (on the horizontal.) This plot can be used to assess potential non-linearity, non-constant variance, and non-Normality in the residuals.\n\ngala$stures &lt;- rstudent(model1); gala$fits &lt;- fitted(model1)\nggplot(gala, aes(x = fits, y = stures)) +\n    geom_point(size = 3, shape = 1) +\n    geom_smooth(col = \"blue\", se = FALSE, method = \"loess\",\n                formula = y ~ x, weight = 0.5) +\n    geom_hline(aes(yintercept = 0), linetype = \"dashed\") +\n    labs(x = \"Fitted Values of Species\", \n         y = \"Studentized Residuals\",\n         title = \"gala data: Model 1\",\n         subtitle = \"Studentized Residuals vs. Fitted Values\")\n\n\n\n\n\n\n\n\n32.11.1 Questions about Studentized Residuals vs. Fitted Values\n\nConsider the point at bottom right. What can you infer about this observation?\nWhy did I include the dotted horizontal line at Studentized Residual = 0?\nWhat is the purpose of the thin blue line?\nWhat does this plot suggest about the potential for outliers in the residuals?",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#automatic-regression-diagnostics-for-model-1",
    "href": "32-galapagos.html#automatic-regression-diagnostics-for-model-1",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.12 Automatic Regression Diagnostics for Model 1",
    "text": "32.12 Automatic Regression Diagnostics for Model 1\n\npar(mfrow=c(2,2))\nplot(model1)\n\n\n\n\n\n\npar(mfrow=c(1,1))",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#model-1-diagnostic-plot-1",
    "href": "32-galapagos.html#model-1-diagnostic-plot-1",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.13 Model 1: Diagnostic Plot 1",
    "text": "32.13 Model 1: Diagnostic Plot 1\nAs we’ve seen, the first of R’s automated diagnostic plots for a linear model is a plot of the residuals vs. the fitted values.\n\nplot(model1, which=1)\n\n\n\n\n\n\n\n\n32.13.1 Questions about Diagnostic Plot 1: Residuals vs. Fitted Values\n\nWhat type of regression residuals is R plotting here?\nWhich points are identified by numbers here?\nWhy did R include the gray dotted line at Residual = 0?\nWhat is the purpose of the thin red line?\nWhat can you tell about the potential for outliers in the model 1 residuals from the plot?\nWhat are we looking for in this plot that would let us conclude there were no important assumption violations implied by it? Which assumptions can we assess with it?\nWhat would we do if we saw a violation of assumptions in this plot?\nWhat are the key differences between this plot and the one I showed earlier?",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#diagnostic-plot-2-assessing-normality",
    "href": "32-galapagos.html#diagnostic-plot-2-assessing-normality",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.14 Diagnostic Plot 2: Assessing Normality",
    "text": "32.14 Diagnostic Plot 2: Assessing Normality\nThe second diagnostic plot prepared by R for any linear model using the plot command is a Normal Q-Q plot of the standardized residuals from the model.\n\nplot(model1, which=2)\n\n\n\n\n\n\n\n\n32.14.1 Questions about Diagnostic Plot 2: Normal Plot of Standardized Residuals\n\nWhich points are being identified here by number?\nWhich assumption(s) of multiple regression does this plot help us check?\nWhat are we looking for in this plot that would let us conclude there were no important assumption violations implied by it?\nWhat would we do if we saw a violation of assumptions in this plot?\n\nWe could also look at studentized residuals, or we could apply a more complete set of plots and other assessments of normality. Usually, I don’t.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#diagnostic-plot-3-assessing-constant-variance",
    "href": "32-galapagos.html#diagnostic-plot-3-assessing-constant-variance",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.15 Diagnostic Plot 3: Assessing Constant Variance",
    "text": "32.15 Diagnostic Plot 3: Assessing Constant Variance\nThe third diagnostic plot prepared by R for any linear model using the plot command shows the square root of the model’s standardized residuals vs. its fitted values. R calls this a scale-location plot.\n\nplot(model1, which=3)\n\n\n\n\n\n\n\n\n32.15.1 Questions about Diagnostic Plot 3: Scale-Location Plot\n\nWhich points are being identified here by number?\nWhich assumption(s) of multiple regression does this plot help us check?\nWhat is the role of the thin red line in this plot?\nWhat are we looking for in this plot that would let us conclude there were no important assumption violations implied by it?\nWhat would we do if we saw a violation of assumptions in this plot?",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#obtaining-fitted-values-and-residuals-from-a-model",
    "href": "32-galapagos.html#obtaining-fitted-values-and-residuals-from-a-model",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.16 Obtaining Fitted Values and Residuals from a Model",
    "text": "32.16 Obtaining Fitted Values and Residuals from a Model\nWe can use the augment() function from the broom package to find the predictions made by the regression model for each of the observations used to create the model.\n\ngala_aug &lt;- augment(model1)\n\nnames(gala_aug)\n\n [1] \"species\"    \"area\"       \"elevation\"  \"nearest\"    \"scruz\"     \n [6] \"adjacent\"   \".fitted\"    \".resid\"     \".hat\"       \".sigma\"    \n[11] \".cooksd\"    \".std.resid\"\n\n\nHere, we obtain\n\npredicted (fitted) values for the outcome, in .fitted\n\nresiduals (observed - fitted) values for each observation, in .resid\n\nleverage values in .hat for each observation\nstandard errors associated with each observation in .sigma\n\nCook’s distance values for each observation in .cooksd\n\nstandardized residuals in .std.resid\n\n\nHere, for instance, are the first few rows.\n\nhead(gala_aug) |&gt; kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\narea\nelevation\nnearest\nscruz\nadjacent\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n58\n25.09\n346\n0.6\n0.6\n1.84\n116.73\n-58.73\n0.08\n60.97\n0.01\n-1.00\n\n\n31\n1.24\n109\n0.6\n26.3\n572.33\n-7.27\n38.27\n0.09\n61.72\n0.01\n0.66\n\n\n3\n0.21\n114\n2.8\n58.7\n0.78\n29.33\n-26.33\n0.06\n62.03\n0.00\n-0.45\n\n\n25\n0.10\n46\n1.9\n47.4\n0.18\n10.36\n14.64\n0.07\n62.21\n0.00\n0.25\n\n\n2\n0.05\n77\n1.9\n1.9\n903.82\n-36.38\n38.38\n0.17\n61.66\n0.02\n0.69\n\n\n18\n0.34\n119\n8.0\n8.0\n1.84\n43.09\n-25.09\n0.07\n62.05\n0.00\n-0.43\n\n\n\n\n\n\n32.16.1 Questions about Fitted Values\n\nVerify that the first fitted value [116.73] is in fact what you get for Baltra (observation 1) when you apply the regression equation:\n\nspecies = 7.07 - 0.02 area + 0.32 elevation \n               + 0.009 nearest - 0.24 scruz - 0.07 adjacent\nWe can compare these predictions to the actual observed counts of the number of species on each island. Subtracting the fitted values from the observed values gives us the residuals, as does the resid function.\n\nround(resid(model1),2)\n\n      1       2       3       4       5       6       7       8       9      10 \n -58.73   38.27  -26.33   14.64   38.38  -25.09   -9.92   19.02  -20.31  -28.79 \n     11      12      13      14      15      16      17      18      19      20 \n  49.34   -3.99   62.03  -59.63   40.50  -39.40  -37.69   -2.04 -111.68  -42.48 \n     21      22      23      24      25      26      27      28      29      30 \n -23.08   -5.55   73.05  -40.68  182.58  -23.38   89.38   -5.81  -36.94   -5.70 \n\n\n\n32.16.2 Questions about Residuals\n\nWhat does a positive residual indicate?\nWhat does a negative residual indicate?\nThe standard deviation of the full set of 30 residuals turns out to be 55.47. How does this compare to the residual standard error?\nThe command below identifies Santa Cruz. What does it indicate about Santa Cruz, specifically?\n\n\ngala$island[which.max(resid(model1))]\n\n[1] \"SantaCruz\"\n\n\n\nFrom the results below, what is the model1 residual for Santa Cruz? What does this imply about the species prediction made by Model 1 for Santa Cruz?\n\n\nwhich.max(resid(model1))\n\n25 \n25 \n\nround(resid(model1),2)\n\n      1       2       3       4       5       6       7       8       9      10 \n -58.73   38.27  -26.33   14.64   38.38  -25.09   -9.92   19.02  -20.31  -28.79 \n     11      12      13      14      15      16      17      18      19      20 \n  49.34   -3.99   62.03  -59.63   40.50  -39.40  -37.69   -2.04 -111.68  -42.48 \n     21      22      23      24      25      26      27      28      29      30 \n -23.08   -5.55   73.05  -40.68  182.58  -23.38   89.38   -5.81  -36.94   -5.70 \n\ngala[which.max(resid(model1)),]\n\n# A tibble: 1 × 10\n     id island    species  area elevation nearest scruz adjacent stures  fits\n  &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1    25 SantaCruz     444  904.       864     0.6     0     0.52   4.38  261.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#relationship-between-fitted-and-observed-values",
    "href": "32-galapagos.html#relationship-between-fitted-and-observed-values",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.17 Relationship between Fitted and Observed Values",
    "text": "32.17 Relationship between Fitted and Observed Values\nWe’ve already seen that the fitted command can produce predicted values for each observations used to develop a regression model, and that the resid command can produce the residuals (observed - predicted) for those same observations. Returning to our original model1, let’s compare the fitted values (stored earlier in fits) to the observed values.\n\nggplot(gala, aes(x = species, y = fits)) +\n    geom_point(size = 3, shape = 1) + \n    geom_abline(intercept = 0, slope = 1, \n                col = \"purple\", linetype = \"dashed\") +\n    labs(x = \"Observed Species Count\", \n         y = \"Model Predicted Species\")\n\n\n\n\n\n\n\n\n32.17.1 Questions about Fitted and Observed Values\n\nWhy did I draw the dotted purple line with y-intercept 0 and slope 1? Why is that particular line of interest?\nIf a point on this plot is in the top left here, above the dotted line, what does that mean?\nIf a point is below the dotted line here, what does that mean?\nHow does this plot display the size of an observation’s residual?",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#standardizing-residuals",
    "href": "32-galapagos.html#standardizing-residuals",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.18 Standardizing Residuals",
    "text": "32.18 Standardizing Residuals\nWe’ve already seen that the raw residuals from a regression model can be obtained using the resid function. Residuals are defined to have mean 0. This is one of the requirements of the least squares procedure for estimating a linear model, and their true standard deviation is effectively estimated using the residual standard error.\nThere are two additional types of residuals for us to be aware of: standardized residuals, and studentized (sometimes called externally standardized, or jackknife) residuals. Each approach standardizes the residuals by dividing them by a standard deviation estimate, so the resulting residuals should have mean 0 and standard deviation 1 if assumptions hold.\n\n\nStandardized residuals are the original (raw) residuals, scaled by a standard deviation estimate developed using the entire data set. This summary is part of what is provided by augment from the broom package, as .std.resid.\n\nStudentized residuals are the original (raw) residuals, scaled by a standard deviation estimate developed using the entire data set EXCEPT for this particular observation.\n\nThe rstudent function, when applied to a linear regression model, will the model’s studentized residuals.\n\ngala_aug &lt;- gala_aug |&gt;\n    mutate(.stu.resid = rstudent(model1))\n\nggplot(gala_aug, aes(x = .std.resid, \n                           y = .stu.resid)) +\n    geom_point(size = 3, pch = 1) +\n    geom_abline(intercept = 0, slope = 1, col = \"red\") +\n    geom_label(x = 2.5, y = 1, \n               label = \"y = x\", col = \"red\") +\n    theme(aspect.ratio = 1) +\n    labs(x = \"Standardized Residuals for Model 1\", \n         y = \"Studentized Residuals for Model 1\")\n\n\n\n\n\n\n\n\n32.18.1 Questions about Standardized and Studentized Residuals\n\nFrom the plot above, what conclusions can you draw about the two methods of standardizing residuals as they apply in the case of our model1?",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#influence-measures-for-multiple-regression",
    "href": "32-galapagos.html#influence-measures-for-multiple-regression",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.19 Influence Measures for Multiple Regression",
    "text": "32.19 Influence Measures for Multiple Regression\nR can output a series of influence measures for a regression model. Let me show you all of the available measures for model 1, but just for three of the data points - #1 (which is not particularly influential) and #12 and #16 (which are).\nFirst, we’ll look at the raw data:\n\ngala[c(1,12,16),]\n\n# A tibble: 3 × 10\n     id island     species   area elevation nearest scruz adjacent stures  fits\n  &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1     1 Baltra          58   25.1       346     0.6   0.6     1.84 -1.00  117. \n2    12 Fernandina      93  634.       1494     4.3  95.3  4669.   -0.286  97.0\n3    16 Isabela        347 4669.       1707     0.7  28.1   634.   -5.33  386. \n\n\nAnd then, we’ll gather the output available in the influence.measures function.\n\ninfluence.measures(model1)\n\nHere’s an edited version of this output…\nInfluence measures of\nlm(formula = species ~ area + elevation + nearest + scruz + adjacent, \ndata = gala) :\n\n     dfb.1_  dfb.area  dfb.elvt dfb.nrst  dfb.scrz  dfb.adjc\n1  -0.15064   0.13572 -0.122412  0.07684  0.084786  1.14e-01\n12  0.16112   0.16395 -0.122578  0.03093 -0.059059 -8.27e-01\n16 -1.18618 -20.87453  4.885852  0.36713 -1.022431 -8.09e-01\n\n     dffit   cov.r   cook.d    hat inf\n1   -0.29335  1.0835 1.43e-02 0.0787    \n12  -1.24249 25.1101 2.68e-01 0.9497   *\n16 -29.59041  0.3275 6.81e+01 0.9685   *\nThis output presents dfbetas for each coefficient, followed by dffit statistics, covariance ratios, Cook’s distance and leverage values (hat) along with an indicator of influence.\nWe’ll consider each of these elements in turn.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#dfbetas",
    "href": "32-galapagos.html#dfbetas",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.20 DFBETAs",
    "text": "32.20 DFBETAs\nThe first part of the influence measures output concerns what are generally called dfbetas …\n\n\n\n\n\n\n\n\n\n\n\n\nid\nisland\ndfb.1_\ndfb.area\ndfb.elvt\ndfb.nrst\ndfb.scrz\ndfb.adjc\n\n\n\n1\nBaltra\n-0.151\n0.136\n-0.122\n0.077\n0.085\n0.114\n\n\n12\nFernandina\n0.161\n0.164\n-0.123\n0.031\n-0.059\n-0.827\n\n\n16\nIsabela\n-1.186\n-20.875\n4.886\n0.367\n-1.022\n-0.809\n\n\n\nThe dfbetas look at a standardized difference in the estimate of a coefficient (slope) that will occur if the specified point (here, island) is removed from the data set.\n\nPositive values indicate that deleting the point will yield a smaller coefficient.\nNegative values indicate that deleting the point will yield a larger coefficient.\nIf the absolute value of the dfbeta is greater than \\(2 / \\sqrt{n}\\), where \\(n\\) is the sample size, then the dfbeta is considered to be large.\n\nIn this case, our cutoff would be \\(2 / \\sqrt{30}\\) or 0.365, so that the Isabela dfbeta values are all indicative of large influence. Essentially, if we remove Isabela from the data, and refit the model, our regression slopes will change a lot (see below). Fernandina has some influence as well, especially on the adjacent coefficient.\n\n\n\n\n\n\n\nPredictor\nCoefficient (p) all 30 islands\n\nCoefficient (p) without Isabela\n\n\n\n\nIntercept\n7.07 (p = 0.72)\n22.59 (p = 0.11)\n\n\narea\n-0.02 (p = 0.30)\n0.30 (p &lt; 0.01)\n\n\nelevation\n0.32 (p &lt; 0.01)\n0.14 (p &lt; 0.01)\n\n\nnearest\n0.01 (p = 0.99)\n-0.26 (p = 0.73)\n\n\nscruz\n-0.24 (p = 0.28)\n-0.09 (p = 0.55)\n\n\nadjacent\n-0.08 (p &lt; 0.01)\n-0.07 (p &lt; 0.01)\n\n\n\nAfter the dfbetas, the influence.measures output presents dffit, covariance ratios, Cook’s distance and leverage values (hat) for each observation, along with an indicator of influence.\nid  island         dffit   cov.r   cook.d    hat inf\n1   Baltra      -0.29335  1.0835 1.43e-02 0.0787    \n12  Fernandina  -1.24249 25.1101 2.68e-01 0.9497   *\n16  Isabela    -29.59041  0.3275 6.81e+01 0.9685   *",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#cooks-d-or-cooks-distance",
    "href": "32-galapagos.html#cooks-d-or-cooks-distance",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.21 Cook’s d or Cook’s Distance",
    "text": "32.21 Cook’s d or Cook’s Distance\nThe main measure of influence is Cook’s Distance, also called Cook’s d. Cook’s d provides a summary of the influence of a particular point on all of the regression coefficients. It is a function of the standardized residual and the leverage. We’ve seen that the augment() function in the broom package provides both Cook’s distance and leverage values for all observations in our model.\n\nCook’s distance values greater than 1 are generally indicators of high influence.\nObviously, Isabela (with a value of Cook’s d = 68.1) is a highly influential observation by this measure.\n\n\n32.21.1 Plotting Cook’s Distance\nAs one of its automated regression diagnostic plots, R will produce an index plot of the Cook’s distance values. Note the relatively enormous influence for island 16 (Isabela).\n\nplot(model1, which = 4)",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#dffits",
    "href": "32-galapagos.html#dffits",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.22 DFFITS",
    "text": "32.22 DFFITS\nA similar measure to Cook’s distance is called DFFITS. The DFFITS value describes the influence of the point on the fitted value. It’s the number of standard deviations that the fitted value changes if the observation is removed. This is defined as a function of the studentized residual and the leverage.\n\nIf the absolute value of DFFITS is greater than 2 times \\(\\sqrt{p / n-p}\\), where p is the number of predictors (not including the intercept), we deem the observation influential.\nFor the gala data, we’d consider any point with DFFITS greater than 2 x \\(\\sqrt{5 / (30-5)}\\) = 0.894 to be influential by this standard, since n = 30 and we are estimating p = 5 slopes in our model. This is true of both Fernandina and Isabela.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#covariance-ratio",
    "href": "32-galapagos.html#covariance-ratio",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.23 Covariance Ratio",
    "text": "32.23 Covariance Ratio\nThe covariance ratio cov.r indicates the role of the observation on the precision of estimation. If cov.r is greater than 1, then this observation improves the precision, overall, and if it’s less than 1, the observation drops the precision of estimation, and these are the points about which we’ll be most concerned.\n\nAs with most of our other influence measures, Isabela appears to be a concern.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#leverage",
    "href": "32-galapagos.html#leverage",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.24 Leverage",
    "text": "32.24 Leverage\nThe hat value is a measure of leverage. Specifically, this addresses whether or not the point in question is unusual in terms of its combination of predictor values. We’ve seen that the augment() function in the broom package provides leverage values for all observations in our model.\n\nThe usual cutoff for a large leverage value is 2.5 times the average leverage across all observations, where the average leverage is equal to k/n, where n is the number of observations included in the regression model, and k is the number of model coefficients (slopes plus intercept).\nIn the gala example, we’d regard any observation with a hat value larger than 2.5 x 6/30 = 0.5 to have large leverage. This includes Fernandina and Isabela.\n\n\n32.24.1 Indicator of Influence\nThe little asterisk indicates an observation which is influential according to R’s standards for any of these measures. You can take the absence of an asterisk as a clear indication that a point is NOT influential. Points with asterisks may or may not be influential in an important way. In practice, I usually focus on the Cook’s distance to make decisions about likely influence, when the results aren’t completely clear.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#building-predictions-from-our-models",
    "href": "32-galapagos.html#building-predictions-from-our-models",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.25 Building Predictions from our models",
    "text": "32.25 Building Predictions from our models\nThe predict function, when applied to a linear regression model, produces the fitted values, just as the augment() function did, and, as we’ve seen, it can be used to generate prediction intervals for a single new observation, or confidence intervals for a group of new observations with the same predictor values.\nLet us, just for a moment, consider a “typical” island, exemplified by the median value of all the predictors. There’s a trick to creating this and dumping it in a vector I will call x.medians.\n\nx &lt;- model.matrix(model1)\nx.medians &lt;- apply(x, 2, function(x) median(x))\nx.medians\n\n(Intercept)        area   elevation     nearest       scruz    adjacent \n       1.00        2.59      192.00        3.05       46.65        2.59 \n\n\nWe want to use the model to predict our outcome (species) on the basis of the inputs above: a new island with values of all predictors equal to the median of the existing islands. As before, building an interval forecast around a fitted value requires us to decide whether we are:\n\npredicting the number of species for one particular island with the specified characteristics (in which case we use something called a prediction interval) or\npredicting the mean number of species across all islands that have the specified characteristics (in which case we use the confidence interval).\n\n\nnewdata &lt;- data.frame(t(x.medians))\npredict(model1, newdata, interval=\"prediction\", level = 0.95)\n\n       fit       lwr      upr\n1 56.95714 -72.06155 185.9758\n\npredict(model1, newdata, interval=\"confidence\", level = 0.95)\n\n       fit      lwr      upr\n1 56.95714 28.52381 85.39048\n\n\n\n32.25.1 Questions about the Prediction and Confidence Interval Methods\n\nWhat is the 95% prediction interval for this new observation? Does that make sense?\nWhich interval (prediction or confidence) is wider? Does that make sense?\nIs there an island that has characteristics that match our new medians variable?\nWhat happens if we don’t specify new data in making a prediction?",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#making-a-prediction-with-new-data-without-broom",
    "href": "32-galapagos.html#making-a-prediction-with-new-data-without-broom",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.26 Making a Prediction with New Data (without broom)",
    "text": "32.26 Making a Prediction with New Data (without broom)\n\nHow does the output below help us to make a prediction with a new data point, or series of them? Interpret the resulting intervals.\n\n\nnewdata2 &lt;- tibble(area = 2, elevation = 100, nearest = 3, \n                       scruz = 5, adjacent = 1)\npredict(model1, newdata2, interval=\"prediction\", level = 0.95)\n\n       fit       lwr      upr\n1 37.71683 -92.46825 167.9019\n\npredict(model1, newdata2, interval=\"confidence\", level = 0.95)\n\n       fit      lwr     upr\n1 37.71683 4.388351 71.0453",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#scaling-predictors-using-z-scores-semi-standardized-coefficients",
    "href": "32-galapagos.html#scaling-predictors-using-z-scores-semi-standardized-coefficients",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.27 Scaling Predictors using Z Scores: Semi-Standardized Coefficients",
    "text": "32.27 Scaling Predictors using Z Scores: Semi-Standardized Coefficients\nWe know that the interpretation of the coefficients in a regression model is sensitive to the scale of the predictors. We have already seen how to “standardize” each predictor by subtracting its mean and dividing by its standard deviation.\n\nEach coefficient in this semi-standardized model has the following interpretation: the expected difference in the outcome, comparing units (subjects) that differ by one standard deviation in the variable of interest, but for which all other variables are fixed at their average.\nRemember also that the intercept in such a model shows the mean outcome across all subjects.\n\nConsider a two-variable model, using area and elevation to predict the number of species…\n\nmodel2 &lt;- lm(species ~ area + elevation, data=gala)\nsummary(model2)\n\n\nCall:\nlm(formula = species ~ area + elevation, data = gala)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-192.619  -33.534  -19.199    7.541  261.514 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) 17.10519   20.94211   0.817  0.42120   \narea         0.01880    0.02594   0.725  0.47478   \nelevation    0.17174    0.05317   3.230  0.00325 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 79.34 on 27 degrees of freedom\nMultiple R-squared:  0.554, Adjusted R-squared:  0.521 \nF-statistic: 16.77 on 2 and 27 DF,  p-value: 1.843e-05\n\n\nNow compare these results to the ones we get after scaling the area and elevation variables. Remember that the scale function centers a variable on zero by subtracting the mean from each observation, and then scales the result by dividing by the standard deviation. This ensures that each regression input has mean 0 and standard deviation 1, and is thus a z score.\n\nmodel2.z &lt;- lm(species ~ scale(area) + scale(elevation), \n               data=gala)\nsummary(model2.z)\n\n\nCall:\nlm(formula = species ~ scale(area) + scale(elevation), data = gala)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-192.619  -33.534  -19.199    7.541  261.514 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         85.23      14.48   5.884 2.87e-06 ***\nscale(area)         16.25      22.42   0.725  0.47478    \nscale(elevation)    72.41      22.42   3.230  0.00325 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 79.34 on 27 degrees of freedom\nMultiple R-squared:  0.554, Adjusted R-squared:  0.521 \nF-statistic: 16.77 on 2 and 27 DF,  p-value: 1.843e-05\n\n\n\n32.27.1 Questions about the Semi-Standardized Model\n\nWhat changes after centering and rescaling the predictors, and what does not?\nWhy might rescaling like this be a helpful thing to do if you want to compare predictors in terms of importance?",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#fully-standardized-regression-coefficients",
    "href": "32-galapagos.html#fully-standardized-regression-coefficients",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.28 Fully Standardized Regression Coefficients",
    "text": "32.28 Fully Standardized Regression Coefficients\nSuppose we standardize the coefficients by also taking centering and scaling (using the z score) the outcome variable: species, creating a fully standardized model.\n\nmodel2.zout &lt;- lm(scale(species) ~ \n                  scale(area) + scale(elevation), data=gala)\nsummary(model2.zout)\n\n\nCall:\nlm(formula = scale(species) ~ scale(area) + scale(elevation), \n    data = gala)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.68031 -0.29253 -0.16749  0.06578  2.28131 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)      4.594e-17  1.264e-01   0.000  1.00000   \nscale(area)      1.418e-01  1.956e-01   0.725  0.47478   \nscale(elevation) 6.316e-01  1.956e-01   3.230  0.00325 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6921 on 27 degrees of freedom\nMultiple R-squared:  0.554, Adjusted R-squared:  0.521 \nF-statistic: 16.77 on 2 and 27 DF,  p-value: 1.843e-05\n\n\n\n32.28.1 Questions about the Standardized Model\n\nHow do you interpret the value 0.142 of the scale(area) coefficient here? You may want to start by reviewing the summary of the original gala data shown here.\n\n\nsummary(gala[c(\"species\", \"area\", \"elevation\")])\n\n    species            area            elevation      \n Min.   :  2.00   Min.   :   0.010   Min.   :  25.00  \n 1st Qu.: 13.00   1st Qu.:   0.258   1st Qu.:  97.75  \n Median : 42.00   Median :   2.590   Median : 192.00  \n Mean   : 85.23   Mean   : 261.709   Mean   : 368.03  \n 3rd Qu.: 96.00   3rd Qu.:  59.237   3rd Qu.: 435.25  \n Max.   :444.00   Max.   :4669.320   Max.   :1707.00  \n\n\n\nHow do you interpret the value 0.632 of the scale(elevation) coefficient in the standardized model?\nWhat is the intercept in this setting? Will this be the case whenever you scale like this?\nWhat are some of the advantages of looking at scaled regression coefficients?\nWhy are these called fully standardized coefficients while the previous page described semi-standardized coefficients?\nWhat would motivate you to use one of these two methods of standardization (fully standardized or semi-standardized) vs. the other?",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#robust-standardization-of-regression-coefficients",
    "href": "32-galapagos.html#robust-standardization-of-regression-coefficients",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.29 Robust Standardization of Regression Coefficients",
    "text": "32.29 Robust Standardization of Regression Coefficients\nAnother common option for scaling is to specify lower and upper comparison points, perhaps by comparing the impact of a move from the 25th to the 75th percentile for each variable, while holding all of the other variables constant.\nOccasionally, you will see robust semi-standardized regression coefficients, which measure the increase in the outcome, Y, associated with an increase in that particular predictor of one IQR (inter-quartile range).\n\ngala$area.scaleiqr &lt;- \n    (gala$area - mean(gala$area)) / IQR(gala$area)\ngala$elevation.scaleiqr &lt;- \n    (gala$elevation - mean(gala$elevation)) / \n                            IQR(gala$elevation)\n\nmodel2.iqr &lt;- lm(species ~ \n                     area.scaleiqr + elevation.scaleiqr,\n                 data=gala)\nsummary(model2.iqr)\n\n\nCall:\nlm(formula = species ~ area.scaleiqr + elevation.scaleiqr, data = gala)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-192.619  -33.534  -19.199    7.541  261.514 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          85.233     14.485   5.884 2.87e-06 ***\narea.scaleiqr         1.109      1.530   0.725  0.47478    \nelevation.scaleiqr   57.963     17.946   3.230  0.00325 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 79.34 on 27 degrees of freedom\nMultiple R-squared:  0.554, Adjusted R-squared:  0.521 \nF-statistic: 16.77 on 2 and 27 DF,  p-value: 1.843e-05\n\n\n\n32.29.1 Questions about Robust Standardization\n\nHow should we interpret the 57.96 value for the scaled elevation variable? You may want to start by considering the summary of the original elevation data below.\n\n\nsummary(gala$elevation)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.00   97.75  192.00  368.03  435.25 1707.00 \n\n\nA robust standardized coefficient analysis measures the increase in Y (in IQR of Y) associated with an increase in the predictor of interest of one IQR.\n\ngala$species.scaleiqr &lt;- (gala$species - mean(gala$species)) / IQR(gala$species)\nmodel2.iqrout &lt;- lm(species.scaleiqr ~ area.scaleiqr + elevation.scaleiqr, data=gala)\nmodel2.iqrout\n\n\nCall:\nlm(formula = species.scaleiqr ~ area.scaleiqr + elevation.scaleiqr, \n    data = gala)\n\nCoefficients:\n       (Intercept)       area.scaleiqr  elevation.scaleiqr  \n        -1.013e-16           1.336e-02           6.983e-01  \n\n\n\nWhat can we learn from the R output above?",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "32-galapagos.html#scaling-inputs-by-dividing-by-2-standard-deviations",
    "href": "32-galapagos.html#scaling-inputs-by-dividing-by-2-standard-deviations",
    "title": "\n32  Species Found on the Galapagos Islands\n",
    "section": "\n32.30 Scaling Inputs by Dividing by 2 Standard Deviations",
    "text": "32.30 Scaling Inputs by Dividing by 2 Standard Deviations\nIt turns out that standardizing the inputs to a regression model by dividing by a standard deviation creates some difficulties when you want to include a binary predictor in the model.\nInstead, Andrew Gelman recommends that you consider centering all of the predictors (binary or continuous) by subtracting off the mean, and then, for the non-binary predictors, also dividing not by one, but rather by two standard deviations.\n\nSuch a standardization can go a long way to helping us understand a model whose predictors are on different scales, and provides an interpretable starting point.\nAnother appealing part of this approach is that in the arm library, Gelman and his colleagues have created an R function called standardize, which can be used to automate the process of checking coefficients that have been standardized in this manner, after the regression model has been fit.\n\n\nmodel2\n\n\nCall:\nlm(formula = species ~ area + elevation, data = gala)\n\nCoefficients:\n(Intercept)         area    elevation  \n    17.1052       0.0188       0.1717  \n\narm::standardize(model2)\n\n\nCall:\nlm(formula = species ~ z.area + z.elevation, data = gala)\n\nCoefficients:\n(Intercept)       z.area  z.elevation  \n      85.23        32.50       144.81  \n\n\n\n32.30.1 Questions about Standardizing by Dividing by Two SD\n\nHow does this result compare to the semi-standardized regression coefficients we have seen on the last few analyses?\nHow should we interpret the z.area coefficient of 32.5 here? Again, you may want to start by obtaining a statistical summary of the original area data, as shown below.\n\n\nsummary(gala$area)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   0.010    0.258    2.590  261.709   59.238 4669.320 \n\n\nTo standardize the outcome in this way, as well, we use:\n\narm::standardize(model2, standardize.y=TRUE)\n\n\nCall:\nlm(formula = z.species ~ z.area + z.elevation, data = gala)\n\nCoefficients:\n(Intercept)       z.area  z.elevation  \n  1.653e-19    1.418e-01    6.316e-01  \n\n\n\nHow should we interpret the z.area coefficient of 0.142 here?\nHow does these relate to the standardized regression coefficients we’ve seen before?\n\nAgain, if you have questions about how to interpret the output in light of the questions contained in this Chapter, please ask us for help on Piazza or in TA office hours.\n\n\n\n\nFaraway, Julian J. 2015. Linear Models with r. Second. Boca Raton, FL: CRC Press.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Species Found on the Galapagos Islands</span>"
    ]
  },
  {
    "objectID": "33-nonlinearity.html",
    "href": "33-nonlinearity.html",
    "title": "33  Introduction to Non-linear Terms",
    "section": "",
    "text": "33.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(broom)\nlibrary(car)\nlibrary(Hmisc)\nlibrary(rms)\nlibrary(mosaic)\nlibrary(patchwork)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Introduction to Non-linear Terms</span>"
    ]
  },
  {
    "objectID": "33-nonlinearity.html#the-pollution-data",
    "href": "33-nonlinearity.html#the-pollution-data",
    "title": "33  Introduction to Non-linear Terms",
    "section": "\n33.2 The pollution data",
    "text": "33.2 The pollution data\nConsider the pollution data set, which contain 15 independent variables and a measure of mortality, describing 60 US metropolitan areas in 1959-1961. The data come from McDonald and Schwing (1973), and are available at http://www4.stat.ncsu.edu/~boos/var.select/pollution.html and our web site.\n\npollution &lt;- read_csv(\"data/pollution.csv\", show_col_types = FALSE)\n\nHere’s a codebook:\n\n\n\n\n\n\nVariable\nDescription\n\n\n\ny\nTotal Age Adjusted Mortality Rate\n\n\nx1\nMean annual precipitation in inches\n\n\nx2\nMean January temperature in degrees Fahrenheit\n\n\nx3\nMean July temperature in degrees Fahrenheit\n\n\nx4\nPercent of 1960 SMSA population that is 65 years of age or over\n\n\nx5\nPopulation per household, 1960 SMSA\n\n\nx6\nMedian school years completed for those over 25 in 1960 SMSA\n\n\nx7\nPercent of housing units that are found with facilities\n\n\nx8\nPopulation per square mile in urbanized area in 1960\n\n\nx9\nPercent of 1960 urbanized area population that is non-white\n\n\nx10\nPercent employment in white-collar occupations in 1960 urbanized area\n\n\nx11\nPercent of families with income under 3; 000 in 1960 urbanized area\n\n\nx12\nRelative population potential of hydrocarbons, HC\n\n\nx13\nRelative pollution potential of oxides of nitrogen, NOx\n\n\nx14\nRelative pollution potential of sulfur dioxide, SO2\n\n\nx15\nPercent relative humidity, annual average at 1 p.m.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Introduction to Non-linear Terms</span>"
    ]
  },
  {
    "objectID": "33-nonlinearity.html#a-simple-linear-model",
    "href": "33-nonlinearity.html#a-simple-linear-model",
    "title": "33  Introduction to Non-linear Terms",
    "section": "\n33.3 A simple linear model",
    "text": "33.3 A simple linear model\nConsider the relationship between y, the age-adjusted mortality rate, and x2, the mean January temperature, across these 60 areas. I’ll include both a linear model (in blue) and a loess smooth (in red.) Does the relationship appear to be linear?\n\nggplot(pollution, aes(x = x2, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", col = \"blue\", \n                formula = y ~ x, se = F) +\n    geom_smooth(method = \"loess\", col = \"red\", \n                formula = y ~ x, se = F)\n\n\n\n\n\n\n\nSuppose we plot the residuals that emerge from the linear model shown in blue, above. Do we see a curve in a plot of residuals against fitted values?\n\nplot(lm(y ~ x2, data = pollution), which = 1)",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Introduction to Non-linear Terms</span>"
    ]
  },
  {
    "objectID": "33-nonlinearity.html#quadratic-polynomial-model",
    "href": "33-nonlinearity.html#quadratic-polynomial-model",
    "title": "33  Introduction to Non-linear Terms",
    "section": "\n33.4 Quadratic polynomial model`",
    "text": "33.4 Quadratic polynomial model`\nA polynomial in the variable x of degree D is a linear combination of the powers of x up to D.\nFor example:\n\nLinear: \\(y = \\beta_0 + \\beta_1 x\\)\n\nQuadratic: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\)\n\nCubic: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3\\)\n\nQuartic: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4 x^4\\)\n\nQuintic: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4 x^4 + \\beta_5 x^5\\)\n\n\nFitting such a model creates a polynomial regression.\n\n33.4.1 The raw quadratic model\nLet’s look at a quadratic model which predicts y using x2 and the square of x2, so that our model is of the form:\n\\[\ny = \\beta_0 + \\beta_1 x_2 + \\beta_2 x_2^2 + error\n\\]\nThere are several ways to fit this exact model.\n\nOne approach is to calculate the square of x2 within our pollution data set, and then feed both x2 and x2squared to lm.\nAnother approach uses the I function within our lm to specify the use of both x2 and its square.\nYet another approach uses the poly function within our lm, which can be used to specify raw models including x2 and x2squared.\n\n\npollution &lt;- pollution |&gt; mutate(x2squared = x2^2)\n\nmod2a &lt;- lm(y ~ x2 + x2squared, data = pollution)\nmod2b &lt;- lm(y ~ x2 + I(x2^2), data = pollution)\nmod2c &lt;- lm(y ~ poly(x2, degree = 2, raw = TRUE), data = pollution)\n\nEach of these approaches produces the same model, as they are just different ways of expressing the same idea.\n\nsummary(mod2a)\n\n\nCall:\nlm(formula = y ~ x2 + x2squared, data = pollution)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-148.977  -38.651    6.889   35.312  189.346 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 785.77449   79.54086   9.879 5.87e-14 ***\nx2            8.87640    4.27394   2.077   0.0423 *  \nx2squared    -0.11704    0.05429  -2.156   0.0353 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 60.83 on 57 degrees of freedom\nMultiple R-squared:  0.07623,   Adjusted R-squared:  0.04382 \nF-statistic: 2.352 on 2 and 57 DF,  p-value: 0.1044\n\n\nAnd if we plot the fitted values for this mod2 using whatever approach you like, we get exactly the same result.\n\nmod2a_aug &lt;- augment(mod2a, pollution)\n\nggplot(mod2a_aug, aes(x = x2, y = y)) +\n    geom_point() +\n    geom_line(aes(x = x2, y = .fitted), col = \"red\") +\n    labs(title = \"Model 2a: Quadratic fit using x2 and x2^2\")\n\n\n\n\n\n\n\n\nmod2b_aug &lt;- augment(mod2b, pollution)\n\nmod2c_aug &lt;- augment(mod2c, pollution)\n\np1 &lt;- ggplot(mod2b_aug, aes(x = x2, y = y)) +\n    geom_point() +\n    geom_line(aes(x = x2, y = .fitted), col = \"red\") +\n    labs(title = \"Model 2b: Quadratic fit\")\n\np2 &lt;- ggplot(mod2c_aug, aes(x = x2, y = y)) +\n    geom_point() +\n    geom_line(aes(x = x2, y = .fitted), col = \"blue\") +\n    labs(title = \"Model 2c: Quadratic fit\")\n\np1 + p2\n\n\n\n\n\n\n\n\n33.4.2 Raw quadratic fit after centering\nSometimes, we’ll center (and perhaps rescale, too) the x2 variable before including it in a quadratic fit like this.\n\npollution &lt;- pollution |&gt;\n    mutate(x2_c = x2 - mean(x2))\n\nmod2d &lt;- lm(y ~ x2_c + I(x2_c^2), data = pollution)\n\nsummary(mod2d)\n\n\nCall:\nlm(formula = y ~ x2_c + I(x2_c^2), data = pollution)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-148.977  -38.651    6.889   35.312  189.346 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 952.25941    9.59896  99.204   &lt;2e-16 ***\nx2_c          0.92163    0.93237   0.988   0.3271    \nI(x2_c^2)    -0.11704    0.05429  -2.156   0.0353 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 60.83 on 57 degrees of freedom\nMultiple R-squared:  0.07623,   Adjusted R-squared:  0.04382 \nF-statistic: 2.352 on 2 and 57 DF,  p-value: 0.1044\n\n\nNote that this model looks very different, with the exception of the second order quadratic term. But, it produces the same fitted values as the models we fit previously.\n\nmod2d_aug &lt;- augment(mod2d, pollution)\n\nggplot(mod2d_aug, aes(x = x2, y = y)) +\n    geom_point() +\n    geom_line(aes(x = x2, y = .fitted), col = \"red\") +\n    labs(title = \"Model 2d: Quadratic fit using centered x2 and x2^2\")\n\n\n\n\n\n\n\nOr, if you don’t believe me yet, look at the four sets of fitted values another way.\n\nmosaic::favstats(~ .fitted, data = mod2a_aug)\n\n      min       Q1  median       Q3     max     mean       sd  n missing\n 855.1041 936.7155 945.597 950.2883 954.073 940.3585 17.17507 60       0\n\nmosaic::favstats(~ .fitted, data = mod2b_aug)\n\n      min       Q1  median       Q3     max     mean       sd  n missing\n 855.1041 936.7155 945.597 950.2883 954.073 940.3585 17.17507 60       0\n\nmosaic::favstats(~ .fitted, data = mod2c_aug)\n\n      min       Q1  median       Q3     max     mean       sd  n missing\n 855.1041 936.7155 945.597 950.2883 954.073 940.3585 17.17507 60       0\n\nmosaic::favstats(~ .fitted, data = mod2d_aug)\n\n      min       Q1  median       Q3     max     mean       sd  n missing\n 855.1041 936.7155 945.597 950.2883 954.073 940.3585 17.17507 60       0",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Introduction to Non-linear Terms</span>"
    ]
  },
  {
    "objectID": "33-nonlinearity.html#orthogonal-polynomials",
    "href": "33-nonlinearity.html#orthogonal-polynomials",
    "title": "33  Introduction to Non-linear Terms",
    "section": "\n33.5 Orthogonal Polynomials",
    "text": "33.5 Orthogonal Polynomials\nNow, let’s fit an orthogonal polynomial of degree 2 to predict y using x2.\n\nmod2_orth &lt;- lm(y ~ poly(x2, 2), data = pollution)\n\nsummary(mod2_orth)\n\n\nCall:\nlm(formula = y ~ poly(x2, 2), data = pollution)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-148.977  -38.651    6.889   35.312  189.346 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   940.358      7.853 119.746   &lt;2e-16 ***\npoly(x2, 2)1  -14.345     60.829  -0.236   0.8144    \npoly(x2, 2)2 -131.142     60.829  -2.156   0.0353 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 60.83 on 57 degrees of freedom\nMultiple R-squared:  0.07623,   Adjusted R-squared:  0.04382 \nF-statistic: 2.352 on 2 and 57 DF,  p-value: 0.1044\n\n\nNow this looks very different in the equation, but, again, we can see that this produces exactly the same fitted values as our previous models, and the same model fit summaries. Is it, in fact, the same model? Here, we’ll plot the fitted Model 2a in a red line, and this new Model 2 with Orthogonal Polynomials as blue points.\n\nmod2orth_aug &lt;- augment(mod2_orth, pollution)\n\nggplot(mod2orth_aug, aes(x = x2, y = y)) +\n    geom_point() +\n    geom_point(aes(x = x2, y = .fitted), \n               col = \"blue\", size = 2) +\n    geom_line(data = mod2a_aug, aes(x = x2, y = .fitted),\n              col = \"red\") +\n    labs(title = \"Model 2 with Orthogonal Polynomial, degree 2\")\n\n\n\n\n\n\n\nYes, it is again the same model in terms of the predictions it makes for y.\nBy default, with raw = FALSE, the poly() function within a linear model computes what is called an orthogonal polynomial. An orthogonal polynomial sets up a model design matrix using the coding we’ve seen previously: x2 and x2^2 in our case, and then scales those columns so that each column is orthogonal to the previous ones. This eliminates the collinearity (correlation between predictors) and lets our t tests tell us whether the addition of any particular polynomial term improves the fit of the model over the lower orders.\nWould the addition of a cubic term help us much in predicting y from x2?\n\nmod3 &lt;- lm(y ~ poly(x2, 3), data = pollution)\nsummary(mod3)\n\n\nCall:\nlm(formula = y ~ poly(x2, 3), data = pollution)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-146.262  -39.679    5.569   35.984  191.536 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   940.358      7.917 118.772   &lt;2e-16 ***\npoly(x2, 3)1  -14.345     61.328  -0.234   0.8159    \npoly(x2, 3)2 -131.142     61.328  -2.138   0.0369 *  \npoly(x2, 3)3   16.918     61.328   0.276   0.7837    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 61.33 on 56 degrees of freedom\nMultiple R-squared:  0.07748,   Adjusted R-squared:  0.02806 \nF-statistic: 1.568 on 3 and 56 DF,  p-value: 0.2073\n\n\nIt doesn’t appear that the cubic term adds much here, if anything. The p value is not significant for the third degree polynomial, the summaries of fit quality aren’t much improved, and as we can see from the plot below, the predictions don’t actually change all that much.\n\nmod3_aug &lt;- augment(mod3, pollution)\n\nggplot(mod3_aug, aes(x = x2, y = y)) +\n    geom_point() +\n    geom_line(aes(x = x2, y = .fitted), \n              col = \"blue\") +\n    geom_line(data = mod2orth_aug, aes(x = x2, y = .fitted),\n              col = \"red\") +\n    labs(title = \"Quadratic (red) vs. Cubic (blue) Polynomial Fits\")",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Introduction to Non-linear Terms</span>"
    ]
  },
  {
    "objectID": "33-nonlinearity.html#a-cubic-polynomial-model",
    "href": "33-nonlinearity.html#a-cubic-polynomial-model",
    "title": "33  Introduction to Non-linear Terms",
    "section": "\n33.6 A Cubic Polynomial Model",
    "text": "33.6 A Cubic Polynomial Model\nWhat if we consider another predictor instead? Let’s look at x3, the Mean July temperature in degrees Fahrenheit. Here is the loess smooth.\n\nggplot(pollution, aes(x = x3, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"loess\",\n                formula = y ~ x, se = TRUE)\n\n\n\n\n\n\n\nThat looks pretty curvy - perhaps we need a more complex polynomial. We’ll consider a linear model (mod4_L), a quadratic fit (mod4_Q) and a polynomial of degree 3: a cubic fit (mod_4C)\n\nmod4_L &lt;- lm(y ~ x3, data = pollution)\nsummary(mod4_L)\n\n\nCall:\nlm(formula = y ~ x3, data = pollution)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-139.813  -34.341    4.271   38.197  149.587 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  670.529    123.140   5.445  1.1e-06 ***\nx3             3.618      1.648   2.196   0.0321 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 60.29 on 58 degrees of freedom\nMultiple R-squared:  0.07674,   Adjusted R-squared:  0.06082 \nF-statistic: 4.821 on 1 and 58 DF,  p-value: 0.03213\n\nmod4_Q &lt;- lm(y ~ poly(x3, 2), data = pollution)\nsummary(mod4_Q)\n\n\nCall:\nlm(formula = y ~ poly(x3, 2), data = pollution)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-132.004  -42.184    4.069   47.126  157.396 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   940.358      7.553 124.503   &lt;2e-16 ***\npoly(x3, 2)1  132.364     58.504   2.262   0.0275 *  \npoly(x3, 2)2 -125.270     58.504  -2.141   0.0365 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 58.5 on 57 degrees of freedom\nMultiple R-squared:  0.1455,    Adjusted R-squared:  0.1155 \nF-statistic: 4.852 on 2 and 57 DF,  p-value: 0.01133\n\nmod4_C &lt;- lm(y ~ poly(x3, 3), data = pollution)\nsummary(mod4_C)\n\n\nCall:\nlm(formula = y ~ poly(x3, 3), data = pollution)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-148.004  -29.998    1.441   34.579  141.396 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   940.358      7.065 133.095  &lt; 2e-16 ***\npoly(x3, 3)1  132.364     54.728   2.419  0.01886 *  \npoly(x3, 3)2 -125.270     54.728  -2.289  0.02588 *  \npoly(x3, 3)3 -165.439     54.728  -3.023  0.00377 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 54.73 on 56 degrees of freedom\nMultiple R-squared:  0.2654,    Adjusted R-squared:  0.226 \nF-statistic: 6.742 on 3 and 56 DF,  p-value: 0.0005799\n\n\nIt looks like the cubic polynomial term is of some real importance here. Do the linear, quadratic and cubic model fitted values look different?\n\nmod4_L_aug &lt;- augment(mod4_L, pollution)\n\nmod4_Q_aug &lt;- augment(mod4_Q, pollution)\n\nmod4_C_aug &lt;- augment(mod4_C, pollution)\n\nggplot(pollution, aes(x = x3, y = y)) +\n    geom_point() +\n    geom_line(data = mod4_L_aug, aes(x = x3, y = .fitted), \n              col = \"blue\", size = 1.25) +\n    geom_line(data = mod4_Q_aug, aes(x = x3, y = .fitted),\n              col = \"black\", size = 1.25) +\n    geom_line(data = mod4_C_aug, aes(x = x3, y = .fitted),\n              col = \"red\", size = 1.25) +\n    geom_text(x = 66, y = 930, label = \"Linear Fit\", col = \"blue\") +\n    geom_text(x = 64, y = 820, label = \"Quadratic Fit\", col = \"black\") +\n    geom_text(x = 83, y = 900, label = \"Cubic Fit\", col = \"red\") +\n    labs(title = \"Linear, Quadratic and Cubic Fits predicting y with x3\") \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Introduction to Non-linear Terms</span>"
    ]
  },
  {
    "objectID": "33-nonlinearity.html#a-restricted-cubic-spline",
    "href": "33-nonlinearity.html#a-restricted-cubic-spline",
    "title": "33  Introduction to Non-linear Terms",
    "section": "\n33.7 A restricted cubic spline",
    "text": "33.7 A restricted cubic spline\n\nA linear spline is a continuous function formed by connecting points (called knots of the spline) by line segments.\nA restricted cubic spline is a way to build highly complicated curves into a regression equation in a fairly easily structured way.\nA restricted cubic spline is a series of polynomial functions joined together at the knots.\n\nSuch a spline gives us a way to flexibly account for non-linearity without over-fitting the model.\nRestricted cubic splines can fit many different types of non-linearities.\nSpecifying the number of knots is all you need to do in R to get a reasonable result from a restricted cubic spline.\n\n\n\nThe most common choices are 3, 4, or 5 knots. Each additional knot adds to the non-linearity, and spends an additional degree of freedom:\n\n3 Knots, 2 degrees of freedom, allows the curve to “bend” once.\n4 Knots, 3 degrees of freedom, lets the curve “bend” twice.\n5 Knots, 4 degrees of freedom, lets the curve “bend” three times.\n\nFor most applications, three to five knots strike a nice balance between complicating the model needlessly and fitting data pleasingly. Let’s consider a restricted cubic spline model for our y based on x3 again, but now with:\n\nin mod5a, 3 knots,\nin mod5b, 4 knots, and\nin mod5c, 5 knots\n\n\nmod5a_rcs &lt;- lm(y ~ rcs(x3, 3), data = pollution)\nmod5b_rcs &lt;- lm(y ~ rcs(x3, 4), data = pollution)\nmod5c_rcs &lt;- lm(y ~ rcs(x3, 5), data = pollution)\n\nHere, for instance, is the summary of the 5-knot model:\n\nsummary(mod5c_rcs)\n\n\nCall:\nlm(formula = y ~ rcs(x3, 5), data = pollution)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-141.522  -32.009    1.674   31.971  147.878 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)      468.113    396.319   1.181    0.243\nrcs(x3, 5)x3       6.447      5.749   1.121    0.267\nrcs(x3, 5)x3'    -25.633     46.810  -0.548    0.586\nrcs(x3, 5)x3''   323.137    293.065   1.103    0.275\nrcs(x3, 5)x3''' -612.578    396.270  -1.546    0.128\n\nResidual standard error: 54.35 on 55 degrees of freedom\nMultiple R-squared:  0.2883,    Adjusted R-squared:  0.2366 \nF-statistic: 5.571 on 4 and 55 DF,  p-value: 0.0007734\n\n\nWe’ll begin by storing the fitted values from these three models and other summaries, for plotting.\n\nmod5a_aug &lt;- augment(mod5a_rcs, pollution)\n\nmod5b_aug &lt;- augment(mod5b_rcs, pollution)\n\nmod5c_aug &lt;- augment(mod5c_rcs, pollution)\n\n\np2 &lt;- ggplot(pollution, aes(x = x3, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"loess\", col = \"purple\", \n                formula = y ~ x, se = F) +\n    labs(title = \"Loess Smooth\") \n\np3 &lt;- ggplot(mod5a_aug, aes(x = x3, y = y)) +\n    geom_point() +\n    geom_line(aes(x = x3, y = .fitted), \n              col = \"blue\", size = 1.25) +\n    labs(title = \"RCS, 3 knots\") \n\np4 &lt;- ggplot(mod5b_aug, aes(x = x3, y = y)) +\n    geom_point() +\n    geom_line(aes(x = x3, y = .fitted), \n              col = \"black\", size = 1.25) +\n    labs(title = \"RCS, 4 knots\") \n\np5 &lt;- ggplot(mod5c_aug, aes(x = x3, y = y)) +\n    geom_point() +\n    geom_line(aes(x = x3, y = .fitted), \n              col = \"red\", size = 1.25) +\n    labs(title = \"RCS, 5 knots\") \n\n(p2 + p3) / (p4 + p5)\n\n\n\n\n\n\n\nDoes it seem like the fit improves markedly (perhaps approaching the loess smooth result) as we increase the number of knots?\n\nanova(mod5a_rcs, mod5b_rcs, mod5c_rcs)\n\nAnalysis of Variance Table\n\nModel 1: y ~ rcs(x3, 3)\nModel 2: y ~ rcs(x3, 4)\nModel 3: y ~ rcs(x3, 5)\n  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   \n1     57 194935                                \n2     56 171448  1   23486.9 7.9503 0.006672 **\n3     55 162481  1    8967.2 3.0354 0.087057 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBased on an ANOVA comparison, the fourth knot adds significant predictive value (p = 0.0067), but the fifth knot is borderline (p = 0.0871). From the glance function in the broom package, we can also look at some key summaries.\n\nglance(mod5a_rcs)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.146         0.116  58.5      4.88  0.0111     2  -328.  663.  672.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nglance(mod5b_rcs)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.249         0.209  55.3      6.19 0.00104     3  -324.  658.  668.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nglance(mod5c_rcs)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.288         0.237  54.4      5.57 0.000773     4  -322.  657.  669.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\nModel\nKnots\nR2\n\nAdj. R2\n\nAIC\nBIC\n\n\n\n5a\n3\n0.146\n0.116\n663.4\n671.8\n\n\n5b\n4\n0.249\n0.209\n657.7\n668.2\n\n\n5c\n5\n0.288\n0.237\n656.5\n669.1\n\n\n\nWithin our sample, the five-knot RCS outperforms the 3- and 4-knot versions on adjusted R2 and AIC (barely) and does a little worse than the 4-knot RCS on BIC.\nOf course, we could also use the cross-validation methods we’ve developed for other linear regressions to assess predictive capacity of these models. I’ll skip that for now.\nTo see the values of x3 where the splines place their knots, we can use the attributes function.\n\nattributes(rcs(pollution$x3, 5))\n\n$dim\n[1] 60  4\n\n$dimnames\n$dimnames[[1]]\nNULL\n\n$dimnames[[2]]\n[1] \"pollution\"    \"pollution'\"   \"pollution''\"  \"pollution'''\"\n\n\n$class\n[1] \"rms\"\n\n$name\n[1] \"pollution\"\n\n$label\n[1] \"pollution\"\n\n$assume\n[1] \"rcspline\"\n\n$assume.code\n[1] 4\n\n$parms\n[1] 68 72 74 77 82\n\n$nonlinear\n[1] FALSE  TRUE  TRUE  TRUE\n\n$colnames\n[1] \"pollution\"    \"pollution'\"   \"pollution''\"  \"pollution'''\"\n\n\nThe knots in this particular 5-knot spline are placed by the computer at 68, 72, 74, 77 and 82, it seems.\nThere are two kinds of Multivariate Regression Models\n\n[Prediction] Those that are built so that we can make accurate predictions.\n[Explanatory] Those that are built to help understand underlying phenomena.\n\nWhile those two notions overlap considerably, they do imply different things about how we strategize about model-building and model assessment. Harrell’s primary concern is effective use of the available data for prediction - this implies some things that will be different from what we’ve seen in the past.\nHarrell refers to multivariable regression modeling strategy as the process of spending degrees of freedom. The main job in strategizing about multivariate modeling is to\n\nDecide the number of degrees of freedom that can be spent\nDecide where to spend them\nSpend them, wisely.\n\nWhat this means is essentially linked to making decisions about predictor complexity, both in terms of how many predictors will be included in the regression model, and about how we’ll include those predictors.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Introduction to Non-linear Terms</span>"
    ]
  },
  {
    "objectID": "33-nonlinearity.html#spending-degrees-of-freedom",
    "href": "33-nonlinearity.html#spending-degrees-of-freedom",
    "title": "33  Introduction to Non-linear Terms",
    "section": "\n33.8 “Spending” Degrees of Freedom",
    "text": "33.8 “Spending” Degrees of Freedom\n\n“Spending” df includes\n\nfitting parameter estimates in models, or\nexamining figures built using the outcome variable Y that tell you how to model the predictors.\n\n\n\nIf you use a scatterplot of Y vs. X or the residuals of the Y-X regression model vs. X to decide whether a linear model is appropriate, then how many degrees of freedom have you actually spent?\nGrambsch and O’Brien conclude that if you wish to preserve the key statistical properties of the various estimation and fitting procedures used in building a model, you can’t retrieve these degrees of freedom once they have been spent.\n\n33.8.1 Overfitting and Limits on Predictor Counts\nSuppose you have a total sample size of \\(n\\) observations, then you really shouldn’t be thinking about estimating more than \\(n / 15\\) regression coefficients, at the most.\n\nIf \\(k\\) is the number of parameters in a full model containing all candidate predictors for a stepwise analysis, then \\(k\\) should be no greater than \\(n / 15\\).\n\n\\(k\\) should include all variables screened for association with the response, including interaction terms.\nSometimes I hold myself to a tougher standard, or \\(n / 50\\) predictors, at maximum.\n\nSo if you have 97 observations in your data, then you can probably just barely justify the use of a stepwise analysis using the main effects alone of 5 candidate variables (with one additional DF for the intercept term) using the \\(n/15\\) limit.\nHarrell (2001) also mentions that if you have a narrowly distributed predictor, without a lot of variation to work with, then an even larger sample size \\(n\\) should be required. See Vittinghoff et al. (2012), Section 10.3 for more details.\n\n33.8.2 The Importance of Collinearity\n\nCollinearity denotes correlation between predictors high enough to degrade the precision of the regression coefficient estimates substantially for some or all of the correlated predictors\n\n\nVittinghoff et al. (2012), section 10.4.1\n\nCan one predictor in a model be predicted well using the other predictors in the model?\n\nStrong correlations (for instance, \\(r \\geq 0.8\\)) are especially troublesome.\n\n\n\nEffects of collinearity\n\ndecreases precision, in the sense of increasing the standard errors of the parameter estimates\ndecreases power\nincreases the difficulty of interpreting individual predictor effects\noverall F test is significant, but individual t tests may not be\n\n\n\nSuppose we want to assess whether variable \\(X_j\\) is collinear with the other predictors in a model. We run a regression predicting \\(X_j\\) using the other predictors, and obtain the R2. The VIF is defined as 1 / (1 - this R2), and we usually interpret VIFs above 5 as indicating a serious multicollinearity problem (i.e. R2 values for this predictor of 0.8 and above would thus concern us.)\n\nvif(lm(y ~ x1 + x2 + x3 + x4 + x5 + x6, \n            data = pollution))\n\n      x1       x2       x3       x4       x5       x6 \n2.238862 2.058731 2.153044 4.174448 3.447399 1.792996 \n\n\nAgain, you’ll occasionally see the inverse of VIF reported, and this is called tolerance.\n\ntolerance = 1 / VIF\n\n33.8.3 Collinearity in an Explanatory Model\n\nWhen we are attempting to identify multiple independent predictors (the explanatory model approach), then we will need to choose between collinear variables\n\noptions suggested by Vittinghoff et al. (2012), p. 422, include choosing on the basis of plausibility as a causal factor,\nchoosing the variable that has higher data quality (is measured more accurately or has fewer missing values.)\nOften, we choose to include a variable that is statistically significant as a predictor, and drop others, should we be so lucky.\n\n\nLarger effects, especially if they are associated with predictors that have minimal correlation with the other predictors under study, cause less trouble in terms of potential violation of the \\(n/15\\) rule for what constitutes a reasonable number of predictors.\n\n33.8.4 Collinearity in a Prediction Model\n\nIf we are primarily building a prediction model for which inference on the individual predictors is not of interest, then it is totally reasonable to use both predictors in the model, if doing so reduces prediction error.\n\nCollinearity doesn’t affect predictions in our model development sample.\nCollinearity doesn’t affect predictions on new data so long as the new data have similar relationships between predictors.\nIf our key predictor is correlated strongly with a confounder, then if the predictor remains significant after adjustment for the confounder, then this suggests a meaningful independent effect.\n\nIf the effects of the predictor are clearly confounded by the adjustment variable, we again have a clear result.\nIf neither is statistically significant after adjustment, the data may be inadequate.\n\n\nIf the collinearity is between adjustment variables, but doesn’t involve the key predictor, then inclusion of the collinear variables is unlikely to cause substantial problems.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Introduction to Non-linear Terms</span>"
    ]
  },
  {
    "objectID": "33-nonlinearity.html#spending-degrees-of-freedom-1",
    "href": "33-nonlinearity.html#spending-degrees-of-freedom-1",
    "title": "33  Introduction to Non-linear Terms",
    "section": "\n33.9 Spending Degrees of Freedom",
    "text": "33.9 Spending Degrees of Freedom\nWe need a flexible approach to assessing non-linearity and fitting models with non-linear predictors. This will lead us to a measure of what Harrell (2001) calls potential predictive punch which hides the true form of the regression from the analyst so as to preserve statistical properties, but that lets us make sensible decisions about whether a predictor should be included in a model, and the number of parameters (degrees of freedom, essentially) we are willing to devote to it.\nWhat if we want to consider where best to spend our degrees of freedom on non-linear predictor terms, like interactions, polynomial functions or curved splines to represent our input data? The approach we’ll find useful in the largest variety of settings is a combination of\n\na rank correlation assessment of potential predictive punch (using a Spearman \\(\\rho^2\\) plot, available in the Hmisc package), followed by\nthe application of restricted cubic splines to fit and assess models.\n\nLet’s try such a plot for our fifteen predictors:\n\nsp2 &lt;- Hmisc::spearman2(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 +\n                          x8 + x9 + x10 + x11 + x12 + x13 +\n                          x14 + x15, data = pollution)\n\nplot(sp2)\n\n\n\n\n\n\n\nThe variable with the largest adjusted squared Spearman \\(\\rho\\) statistic in this setting is x9, followed by x6 and x14. With only 60 observations, we might well want to restrict ourselves to a very small model. What the Spearman plot suggests is that we focus any non-linear terms on x9 first, and then perhaps x6 and x14 as they have some potential predictive power. It may or may not work out that the non-linear terms are productive.\n\n33.9.1 Fitting a Big Model\nSo, one possible model built in reaction this plot might be to fit:\n\na restricted cubic spline with 5 knots on x9,\na restricted cubic spline with 3 knots on x6,\na quadratic polynomial on x14, and\na linear fit to x1 and x13\n\n\nThat’s way more degrees of freedom (4 for x9, 2 for x6, 2 for x14 and 1 each for x1 and x13 makes a total of 10 without the intercept term) than we can really justify with a sample of 60 observations. But let’s see what happens.\n\nmod_big &lt;- lm(y ~ rcs(x9, 5) + rcs(x6, 3) + poly(x14, 2) + \n                  x1 + x13, data = pollution)\n\nanova(mod_big)\n\nAnalysis of Variance Table\n\nResponse: y\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nrcs(x9, 5)    4 100164 25040.9 17.8482 4.229e-09 ***\nrcs(x6, 3)    2  38306 19152.8 13.6513 1.939e-05 ***\npoly(x14, 2)  2  15595  7797.7  5.5579  0.006677 ** \nx1            1   4787  4787.3  3.4122  0.070759 .  \nx13           1    712   711.9  0.5074  0.479635    \nResiduals    49  68747  1403.0                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis anova suggests that we have at least some predictive value in each spline (x9 and x6) and some additional value in x14, although it’s not as clear that the linear terms (x1 and x13) did much good.\n\n33.9.2 Limitations of lm fits\nWe can certainly assess this big, complex model using lm in comparison to other models:\n\nwith in-sample summary statistics like adjusted R2, AIC and BIC,\nwe can assess its assumptions with residual plots, and\nwe can also compare out-of-sample predictive quality through cross-validation,\n\nBut to really delve into the details of how well this complex model works, and to help plot what is actually being fit, we’ll probably want to fit the model using an alternative method for fitting linear models, called ols, from the rms package developed by Frank Harrell and colleagues. That will be the focus of our next chapter.\n\n\n\n\nHarrell, Frank E. 2001. Regression Modeling Strategies. New York: Springer.\n\n\nMcDonald, Gary C., and Richard C. Schwing. 1973. “Instabilities of Regression Estimates Relating Air Pollution to Mortality.” Technometrics 15 (3): 463–81.\n\n\nVittinghoff, Eric, David V. Glidden, Stephen C. Shiboski, and Charles E. McCulloch. 2012. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. Second. Springer-Verlag, Inc. http://www.biostat.ucsf.edu/vgsm/.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Introduction to Non-linear Terms</span>"
    ]
  },
  {
    "objectID": "34-olsfits.html",
    "href": "34-olsfits.html",
    "title": "\n34  Using ols to fit linear models\n",
    "section": "",
    "text": "34.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(broom)\nlibrary(Hmisc)\nlibrary(rms)\nlibrary(patchwork)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Using `ols` to fit linear models</span>"
    ]
  },
  {
    "objectID": "34-olsfits.html#where-we-are",
    "href": "34-olsfits.html#where-we-are",
    "title": "\n34  Using ols to fit linear models\n",
    "section": "\n34.2 Where We Are",
    "text": "34.2 Where We Are\nAt the end of Chapter 33, we had fit a model to the pollution data that predicted our outcome y = Age-Adjusted Mortality Rate, using:\n\na restricted cubic spline with 5 knots on x9\n\na restricted cubic spline with 3 knots on x6\n\na polynomial in 2 degrees on x14\n\nlinear terms for x1 and x13\n\n\nbut this model was hard to evaluate in some ways. Now, instead of using lm to fit this model, we’ll use a new function called ols from the rms package developed by Frank Harrell and colleagues, in part to support ideas developed in Harrell (2001) for clinical prediction models.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Using `ols` to fit linear models</span>"
    ]
  },
  {
    "objectID": "34-olsfits.html#fitting-a-model-with-ols",
    "href": "34-olsfits.html#fitting-a-model-with-ols",
    "title": "\n34  Using ols to fit linear models\n",
    "section": "\n34.3 Fitting a model with ols\n",
    "text": "34.3 Fitting a model with ols\n\nWe will use the datadist approach when fitting a linear model with ols from the rms package, so as to store additional important elements of the model fit.\n\npollution &lt;- read_csv(\"data/pollution.csv\",\n                      show_col_types = FALSE)\n\nd &lt;- datadist(pollution)\noptions(datadist = \"d\")\n\nNext, we’ll fit the model using ols and place its results in newmod.\n\nnewmod &lt;- ols(y ~ rcs(x9, 5) + rcs(x6, 3) + pol(x14, 2) + \n                  x1 + x13, \n              data = pollution, x = TRUE, y = TRUE)\nnewmod\n\nLinear Regression Model\n\nols(formula = y ~ rcs(x9, 5) + rcs(x6, 3) + pol(x14, 2) + x1 + \n    x13, data = pollution, x = TRUE, y = TRUE)\n\n                 Model Likelihood    Discrimination    \n                       Ratio Test           Indexes    \nObs       60    LR chi2     72.02    R2       0.699    \nsigma37.4566    d.f.           10    R2 adj   0.637    \nd.f.      49    Pr(&gt; chi2) 0.0000    g       58.961    \n\nResiduals\n\n    Min      1Q  Median      3Q     Max \n-86.189 -18.554  -1.799  18.645 104.307 \n\n          Coef      S.E.     t     Pr(&gt;|t|)\nIntercept  796.2658 162.3269  4.91 &lt;0.0001 \nx9          -2.6328   6.3504 -0.41 0.6803  \nx9'        121.4651 124.4827  0.98 0.3340  \nx9''      -219.8025 227.6775 -0.97 0.3391  \nx9'''      151.5700 171.3867  0.88 0.3808  \nx6           7.6817  15.5230  0.49 0.6229  \nx6'        -29.4388  18.0531 -1.63 0.1094  \nx14          0.5652   0.2547  2.22 0.0311  \nx14^2       -0.0010   0.0010 -0.96 0.3407  \nx1           1.0717   0.7317  1.46 0.1494  \nx13         -0.1028   0.1443 -0.71 0.4796  \n\n\nSome of the advantages and disadvantages of fitting linear regression models with ols or lm will reveal themselves over time. For now, one advantage for ols is that the entire variance-covariance matrix is saved. Most of the time, there will be some value to considering both ols and lm approaches.\nMost of this output should be familiar, but a few pieces are different.\n\n34.3.1 The Model Likelihood Ratio Test\nThe Model Likelihood Ratio Test compares newmod to the null model with only an intercept term. It is a goodness-of-fit test that we’ll use in several types of model settings this semester.\n\nIn many settings, the logarithm of the likelihood ratio, multiplied by -2, yields a value which can be compared to a \\(\\chi^2\\) distribution. So here, the value 72.02 is -2(log likelihood), and is compared to a \\(\\chi^2\\) distribution with 10 degrees of freedom. We reject the null hypothesis that newmod is no better than the null model, and conclude instead that at least one of these predictors adds statistically significant value.\n\nFor ols, interpret the model likelihood ratio test like the global (ANOVA) F test in lm.\nThe likelihood function is the probability of observing our data under the specified model.\nWe can compare two nested models by evaluating the difference in their likelihood ratios and degrees of freedom, then comparing the result to a \\(\\chi^2\\) distribution.\n\n\n\n34.3.2 The g statistic\nThe g statistic is new and is referred to as the g-index. it’s based on Gini’s mean difference and is purported to be a robust and highly efficient measure of variation.\n\nHere, g = 58.9, which implies that if you randomly select two of the 60 areas included in the model, the average difference in predicted y (Age-Adjusted Mortality Rate) using this model will be 58.9.\n\nTechnically, g is Gini’s mean difference of the predicted values.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Using `ols` to fit linear models</span>"
    ]
  },
  {
    "objectID": "34-olsfits.html#anova-for-an-ols-model",
    "href": "34-olsfits.html#anova-for-an-ols-model",
    "title": "\n34  Using ols to fit linear models\n",
    "section": "\n34.4 ANOVA for an ols model",
    "text": "34.4 ANOVA for an ols model\nOne advantage of the ols approach is that when you apply an anova to it, it separates out the linear and non-linear components of restricted cubic splines and polynomial terms (as well as product terms, if your model includes them.)\n\nanova(newmod)\n\n                Analysis of Variance          Response: y \n\n Factor          d.f. Partial SS  MS         F     P     \n x9               4    35219.7647  8804.9412  6.28 0.0004\n  Nonlinear       3     1339.3081   446.4360  0.32 0.8121\n x6               2     9367.6008  4683.8004  3.34 0.0437\n  Nonlinear       1     3730.7388  3730.7388  2.66 0.1094\n x14              2    18679.6957  9339.8478  6.66 0.0028\n  Nonlinear       1     1298.7625  1298.7625  0.93 0.3407\n x1               1     3009.1829  3009.1829  2.14 0.1494\n x13              1      711.9108   711.9108  0.51 0.4796\n TOTAL NONLINEAR  5     6656.1824  1331.2365  0.95 0.4582\n REGRESSION      10   159563.8285 15956.3829 11.37 &lt;.0001\n ERROR           49    68746.8004  1402.9959             \n\n\nUnlike the anova approach in lm, in ols ANOVA, partial F tests are presented - each predictor is assessed as “last predictor in” much like the usual t tests in lm. In essence, the partial sums of squares and F tests here describe the marginal impact of removing each covariate from newmod.\nWe conclude that the non-linear parts of x9 and x6 and x14 combined don’t seem to add much value, but that overall, x9, x6 and x14 seem to be valuable. So it must be the linear parts of those variables within our model that are doing the lion’s share of the work.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Using `ols` to fit linear models</span>"
    ]
  },
  {
    "objectID": "34-olsfits.html#effect-estimates",
    "href": "34-olsfits.html#effect-estimates",
    "title": "\n34  Using ols to fit linear models\n",
    "section": "\n34.5 Effect Estimates",
    "text": "34.5 Effect Estimates\nA particularly useful thing to get out of the ols approach that is not as easily available in lm (without recoding or standardizing our predictors) is a summary of the effects of each predictor in an interesting scale.\n\nsummary(newmod)\n\n             Effects              Response : y \n\n Factor Low   High  Diff. Effect   S.E.    Lower 0.95 Upper 0.95\n x9      4.95 15.65 10.70  40.4060 14.0790  12.1120   68.6990   \n x6     10.40 11.50  1.10 -18.2930  8.1499 -34.6710   -1.9153   \n x14    11.00 69.00 58.00  28.3480 10.6480   6.9503   49.7460   \n x1     32.75 43.25 10.50  11.2520  7.6833  -4.1878   26.6930   \n x13     4.00 23.75 19.75  -2.0303  2.8502  -7.7579    3.6973   \n\n\nThis “effects summary” shows the effect on y of moving from the 25th to the 75th percentile of each variable (along with a standard error and 95% confidence interval) while holding the other variable at the level specified at the bottom of the output.\nThe most useful way to look at this sort of analysis is often a plot.\n\nplot(summary(newmod))\n\n\n\n\n\n\n\nFor x9 note from the summary above that the 25th percentile is 4.95 and the 75th is 15.65. Our conclusion is that the estimated effect of moving x9 from 4.95 to 15.65 is an increase of 40.4 on y, with a 95% CI of (12.1, 68.7).\nFor a categorical variable, the low level is shown first and then the high level.\nThe plot shows the point estimate (arrow head) and then the 90% (narrowest bar), 95% (middle bar) and 99% (widest bar in lightest color) confidence intervals for each predictor’s effect.\n\nIt’s easier to distinguish this in the x9 plot than the one for x13.\nRemember that what is being compared is the first value to the second value’s impact on the outcome, with other predictors held constant.\n\n\n34.5.1 Simultaneous Confidence Intervals\nThese confidence intervals make no effort to deal with the multiple comparisons problem, but just fit individual 95% (or whatever level you choose) confidence intervals for each predictor. The natural alternative is to make an adjustment for multiple comparisons in fitting the confidence intervals, so that the set of (in this case, five - one for each predictor) confidence intervals for effect sizes has a family-wise 95% confidence level. You’ll note that the effect estimates and standard errors are unchanged from those shown above, but the confidence limits are a bit wider.\n\nsummary(newmod, conf.type=c('simultaneous'))\n\n             Effects              Response : y \n\n Factor Low   High  Diff. Effect   S.E.    Lower 0.95 Upper 0.95\n x9      4.95 15.65 10.70  40.4060 14.0790   3.11410  77.6970   \n x6     10.40 11.50  1.10 -18.2930  8.1499 -39.87900   3.2932   \n x14    11.00 69.00 58.00  28.3480 10.6480   0.14536  56.5510   \n x1     32.75 43.25 10.50  11.2520  7.6833  -9.09810  31.6030   \n x13     4.00 23.75 19.75  -2.0303  2.8502  -9.57940   5.5188   \n\n\nRemember that if you’re looking for the usual lm summary for an ols object, use summary.lm, and that the display function from arm does not recognize ols objects.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Using `ols` to fit linear models</span>"
    ]
  },
  {
    "objectID": "34-olsfits.html#the-predict-function-for-an-ols-model",
    "href": "34-olsfits.html#the-predict-function-for-an-ols-model",
    "title": "\n34  Using ols to fit linear models\n",
    "section": "\n34.6 The Predict function for an ols model",
    "text": "34.6 The Predict function for an ols model\nThe Predict function is very flexible, and can be used to produce individual or simultaneous confidence limits.\n\nPredict(newmod, x9 = 12, x6 = 12, x14 = 40, \n        x1 = 40, x13 = 20) # individual limits\n\n  x9 x6 x14 x1 x13     yhat    lower   upper\n1 12 12  40 40  20 923.0982 893.0984 953.098\n\nResponse variable (y): y \n\nLimits are 0.95 confidence limits\n\nPredict(newmod, x9 = 5:15) # individual limits\n\n   x9    x6 x14 x1 x13     yhat    lower    upper\n1   5 11.05  30 38   9 913.7392 889.4802 937.9983\n2   6 11.05  30 38   9 916.3490 892.0082 940.6897\n3   7 11.05  30 38   9 921.3093 898.9657 943.6529\n4   8 11.05  30 38   9 927.6464 907.0355 948.2574\n5   9 11.05  30 38   9 934.3853 913.3761 955.3946\n6  10 11.05  30 38   9 940.5510 917.8371 963.2648\n7  11 11.05  30 38   9 945.2225 921.9971 968.4479\n8  12 11.05  30 38   9 948.2885 926.4576 970.1194\n9  13 11.05  30 38   9 950.2608 930.3003 970.2213\n10 14 11.05  30 38   9 951.6671 932.2370 971.0971\n11 15 11.05  30 38   9 953.0342 932.1662 973.9021\n\nResponse variable (y): y \n\nAdjust to: x6=11.05 x14=30 x1=38 x13=9  \n\nLimits are 0.95 confidence limits\n\nPredict(newmod, x9 = 5:15, conf.type = 'simult')\n\n   x9    x6 x14 x1 x13     yhat    lower    upper\n1   5 11.05  30 38   9 913.7392 882.4988 944.9797\n2   6 11.05  30 38   9 916.3490 885.0033 947.6946\n3   7 11.05  30 38   9 921.3093 892.5355 950.0831\n4   8 11.05  30 38   9 927.6464 901.1040 954.1889\n5   9 11.05  30 38   9 934.3853 907.3299 961.4408\n6  10 11.05  30 38   9 940.5510 911.3004 969.8016\n7  11 11.05  30 38   9 945.2225 915.3131 975.1319\n8  12 11.05  30 38   9 948.2885 920.1750 976.4020\n9  13 11.05  30 38   9 950.2608 924.5560 975.9656\n10 14 11.05  30 38   9 951.6671 926.6453 976.6888\n11 15 11.05  30 38   9 953.0342 926.1607 979.9077\n\nResponse variable (y): y \n\nAdjust to: x6=11.05 x14=30 x1=38 x13=9  \n\nLimits are 0.95 confidence limits\n\n\nThe plot below shows the individual effects in newmod in five subpanels, using the default approach of displaying the same range of values as are seen in the data. Note that each panel shows point and interval estimates of the effects, and spot the straight lines in x1 and x13, the single bends in x14 and x6 and the wiggles in x9, corresponding to the amount of non-linearity specified in the model.\n\nggplot(Predict(newmod))",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Using `ols` to fit linear models</span>"
    ]
  },
  {
    "objectID": "34-olsfits.html#checking-influence-via-dfbeta",
    "href": "34-olsfits.html#checking-influence-via-dfbeta",
    "title": "\n34  Using ols to fit linear models\n",
    "section": "\n34.7 Checking Influence via dfbeta\n",
    "text": "34.7 Checking Influence via dfbeta\n\nFor an ols object, we have several tools for looking at residuals. The most interesting to me is which.influence which is reliant on the notion of dfbeta.\n\nDFBETA is estimated for each observation in the data, and each coefficient in the model.\nThe DFBETA is the difference in the estimated coefficient caused by deleting the observation, scaled by the coefficient’s standard error estimated with the observation deleted.\nThe which.influence command applied to an ols model produces a list of all of the predictors estimated by the model, including the intercept.\n\nFor each predictor, the command lists all observations (by row number) that, if removed from the model, would cause the estimated coefficient (the “beta”) for that predictor to change by at least some particular cutoff.\nThe default is that the DFBETA for that predictor is 0.2 or more.\n\n\n\n\nwhich.influence(newmod)\n\n$Intercept\n[1] \"2\"  \"11\" \"28\" \"32\" \"37\" \"49\" \"59\"\n\n$x9\n[1] \"2\"  \"3\"  \"6\"  \"9\"  \"31\" \"35\" \"49\" \"57\" \"58\"\n\n$x6\n[1] \"2\"  \"11\" \"15\" \"28\" \"32\" \"37\" \"50\" \"56\" \"59\"\n\n$x14\n[1] \"2\"  \"6\"  \"7\"  \"12\" \"13\" \"16\" \"32\" \"37\"\n\n$x1\n[1] \"7\"  \"18\" \"32\" \"37\" \"49\" \"57\"\n\n$x13\n[1] \"29\" \"32\" \"37\"\n\n\nThe implication here, for instance, is that if we drop row 3 from our data frame, and refit the model, this will have a meaningful impact on the estimate of x9 but not on the other coefficients. But if we drop, say, row 37, we will affect the estimates of the intercept, x6, x14, x1, and x13.\n\n34.7.1 Using the residuals command for dfbetas\n\nTo see the dfbeta values, standardized according to the approach I used above, you can use the following code (I’ll use head to just show the first few rows of results) to get a matrix of the results.\n\nhead(residuals(newmod, type = \"dfbetas\"))\n\n    Intercept           x9          x9'        x9''       x9'''          x6\n1  0.03071160 -0.023775487 -0.004055111  0.01205425 -0.03260003 -0.02392315\n2 -0.38276573 -0.048404993 -0.142293606  0.17009666 -0.22350621  0.44737372\n3  0.17226780 -0.426153536  0.350913139 -0.32949129  0.25777913 -0.10263448\n4  0.06175110 -0.006460916  0.024828272 -0.03009337  0.04154812 -0.06254145\n5  0.16875200  0.039839994 -0.058178534  0.06449504 -0.07772208 -0.18058630\n6  0.03322073  0.112699877 -0.203543632  0.23987378 -0.35201736 -0.04075617\n          x6'         x14        x14^2           x1         x13\n1  0.01175375 -0.06494414  0.060929683 -0.011042644  0.03425156\n2 -0.48562818  0.19372285 -0.212186731 -0.107830147 -0.01503250\n3  0.05005284 -0.02049877  0.014059330  0.010793169  0.04924166\n4  0.05498432  0.01135031 -0.001877983 -0.005490454 -0.01254111\n5  0.16151742  0.02723710  0.065483158  0.003326357 -0.05570035\n6  0.02900006 -0.21508009  0.171627718  0.019241676  0.05775536\n\n\n\n34.7.2 Using the residuals command for other summaries\nThe residuals command will also let you get ordinary residuals, leverage values and dffits values, which are the normalized differences in predicted values when observations are omitted. See ?residuals.ols for more details.\n\ntemp &lt;- tibble(area = 1:60)\ntemp$residual &lt;- residuals(newmod, type = \"ordinary\")\ntemp$leverage &lt;- residuals(newmod, type = \"hat\")\ntemp$dffits &lt;- residuals(newmod, type = \"dffits\")\n\nggplot(temp, aes(x = area, y = dffits)) +\n    geom_point() +\n    geom_line()\n\n\n\n\n\n\n\nIt appears that point 37 has the largest (positive) dffits value. Recall that point 37 seemed influential on several predictors and the intercept term. Point 32 has the smallest (or largest negative) dffits, and also appears to have been influential on several predictors and the intercept.\n\nwhich.max(temp$dffits)\n\n37 \n37 \n\nwhich.min(temp$dffits)\n\n32 \n32",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Using `ols` to fit linear models</span>"
    ]
  },
  {
    "objectID": "34-olsfits.html#model-validation-and-correcting-for-optimism",
    "href": "34-olsfits.html#model-validation-and-correcting-for-optimism",
    "title": "\n34  Using ols to fit linear models\n",
    "section": "\n34.8 Model Validation and Correcting for Optimism",
    "text": "34.8 Model Validation and Correcting for Optimism\nWe have learned about splitting our regression models into training samples and test samples, performing variable selection work on the training sample to identify two or three candidate models (perhaps via a stepwise approach), and then comparing the predictions made by those models in a test sample.\nThe validate function allows us to perform cross-validation of our models for some summary statistics (and then correct those statistics for optimism in describing likely predictive accuracy) in an easy way.\nvalidate develops:\n\nResampling validation with or without backward elimination of variables\nEstimates of the optimism in measures of predictive accuracy\nEstimates of the intercept and slope of a calibration model\n\nwith the following code…\n\nset.seed(432002); validate(newmod, method = \"boot\", B = 40)\n\n          index.orig training      test  optimism index.corrected  n\nR-square      0.6989   0.7426    0.5749    0.1676          0.5312 40\nMSE        1145.7800 963.9565 1617.4042 -653.4478       1799.2278 40\ng            58.9614  59.7891   54.6444    5.1447         53.8168 40\nIntercept     0.0000   0.0000   96.6990  -96.6990         96.6990 40\nSlope         1.0000   1.0000    0.8961    0.1039          0.8961 40\n\n\nSo, for R-square we see that our original estimate was 0.6989\n\nOur estimated R-square across n = 40 training samples was 0.7426, but in the resulting tests, the average R-square was only 0.5749\nThis suggests an optimism of 0.7426 - 0.5749 = 0.1676 (after rounding).\nWe then apply that optimism to obtain a new estimate of R2 corrected for overfitting, at 0.5312, which is probably a better estimate of what our results might look like in new data that were similar to (but not the same as) the data we used in building newmod than our initial estimate of 0.6989\n\nWe also obtain optimism-corrected estimates of the mean squared error (square of the residual standard deviation), the g index, and the intercept and slope of the calibration model. The “corrected” slope is a shrinkage factor that takes overfitting into account.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Using `ols` to fit linear models</span>"
    ]
  },
  {
    "objectID": "34-olsfits.html#building-a-nomogram-for-our-model",
    "href": "34-olsfits.html#building-a-nomogram-for-our-model",
    "title": "\n34  Using ols to fit linear models\n",
    "section": "\n34.9 Building a Nomogram for Our Model",
    "text": "34.9 Building a Nomogram for Our Model\nAnother nice feature of an ols model object is that we can picture the model with a nomogram easily. Here is model newmod.\n\nplot(nomogram(newmod))\n\n\n\n\n\n\n\nFor this model, we can use this plot to predict y as follows:\n\nfind our values of x9 on the appropriate line\ndraw a vertical line up to the points line to count the points associated with our subject\nrepeat the process to obtain the points associated with x6, x14, x1, and x13. Sum the points.\ndraw a vertical line down from that number in the Total Points line to estimate y (the Linear Predictor) = Age-Adjusted Mortality Rate.\n\nThe impact of the non-linearity is seen in the x6 results, for example, which turn around from 9-10 to 11-12. We also see non-linearity’s effects in the scales of the non-linear terms in terms of points awarded.\nAn area with a combination of predictor values leading to a total of 100 points, for instance, would lead to a prediction of a Mortality Rate near 905. An area with a total of 140 points would have a predicted Mortality Rate of 955, roughly.\n\n\n\n\nHarrell, Frank E. 2001. Regression Modeling Strategies. New York: Springer.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Using `ols` to fit linear models</span>"
    ]
  },
  {
    "objectID": "35-emp_bmi.html",
    "href": "35-emp_bmi.html",
    "title": "\n35  BMI and Employment Study\n",
    "section": "",
    "text": "35.1 Setup: Packages Used Here\nknitr::opts_chunk$set(comment = NA)\n\nlibrary(GGally)\nlibrary(mice)\nlibrary(tidyverse)\n\ntheme_set(theme_bw())\nWe’ll also use a function from the Hmisc package.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>BMI and Employment Study</span>"
    ]
  },
  {
    "objectID": "35-emp_bmi.html#the-data",
    "href": "35-emp_bmi.html#the-data",
    "title": "\n35  BMI and Employment Study\n",
    "section": "\n35.2 The Data",
    "text": "35.2 The Data\nA 2016 study published in BMJ Open looked at the differential relationship between employment status and body-mass index among middle-aged and elderly adults living in South Korea. Data from this study were available online thanks to the Dryad data package. The original data came from a nationally representative sample of 7228 participants in the Korean Longitudinal Study of Aging. I sampled these data, and did some data “rectangling” (wrangling) to build the emp_bmi.csv file on our web site.\nThe available data in emp_bmi describe 999 subjects, and included are 8 variables:\n\nemp_bmi &lt;- read_csv(\"data/emp_bmi.csv\", show_col_types = FALSE)\n\n\n\nVariable\nDescription\nNA?\n\n\n\npid\nsubject identification number (categorical)\n0\n\n\nbmi\nour outcome, quantitative, body-mass index\n0\n\n\nage\nsubject’s age (between 51 and 95)\n1\n\n\ngender\nsubject’s gender (male or female)\n0\n\n\nemployed\nemployment status indicator (1/0)\n1\n\n\nmarried\nmarital status indicator (1/0)\n1\n\n\nalcohol\n3-level factor\n2\n\n\neducation\n4-level factor\n5\n\n\n\n\nHmisc::describe(emp_bmi)\n\nemp_bmi \n\n 8  Variables      999  Observations\n--------------------------------------------------------------------------------\npid \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     999        0      999        1    31774    20178     3506     6961 \n     .25      .50      .75      .90      .95 \n   16671    31761    46902    54635    58433 \n\nlowest :    22    41    52    82   112, highest: 61471 61481 61532 61641 61691\n--------------------------------------------------------------------------------\nbmi \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     999        0      373        1    23.24    2.808    19.21    20.13 \n     .25      .50      .75      .90      .95 \n   21.54    23.24    24.78    26.56    27.34 \n\nlowest : 15.6  15.63 16.22 16.44 16.53, highest: 31.11 31.24 32.05 33.5  34.67\n--------------------------------------------------------------------------------\nage \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n     998        1       43    0.999    66.29    11.56       52       53 \n     .25      .50      .75      .90      .95 \n      57       66       74       81       83 \n\nlowest : 51 52 53 54 55, highest: 89 90 92 93 95\n--------------------------------------------------------------------------------\ngender \n       n  missing distinct \n     999        0        2 \n                        \nValue      female   male\nFrequency     570    429\nProportion  0.571  0.429\n--------------------------------------------------------------------------------\nemployed \n       n  missing distinct     Info      Sum     Mean      Gmd \n     998        1        2    0.723      404   0.4048   0.4824 \n\n--------------------------------------------------------------------------------\nmarried \n       n  missing distinct     Info      Sum     Mean      Gmd \n     998        1        2    0.548      758   0.7595   0.3657 \n\n--------------------------------------------------------------------------------\nalcohol \n       n  missing distinct \n     997        2        3 \n                                                                      \nValue                  alcohol dependent                 heavy drinker\nFrequency                             45                           306\nProportion                         0.045                         0.307\n                                        \nValue      normal drinker or non-drinker\nFrequency                            646\nProportion                         0.648\n--------------------------------------------------------------------------------\neducation \n       n  missing distinct \n     994        5        4 \n                                                                  \nValue      1 elem school grad or lower        2 middle school grad\nFrequency                          421                         180\nProportion                       0.424                       0.181\n                                                                  \nValue               3 high school grad    4 college grad or higher\nFrequency                          292                         101\nProportion                       0.294                       0.102\n--------------------------------------------------------------------------------\n\n\n\n35.2.1 Specifying Outcome and Predictors for our Model\nIn the original study, a key goal was to understand the relationship between employment and body-mass index. Our goal in this example will be to create a model to predict bmi focusing on employment status (so our key predictor is employed) while accounting for the additional predictors age, gender, married, alcohol and education. A natural thing to do would be to consider interactions of these predictor variables (for example, does the relationship between bmi and employed change when comparing men to women?) but we’ll postpone that discussion until 432.\n\n35.2.2 Dealing with Missing Predictor Values\n\nmd.pattern(emp_bmi)\n\n\n\n\n\n\n\n    pid bmi gender age employed married alcohol education   \n990   1   1      1   1        1       1       1         1  0\n5     1   1      1   1        1       1       1         0  1\n1     1   1      1   1        1       1       0         1  1\n1     1   1      1   1        1       0       1         1  1\n1     1   1      1   1        0       1       1         1  1\n1     1   1      1   0        1       1       0         1  2\n      0   0      0   1        1       1       2         5 10\n\n\nWe will eventually build a model to predict bmi using all of the other variables besides pid. So we’ll eventually have to account for the 9 people with missing values (one of whom has two missing values, as we see above.) What I’m going to do in this example is to first build a complete-case analysis on the 990 subjects without missing values and then, later, do multiple imputation to account for the 9 subjects with missing values (and their 10 actual missing values) sensibly.\nI’ll put the “complete cases” data set of 990 subjects in emp_bmi_noNA.\n\nemp_bmi_noNA &lt;- emp_bmi |&gt; na.omit()\nemp_bmi_noNA\n\n# A tibble: 990 × 8\n     pid   bmi   age gender employed married alcohol                   education\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                     &lt;chr&gt;    \n 1    22  20.8    58 male          1       1 heavy drinker             4 colleg…\n 2    41  21.4    76 female        0       1 normal drinker or non-dr… 1 elem s…\n 3    52  20.9    66 female        1       1 heavy drinker             1 elem s…\n 4    82  23.7    67 male          1       1 alcohol dependent         2 middle…\n 5   112  25.5    63 female        0       1 heavy drinker             1 elem s…\n 6   181  20.5    51 female        1       1 heavy drinker             3 high s…\n 7   182  25.3    51 male          1       1 alcohol dependent         3 high s…\n 8   411  20.8    66 female        1       1 normal drinker or non-dr… 2 middle…\n 9   491  20.8    64 female        0       1 normal drinker or non-dr… 3 high s…\n10   531  24.3    84 female        0       0 normal drinker or non-dr… 1 elem s…\n# ℹ 980 more rows\n\ncolSums(is.na(emp_bmi_noNA))\n\n      pid       bmi       age    gender  employed   married   alcohol education \n        0         0         0         0         0         0         0         0",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>BMI and Employment Study</span>"
    ]
  },
  {
    "objectID": "35-emp_bmi.html#the-kitchen-sink-model",
    "href": "35-emp_bmi.html#the-kitchen-sink-model",
    "title": "\n35  BMI and Employment Study\n",
    "section": "\n35.3 The “Kitchen Sink” Model",
    "text": "35.3 The “Kitchen Sink” Model\nA “kitchen sink” model includes all available predictors.\n\nebmodel.1 &lt;- lm(bmi ~ age + gender + employed + married + \n                    alcohol + education, data = emp_bmi_noNA)\nsummary(ebmodel.1)\n\n\nCall:\nlm(formula = bmi ~ age + gender + employed + married + alcohol + \n    education, data = emp_bmi_noNA)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.7033 -1.6057 -0.0494  1.4992 11.7303 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                          26.15391    0.88356  29.601  &lt; 2e-16 ***\nage                                  -0.04260    0.01065  -3.998 6.86e-05 ***\ngendermale                            0.29811    0.20271   1.471   0.1417    \nemployed                             -0.45761    0.19153  -2.389   0.0171 *  \nmarried                               0.09438    0.21280   0.444   0.6575    \nalcoholheavy drinker                  0.25317    0.40727   0.622   0.5343    \nalcoholnormal drinker or non-drinker  0.14121    0.40766   0.346   0.7291    \neducation2 middle school grad        -0.28862    0.24020  -1.202   0.2298    \neducation3 high school grad          -0.50123    0.22192  -2.259   0.0241 *  \neducation4 college grad or higher    -0.79862    0.31068  -2.571   0.0103 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.513 on 980 degrees of freedom\nMultiple R-squared:  0.02251,   Adjusted R-squared:  0.01353 \nF-statistic: 2.508 on 9 and 980 DF,  p-value: 0.007716",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>BMI and Employment Study</span>"
    ]
  },
  {
    "objectID": "35-emp_bmi.html#using-categorical-variables-factors-as-predictors",
    "href": "35-emp_bmi.html#using-categorical-variables-factors-as-predictors",
    "title": "\n35  BMI and Employment Study\n",
    "section": "\n35.4 Using Categorical Variables (Factors) as Predictors",
    "text": "35.4 Using Categorical Variables (Factors) as Predictors\nWe have six predictors here, and five of them are categorical. Note that R recognizes each kind of variable in this case and models them appropriately. Let’s look at the coefficients of our model.\n\n35.4.1 gender: A binary variable represented by letters\nThe gender variable contains the two categories: male and female, and R recognizes this as a factor. When building a regression model with such a variable, R assigns the first of the two levels of the factor to the baseline, and includes in the model an indicator variable for the second level. By default, R assigns each factor a level order alphabetically.\nSo, in this case, we have:\n\nis.factor(emp_bmi_noNA$gender)\n\n[1] FALSE\n\nlevels(emp_bmi_noNA$gender)\n\nNULL\n\n\nAs you see in the model, the gender information is captured by the indicator variable gendermale, which is 1 when gender = male and 0 otherwise.\nSo, when our model includes:\nCoefficients:     Estimate Std. Error t value Pr(&gt;|t|)    \ngendermale         0.29811    0.20271   1.471   0.1417    \nthis means that a male subject is predicted to have an outcome that is 0.29811 points higher than a female subject, if they have the same values of all of the other predictors.\nNote that if we wanted to switch the levels so that “male” came first (and so that R would use “male” as the baseline category and “female” as the 1 value in an indicator), we could do so with the forcats package and the fct_relevel command. Building a model with this version of gender will simply reverse the sign of our indicator variable, but not change any of the other output.\n\nemp_bmi_noNA$gender.2 &lt;- fct_relevel(emp_bmi_noNA$gender, \"male\", \"female\")\nrevised.model &lt;- lm(bmi ~ age + gender.2 + employed + married + \n                    alcohol + education, data = emp_bmi_noNA)\nsummary(revised.model)\n\n\nCall:\nlm(formula = bmi ~ age + gender.2 + employed + married + alcohol + \n    education, data = emp_bmi_noNA)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.7033 -1.6057 -0.0494  1.4992 11.7303 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                          26.45202    0.94648  27.948  &lt; 2e-16 ***\nage                                  -0.04260    0.01065  -3.998 6.86e-05 ***\ngender.2female                       -0.29811    0.20271  -1.471   0.1417    \nemployed                             -0.45761    0.19153  -2.389   0.0171 *  \nmarried                               0.09438    0.21280   0.444   0.6575    \nalcoholheavy drinker                  0.25317    0.40727   0.622   0.5343    \nalcoholnormal drinker or non-drinker  0.14121    0.40766   0.346   0.7291    \neducation2 middle school grad        -0.28862    0.24020  -1.202   0.2298    \neducation3 high school grad          -0.50123    0.22192  -2.259   0.0241 *  \neducation4 college grad or higher    -0.79862    0.31068  -2.571   0.0103 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.513 on 980 degrees of freedom\nMultiple R-squared:  0.02251,   Adjusted R-squared:  0.01353 \nF-statistic: 2.508 on 9 and 980 DF,  p-value: 0.007716\n\n\nNote that the two categories here need to be both mutually exclusive (a subject cannot be in more than one category) and collectively exhaustive (all subjects must fit into this set of categories) in order to work properly as a regression predictor.\n\n35.4.2 employed: A binary variable represented a 1/0 indicator\nThe employed and married variables are each described using an indicator variable, which is 1 if the condition of interest holds and 0 if it does not. R doesn’t recognize this as a factor, but rather as a quantitative variable. However, this is no problem for modeling, where we just need to remember that if employed = 1, the subject is employed, and if employed = 0, the subject is not employed, to interpret the results. The same approach is used for married.\nCoefficients:    Estimate Std. Error t value Pr(&gt;|t|)    \nemployed         -0.45761    0.19153  -2.389   0.0171 *  \nmarried           0.09438    0.21280   0.444   0.6575    \nSo, in our model, if subject A is employed, they are expected to have an outcome that is 0.46 points lower (-0.46 points higher) than subject B who is not employed but otherwise identical to subject A.\nSimilarly, if subject X is married, and subject Y is unmarried, but they otherwise have the same values of all predictors, then our model will predict a bmi for X that is 0.094 points higher than for Y.\n\n35.4.3 alcohol: A three-category variable coded by names\nOur alcohol information divides subjects into three categories, which are:\n\nnormal drinker or non-drinker\nheavy drinker\nalcohol dependent\n\nR builds a model using \\(k-1\\) predictors to describe a variable with \\(k\\) levels. As mentioned previously, R selects a baseline category when confronted with a factor variable, and it always selects the first level as the baseline. The levels are sorted alphabetically, unless we tell R to sort them some other way. So, we have\nCoefficients:                        Estimate Std. Error t value Pr(&gt;|t|)    \nalcoholheavy drinker                  0.25317    0.40727   0.622   0.5343    \nalcoholnormal drinker or non-drinker  0.14121    0.40766   0.346   0.7291    \nHow do we interpret this?\n\nSuppose subject A is alcohol dependent, B is a heavy drinker and C is a normal drinker or non-drinker, but subjects A-C have the same values of all other predictors.\nOur model predicts that B would have a BMI that is 0.25 points higher than A.\nOur model predicts that C would have a BMI that is 0.14 points higher than A.\n\nA good way to think about this…\n\n\n\n\n\n\n\n\nSubject\nStatus\nalcoholheavy drinker\nalcoholnormal drinker or non-drinker\n\n\n\nA\nalcohol dependent\n0\n0\n\n\nB\nheavy drinker\n1\n0\n\n\nC\nnormal drinker or non-drinker\n0\n1\n\n\n\nand so, with two variables, we cover each of these three possible alcohol levels.\nWhen we have an ordered variable like this one, we usually want the baseline category to be at either end of the scale (either the highest or the lowest, but not something in the middle.) Another good idea in many settings is to use as the baseline category the most common category. Here, the baseline R chose was “alcohol dependent” which is the least common category, so I might want to use the fct_relevel function again to force R to choose, say, normal drinker/non-drinker as the baseline category.\n\nemp_bmi_noNA$alcohol.2 &lt;- fct_relevel(emp_bmi_noNA$alcohol, \n           \"normal drinker or non-drinker\", \"heavy drinker\")\nrevised.model.2 &lt;- lm(bmi ~ age + gender + employed + married + \n                    alcohol.2 + education, data = emp_bmi_noNA)\nsummary(revised.model.2)\n\n\nCall:\nlm(formula = bmi ~ age + gender + employed + married + alcohol.2 + \n    education, data = emp_bmi_noNA)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.7033 -1.6057 -0.0494  1.4992 11.7303 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                       26.29512    0.82860  31.734  &lt; 2e-16 ***\nage                               -0.04260    0.01065  -3.998 6.86e-05 ***\ngendermale                         0.29811    0.20271   1.471   0.1417    \nemployed                          -0.45761    0.19153  -2.389   0.0171 *  \nmarried                            0.09438    0.21280   0.444   0.6575    \nalcohol.2heavy drinker             0.11196    0.19647   0.570   0.5689    \nalcohol.2alcohol dependent        -0.14121    0.40766  -0.346   0.7291    \neducation2 middle school grad     -0.28862    0.24020  -1.202   0.2298    \neducation3 high school grad       -0.50123    0.22192  -2.259   0.0241 *  \neducation4 college grad or higher -0.79862    0.31068  -2.571   0.0103 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.513 on 980 degrees of freedom\nMultiple R-squared:  0.02251,   Adjusted R-squared:  0.01353 \nF-statistic: 2.508 on 9 and 980 DF,  p-value: 0.007716\n\n\nHow do we interpret this revised model?\n\nAgain, subject A is alcohol dependent, B is a heavy drinker and C is a normal drinker or non-drinker, but subjects A-C have the same values of all other predictors.\nOur model predicts that B would have a BMI that is 0.11 points higher than C.\nOur model predicts that A would have a BMI that is 0.14 points lower than C.\n\nSo, those are the same conclusions, just rephrased.\n\n35.4.4 t tests and multi-categorical variables\nThe usual “last predictor in” t test works perfectly for binary factors, but suppose we have a factor like alcohol which is represented by two different indicator variables. If we want to know whether the alcohol information, as a group, adds statistically significant value to the model that includes all of the other predictors, then our best strategy is to compare two models - one with the alcohol information, and one without.\n\nmodel.with.a &lt;- lm(bmi ~ age + gender + alcohol + employed + married + education, \n                   data = emp_bmi_noNA)\nmodel.no.a &lt;- lm(bmi ~ age + gender + employed + married + education, \n                 data = emp_bmi_noNA)\nanova(model.with.a, model.no.a)\n\nAnalysis of Variance Table\n\nModel 1: bmi ~ age + gender + alcohol + employed + married + education\nModel 2: bmi ~ age + gender + employed + married + education\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1    980 6189.9                           \n2    982 6193.6 -2   -3.6288 0.2873 0.7504\n\n\nThe p value for both of the indicator variables associated with alcohol combined is 0.75, according to an ANOVA F test with 2 degrees of freedom.\nNote that we can get the same information from an ANOVA table of the larger model if we add the alcohol predictor to the model last.\n\nanova(lm(bmi ~ age + gender + employed + married + education + alcohol, \n         data = emp_bmi_noNA))\n\nAnalysis of Variance Table\n\nResponse: bmi\n           Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nage         1   55.5  55.510  8.7884 0.003105 **\ngender      1    0.4   0.369  0.0585 0.809000   \nemployed    1   30.7  30.733  4.8657 0.027626 * \nmarried     1    0.4   0.374  0.0592 0.807746   \neducation   3   51.9  17.311  2.7408 0.042202 * \nalcohol     2    3.6   1.814  0.2873 0.750382   \nResiduals 980 6189.9   6.316                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAgain, we see p for the two alcohol indicators is 0.75.\n\n35.4.5 education: A four-category variable coded by names\nThe education variable’s codes are a little better designed. By preceding the text with a number for each code, we force R to attend to the level order we want to see.\nCoefficients:                     Estimate Std. Error t value Pr(&gt;|t|)    \neducation2 middle school grad     -0.28862    0.24020  -1.202   0.2298    \neducation3 high school grad       -0.50123    0.22192  -2.259   0.0241 *  \neducation4 college grad or higher -0.79862    0.31068  -2.571   0.0103 *  \nSince we have four education levels, we need those three indicator variables.\n\n\neducation2 middle school grad is 1 if the subject is a middle school graduate, and 0 if they have some other status\n\neducation3 high school grad is 1 if the subject is a high school graduate, and 0 if they have some other status\n\neducation4 college grad or higher is 1 if the subject is a college graduate or has more education, and 0 if they have some other status.\nSo the subjects with only elementary school or lower education are represented by zeros in all three indicators.\n\nSuppose we have four subjects now, with the same values of all other predictors, but different levels of education.\n\n\nSubject\nEducation\nEstimated BMI\n\n\n\nA\nelementary school or less\nA\n\n\nB\nmiddle school grad\nA - 0.289\n\n\nC\nhigh school grad\nA - 0.501\n\n\nD\ncollege grad\nA - 0.799\n\n\n\nNote that the four categories are mutually exclusive (a subject cannot be in more than one category) and collectively exhaustive (all subjects must fit into this set of categories.) As we have seen, this is a requirement of categorical variables in a regression analysis.\nLet’s run the ANOVA test for the education information captured in those three indicator variables…\n\nanova(lm(bmi ~ age + gender + employed + married + alcohol + education, \n         data = emp_bmi_noNA))\n\nAnalysis of Variance Table\n\nResponse: bmi\n           Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nage         1   55.5  55.510  8.7884 0.003105 **\ngender      1    0.4   0.369  0.0585 0.809000   \nemployed    1   30.7  30.733  4.8657 0.027626 * \nmarried     1    0.4   0.374  0.0592 0.807746   \nalcohol     2    3.5   1.750  0.2771 0.758055   \neducation   3   52.1  17.354  2.7476 0.041820 * \nResiduals 980 6189.9   6.316                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSo, as a group, the three indicator variables add statistically significant predictive value at the 5% significance level, since the F test for those three variables has p = 0.042\n\n35.4.6 Interpreting the Kitchen Sink Model\nSo, again, here’s our model.\n\nebmodel.1 &lt;- lm(bmi ~ age + gender + employed + married + \n                    alcohol + education, data = emp_bmi_noNA)\nebmodel.1\n\n\nCall:\nlm(formula = bmi ~ age + gender + employed + married + alcohol + \n    education, data = emp_bmi_noNA)\n\nCoefficients:\n                         (Intercept)                                   age  \n                            26.15391                              -0.04260  \n                          gendermale                              employed  \n                             0.29811                              -0.45761  \n                             married                  alcoholheavy drinker  \n                             0.09438                               0.25317  \nalcoholnormal drinker or non-drinker         education2 middle school grad  \n                             0.14121                              -0.28862  \n         education3 high school grad     education4 college grad or higher  \n                            -0.50123                              -0.79862  \n\n\nIf we wanted to predict a BMI level for a new subject like the ones used in the development of this model, that prediction would be:\n\n26.15\nminus 0.426 times the subject’s age\n\nplus 0.298 if the subject’s gender was male\n\nminus 0.458 if the subject’s employment status was employed\n\nplus 0.253 if the subject’s alcohol classification was heavy drinker\n\nplus 0.141 if the subject’s alcohol classification was normal drinker or non-drinker\n\nminus 0.289 if the subject’s education classification was 2 middle school grad\n\nminus 0.501 if the subject’s education classification was 3 high school grad\n\nminus 0.799 if the subject’s education classification was 4 college grad or higher",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>BMI and Employment Study</span>"
    ]
  },
  {
    "objectID": "35-emp_bmi.html#scatterplot-matrix-with-categorical-predictors",
    "href": "35-emp_bmi.html#scatterplot-matrix-with-categorical-predictors",
    "title": "\n35  BMI and Employment Study\n",
    "section": "\n35.5 Scatterplot Matrix with Categorical Predictors",
    "text": "35.5 Scatterplot Matrix with Categorical Predictors\nLet’s look at a scatterplot matrix of a few key predictors, with my favorite approach (at least for quantitative predictors)…\n\nggpairs(emp_bmi_noNA |&gt; select(age, gender, employed, education, bmi))",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>BMI and Employment Study</span>"
    ]
  },
  {
    "objectID": "35-emp_bmi.html#residual-plots-when-we-have-categorical-predictors",
    "href": "35-emp_bmi.html#residual-plots-when-we-have-categorical-predictors",
    "title": "\n35  BMI and Employment Study\n",
    "section": "\n35.6 Residual Plots when we have Categorical Predictors",
    "text": "35.6 Residual Plots when we have Categorical Predictors\nHere are the main residual plots from the kitchen sink model ebmodel.1 defined previously.\n\npar(mfrow=c(2,2))\nplot(ebmodel.1)\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nSometimes, in small samples, the categorical variables will make the regression residuals line up in somewhat strange patterns. But in this case, there’s no real problem. The use of categorical variables also has some impact on leverage, as it’s hard for a subject to be a serious outlier in terms of a predictor if that predictor only has a few possible levels.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>BMI and Employment Study</span>"
    ]
  },
  {
    "objectID": "35-emp_bmi.html#stepwise-regression-and-categorical-predictors",
    "href": "35-emp_bmi.html#stepwise-regression-and-categorical-predictors",
    "title": "\n35  BMI and Employment Study\n",
    "section": "\n35.7 Stepwise Regression and Categorical Predictors",
    "text": "35.7 Stepwise Regression and Categorical Predictors\nWhen R does backwards elimination for stepwise model selection, it makes decisions about each categorical variable as in/out across all of the indicator variables simultaneously, as you’d hope.\n\nstep(ebmodel.1)\n\nStart:  AIC=1834.64\nbmi ~ age + gender + employed + married + alcohol + education\n\n            Df Sum of Sq    RSS    AIC\n- alcohol    2     3.629 6193.6 1831.2\n- married    1     1.243 6191.2 1832.8\n&lt;none&gt;                   6189.9 1834.6\n- gender     1    13.660 6203.6 1834.8\n- education  3    52.063 6242.0 1836.9\n- employed   1    36.057 6226.0 1838.4\n- age        1   100.976 6290.9 1848.7\n\nStep:  AIC=1831.22\nbmi ~ age + gender + employed + married + education\n\n            Df Sum of Sq    RSS    AIC\n- married    1     1.026 6194.6 1829.4\n&lt;none&gt;                   6193.6 1831.2\n- gender     1    19.071 6212.6 1832.3\n- education  3    51.934 6245.5 1833.5\n- employed   1    34.736 6228.3 1834.8\n- age        1   108.599 6302.2 1846.4\n\nStep:  AIC=1829.39\nbmi ~ age + gender + employed + education\n\n            Df Sum of Sq    RSS    AIC\n&lt;none&gt;                   6194.6 1829.4\n- gender     1    22.169 6216.7 1830.9\n- education  3    51.282 6245.9 1831.5\n- employed   1    34.390 6229.0 1832.9\n- age        1   125.431 6320.0 1847.2\n\n\n\nCall:\nlm(formula = bmi ~ age + gender + employed + education, data = emp_bmi_noNA)\n\nCoefficients:\n                      (Intercept)                                age  \n                         26.50206                           -0.04459  \n                       gendermale                           employed  \n                          0.34057                           -0.44571  \n    education2 middle school grad        education3 high school grad  \n                         -0.28787                           -0.49772  \neducation4 college grad or higher  \n                         -0.79037  \n\n\nNote that the stepwise approach first drops two degrees of freedom (two indicator variables) for alcohol and then drops the one degree of freedom for married before it settles on a model with age, gender, education and employed.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>BMI and Employment Study</span>"
    ]
  },
  {
    "objectID": "35-emp_bmi.html#pooling-results-after-multiple-imputation",
    "href": "35-emp_bmi.html#pooling-results-after-multiple-imputation",
    "title": "\n35  BMI and Employment Study\n",
    "section": "\n35.8 Pooling Results after Multiple Imputation",
    "text": "35.8 Pooling Results after Multiple Imputation\nAs mentioned earlier, having built a model using complete cases, we should probably investigate the impact of multiple imputation on the missing observations. We’ll fit 100 imputations using the emp_bmi data and then fit a pooled regression model across those imputations.\n\nemp_bmi_mi &lt;- mice(emp_bmi, m = 100, maxit = 5, \n                   printFlag = FALSE, seed = 4312021)\n\nNow, we’ll fit the pooled kitchen sink regression model to these imputed data sets and pool them.\n\nmodel.empbmi.mi &lt;- \n    with(emp_bmi_mi, lm(bmi ~ age + gender + employed + \n                            married + alcohol + education))\nsummary(pool(model.empbmi.mi))\n\n                                   term    estimate  std.error  statistic\n1                           (Intercept) 26.15304370 0.88053709 29.7012402\n2                                   age -0.04258223 0.01063989 -4.0021319\n3                            gendermale  0.29794032 0.20237455  1.4722223\n4                              employed -0.45695506 0.19112904 -2.3908196\n5                               married  0.09430912 0.21247583  0.4438581\n6                  alcoholheavy drinker  0.25298858 0.40270904  0.6282168\n7  alcoholnormal drinker or non-drinker  0.14108076 0.40341263  0.3497182\n8         education2 middle school grad -0.28861113 0.23994418 -1.2028261\n9           education3 high school grad -0.50119826 0.22138848 -2.2638859\n10    education4 college grad or higher -0.79852096 0.30947717 -2.5802258\n        df       p.value\n1  979.908 9.190979e-139\n2  979.908  6.750699e-05\n3  979.908  1.412819e-01\n4  979.908  1.699872e-02\n5  979.908  6.572432e-01\n6  979.908  5.300083e-01\n7  979.908  7.266253e-01\n8  979.908  2.293342e-01\n9  979.908  2.379907e-02\n10 979.908  1.001814e-02\n\n\nOK. That’s it for now.",
    "crumbs": [
      "Part C. Building Models",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>BMI and Employment Study</span>"
    ]
  },
  {
    "objectID": "98_gettingdataintoR.html",
    "href": "98_gettingdataintoR.html",
    "title": "Appendix A — Getting Data Into R",
    "section": "",
    "text": "Using data from an R package\nTo use data from an R package, for instance, the bechdel data from the fivethirtyeight package, you can simply load the relevant package with library and then the data frame will be available\nlibrary(fivethirtyeight)\nlibrary(tidyverse)\n\nbechdel\n\n# A tibble: 1,794 × 15\n    year imdb      title  test  clean_test binary budget domgross intgross code \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;ord&gt;      &lt;chr&gt;   &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;\n 1  2013 tt1711425 21 & … nota… notalk     FAIL   1.3 e7 25682380   4.22e7 2013…\n 2  2012 tt1343727 Dredd… ok-d… ok         PASS   4.50e7 13414714   4.09e7 2012…\n 3  2013 tt2024544 12 Ye… nota… notalk     FAIL   2   e7 53107035   1.59e8 2013…\n 4  2013 tt1272878 2 Guns nota… notalk     FAIL   6.1 e7 75612460   1.32e8 2013…\n 5  2013 tt0453562 42     men   men        FAIL   4   e7 95020213   9.50e7 2013…\n 6  2013 tt1335975 47 Ro… men   men        FAIL   2.25e8 38362475   1.46e8 2013…\n 7  2013 tt1606378 A Goo… nota… notalk     FAIL   9.2 e7 67349198   3.04e8 2013…\n 8  2013 tt2194499 About… ok-d… ok         PASS   1.20e7 15323921   8.73e7 2013…\n 9  2013 tt1814621 Admis… ok    ok         PASS   1.3 e7 18007317   1.80e7 2013…\n10  2013 tt1815862 After… nota… notalk     FAIL   1.3 e8 60522097   2.44e8 2013…\n# ℹ 1,784 more rows\n# ℹ 5 more variables: budget_2013 &lt;int&gt;, domgross_2013 &lt;dbl&gt;,\n#   intgross_2013 &lt;dbl&gt;, period_code &lt;int&gt;, decade_code &lt;int&gt;",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Getting Data Into R</span>"
    ]
  },
  {
    "objectID": "98_gettingdataintoR.html#using-read_rds-to-read-in-an-r-data-set",
    "href": "98_gettingdataintoR.html#using-read_rds-to-read-in-an-r-data-set",
    "title": "Appendix A — Getting Data Into R",
    "section": "Using read_rds to read in an R data set",
    "text": "Using read_rds to read in an R data set\nWe have provided the nnyfs.Rds data file on the course data page.\nSuppose you have downloaded this data file into a directory on your computer called data which is a sub-directory of the directory where you plan to do your work, perhaps called 431-nnyfs.\nOpen RStudio and create a new project into the 431-nnyfs directory on your computer. You should see a data subdirectory in the Files window in RStudio after the project is created.\nNow, read in the nnyfs.Rds file to a new tibble in R called nnyfs_new with the following command:\n\nnnyfs_new &lt;- read_rds(\"data/nnyfs.Rds\")\n\nHere are the results…\n\nnnyfs_new\n\n# A tibble: 1,518 × 45\n    SEQN sex    age_child race_eth    educ_child language sampling_wt income_pov\n   &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt; &lt;fct&gt;            &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n 1 71917 Female        15 3_Black No…          9 English       28299.       0.21\n 2 71918 Female         8 3_Black No…          2 English       15127.       5   \n 3 71919 Female        14 2_White No…          8 English       29977.       5   \n 4 71920 Female        15 2_White No…          8 English       80652.       0.87\n 5 71921 Male           3 2_White No…         NA English       55592.       4.34\n 6 71922 Male          12 1_Hispanic           6 English       27365.       5   \n 7 71923 Male          12 2_White No…          5 English       86673.       5   \n 8 71924 Female         8 4_Other Ra…          2 English       39549.       2.74\n 9 71925 Male           7 1_Hispanic           0 English       42333.       0.46\n10 71926 Male           8 3_Black No…          2 English       15307.       1.57\n# ℹ 1,508 more rows\n# ℹ 37 more variables: age_adult &lt;dbl&gt;, educ_adult &lt;fct&gt;, respondent &lt;fct&gt;,\n#   salt_used &lt;fct&gt;, energy &lt;dbl&gt;, protein &lt;dbl&gt;, sugar &lt;dbl&gt;, fat &lt;dbl&gt;,\n#   diet_yesterday &lt;fct&gt;, water &lt;dbl&gt;, plank_time &lt;dbl&gt;, height &lt;dbl&gt;,\n#   weight &lt;dbl&gt;, bmi &lt;dbl&gt;, bmi_cat &lt;fct&gt;, arm_length &lt;dbl&gt;, waist &lt;dbl&gt;,\n#   arm_circ &lt;dbl&gt;, calf_circ &lt;dbl&gt;, calf_skinfold &lt;dbl&gt;,\n#   triceps_skinfold &lt;dbl&gt;, subscapular_skinfold &lt;dbl&gt;, active_days &lt;dbl&gt;, …",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Getting Data Into R</span>"
    ]
  },
  {
    "objectID": "98_gettingdataintoR.html#using-read_csv-to-read-in-a-comma-separated-version-of-a-data-file",
    "href": "98_gettingdataintoR.html#using-read_csv-to-read-in-a-comma-separated-version-of-a-data-file",
    "title": "Appendix A — Getting Data Into R",
    "section": "Using read_csv to read in a comma-separated version of a data file",
    "text": "Using read_csv to read in a comma-separated version of a data file\nWe have provided the nnyfs.csv data file on the course data page.\nSuppose you have downloaded this data file into a directory on your computer called data which is a sub-directory of the directory where you plan to do your work, perhaps called 431-nnyfs.\nOpen RStudio and create a new project into the 431-nnyfs directory on your computer. You should see a data subdirectory in the Files window in RStudio after the project is created.\nNow, read in the nnyfs.csv file to a new tibble in R called nnyfs_new2 with the following command:\n\nnnyfs_new2 &lt;- read_csv(\"data/nnyfs.csv\")\n\nRows: 1518 Columns: 45\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (18): sex, race_eth, language, educ_adult, respondent, salt_used, diet_y...\ndbl (27): SEQN, age_child, educ_child, sampling_wt, income_pov, age_adult, e...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnnyfs_new2\n\n# A tibble: 1,518 × 45\n    SEQN sex    age_child race_eth    educ_child language sampling_wt income_pov\n   &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n 1 71917 Female        15 3_Black No…          9 English       28299.       0.21\n 2 71918 Female         8 3_Black No…          2 English       15127.       5   \n 3 71919 Female        14 2_White No…          8 English       29977.       5   \n 4 71920 Female        15 2_White No…          8 English       80652.       0.87\n 5 71921 Male           3 2_White No…         NA English       55592.       4.34\n 6 71922 Male          12 1_Hispanic           6 English       27365.       5   \n 7 71923 Male          12 2_White No…          5 English       86673.       5   \n 8 71924 Female         8 4_Other Ra…          2 English       39549.       2.74\n 9 71925 Male           7 1_Hispanic           0 English       42333.       0.46\n10 71926 Male           8 3_Black No…          2 English       15307.       1.57\n# ℹ 1,508 more rows\n# ℹ 37 more variables: age_adult &lt;dbl&gt;, educ_adult &lt;chr&gt;, respondent &lt;chr&gt;,\n#   salt_used &lt;chr&gt;, energy &lt;dbl&gt;, protein &lt;dbl&gt;, sugar &lt;dbl&gt;, fat &lt;dbl&gt;,\n#   diet_yesterday &lt;chr&gt;, water &lt;dbl&gt;, plank_time &lt;dbl&gt;, height &lt;dbl&gt;,\n#   weight &lt;dbl&gt;, bmi &lt;dbl&gt;, bmi_cat &lt;chr&gt;, arm_length &lt;dbl&gt;, waist &lt;dbl&gt;,\n#   arm_circ &lt;dbl&gt;, calf_circ &lt;dbl&gt;, calf_skinfold &lt;dbl&gt;,\n#   triceps_skinfold &lt;dbl&gt;, subscapular_skinfold &lt;dbl&gt;, active_days &lt;dbl&gt;, …\n\n\nIf you also want to convert the character variables to factors, as you will often want to do before analyzing the results, you should instead use:\n\nnnyfs_new3 &lt;- read_csv(\"data/nnyfs.csv\") %&gt;%\n    mutate(across(where(is.character), as_factor))\n\nRows: 1518 Columns: 45\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (18): sex, race_eth, language, educ_adult, respondent, salt_used, diet_y...\ndbl (27): SEQN, age_child, educ_child, sampling_wt, income_pov, age_adult, e...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnnyfs_new3\n\n# A tibble: 1,518 × 45\n    SEQN sex    age_child race_eth    educ_child language sampling_wt income_pov\n   &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt; &lt;fct&gt;            &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n 1 71917 Female        15 3_Black No…          9 English       28299.       0.21\n 2 71918 Female         8 3_Black No…          2 English       15127.       5   \n 3 71919 Female        14 2_White No…          8 English       29977.       5   \n 4 71920 Female        15 2_White No…          8 English       80652.       0.87\n 5 71921 Male           3 2_White No…         NA English       55592.       4.34\n 6 71922 Male          12 1_Hispanic           6 English       27365.       5   \n 7 71923 Male          12 2_White No…          5 English       86673.       5   \n 8 71924 Female         8 4_Other Ra…          2 English       39549.       2.74\n 9 71925 Male           7 1_Hispanic           0 English       42333.       0.46\n10 71926 Male           8 3_Black No…          2 English       15307.       1.57\n# ℹ 1,508 more rows\n# ℹ 37 more variables: age_adult &lt;dbl&gt;, educ_adult &lt;fct&gt;, respondent &lt;fct&gt;,\n#   salt_used &lt;fct&gt;, energy &lt;dbl&gt;, protein &lt;dbl&gt;, sugar &lt;dbl&gt;, fat &lt;dbl&gt;,\n#   diet_yesterday &lt;fct&gt;, water &lt;dbl&gt;, plank_time &lt;dbl&gt;, height &lt;dbl&gt;,\n#   weight &lt;dbl&gt;, bmi &lt;dbl&gt;, bmi_cat &lt;fct&gt;, arm_length &lt;dbl&gt;, waist &lt;dbl&gt;,\n#   arm_circ &lt;dbl&gt;, calf_circ &lt;dbl&gt;, calf_skinfold &lt;dbl&gt;,\n#   triceps_skinfold &lt;dbl&gt;, subscapular_skinfold &lt;dbl&gt;, active_days &lt;dbl&gt;, …\n\n\nNote that, for example, sex and race_eth are now listed as factor (fctr) variables. One place where this distinction between character and factor variables matters is when you summarize the data.\n\nsummary(nnyfs_new2$race_eth)\n\n   Length     Class      Mode \n     1518 character character \n\n\n\nsummary(nnyfs_new3$race_eth)\n\n  3_Black Non-Hispanic   2_White Non-Hispanic             1_Hispanic \n                   338                    610                    450 \n4_Other Race/Ethnicity \n                   120",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Getting Data Into R</span>"
    ]
  },
  {
    "objectID": "98_gettingdataintoR.html#converting-character-variables-into-factors",
    "href": "98_gettingdataintoR.html#converting-character-variables-into-factors",
    "title": "Appendix A — Getting Data Into R",
    "section": "Converting Character Variables into Factors",
    "text": "Converting Character Variables into Factors\nThe command you want to create newdata from olddata is:\nnewdata &lt;- olddata %&gt;%\n    mutate(across(where(is.character), as_factor))\nFor more on factors, visit https://r4ds.had.co.nz/factors.html",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Getting Data Into R</span>"
    ]
  },
  {
    "objectID": "98_gettingdataintoR.html#converting-data-frames-to-tibbles",
    "href": "98_gettingdataintoR.html#converting-data-frames-to-tibbles",
    "title": "Appendix A — Getting Data Into R",
    "section": "Converting Data Frames to Tibbles",
    "text": "Converting Data Frames to Tibbles\nUse as_tibble() or simply tibble() to assign the attributes of a tibble to a data frame. Note that read_rds and read_csv automatically create tibbles.\nFor more on tibbles, visit https://r4ds.had.co.nz/tibbles.html.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Getting Data Into R</span>"
    ]
  },
  {
    "objectID": "98_gettingdataintoR.html#for-more-advice",
    "href": "98_gettingdataintoR.html#for-more-advice",
    "title": "Appendix A — Getting Data Into R",
    "section": "For more advice",
    "text": "For more advice\nConsider visiting the software tutorials page under the R and Data heading on our main web site.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Getting Data Into R</span>"
    ]
  },
  {
    "objectID": "99_sessioninfo.html",
    "href": "99_sessioninfo.html",
    "title": "Appendix B — Session Information",
    "section": "",
    "text": "B.1 Using sessionInfo()\nOne approach is to use the sessionInfo() function available in base R.\nsessionInfo()\n\nR version 4.4.0 (2024-04-24 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 22631)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.0    fastmap_1.2.0     cli_3.6.2        \n [5] tools_4.4.0       htmltools_0.5.8.1 rstudioapi_0.16.0 rmarkdown_2.27   \n [9] knitr_1.47        jsonlite_1.8.8    xfun_0.45         digest_0.6.35    \n[13] rlang_1.1.4       evaluate_0.24.0",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Session Information</span>"
    ]
  },
  {
    "objectID": "99_sessioninfo.html#using-session_info",
    "href": "99_sessioninfo.html#using-session_info",
    "title": "Appendix B — Session Information",
    "section": "\nB.2 Using session_info()",
    "text": "B.2 Using session_info()\nAnother approach is to use the session_info() function available in the sessioninfo package. Professor Love has a slight preference for this approach, but it’s not an important distinction to make.\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.0 (2024-04-24 ucrt)\n os       Windows 11 x64 (build 22631)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/New_York\n date     2024-06-18\n pandoc   3.1.11 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cli           3.6.2   2023-12-11 [1] CRAN (R 4.4.0)\n digest        0.6.35  2024-03-11 [1] CRAN (R 4.4.0)\n evaluate      0.24.0  2024-06-10 [1] CRAN (R 4.4.0)\n fastmap       1.2.0   2024-05-15 [1] CRAN (R 4.4.0)\n htmltools     0.5.8.1 2024-04-04 [1] CRAN (R 4.4.0)\n htmlwidgets   1.6.4   2023-12-06 [1] CRAN (R 4.4.0)\n jsonlite      1.8.8   2023-12-04 [1] CRAN (R 4.4.0)\n knitr         1.47    2024-05-29 [1] CRAN (R 4.4.0)\n rlang         1.1.4   2024-06-04 [1] CRAN (R 4.4.0)\n rmarkdown     2.27    2024-05-17 [1] CRAN (R 4.4.0)\n rstudioapi    0.16.0  2024-03-24 [1] CRAN (R 4.4.0)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.4.0)\n xfun          0.45    2024-06-16 [1] CRAN (R 4.4.0)\n\n [1] C:/Users/thoma/AppData/Local/R/win-library/4.4\n [2] C:/Program Files/R/R-4.4.0/library\n\n──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Session Information</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Appendix C — References",
    "section": "",
    "text": "Bernard, Gordon R., Arthur P. Wheeler, James A. Russell, Roland Schein,\nWarren R. Summer, Kenneth P. Steinberg, William J. Fulkerson, et al.\n1997. “The Effects of Ibuprofen on the Physiology and Survival of\nPatients with Sepsis.” New England Journal of Medicine\n336: 912–18. http://www.nejm.org/doi/full/10.1056/NEJM199703273361303#t=article.\n\n\nBock, David E., Paul F. Velleman, and Richard D. De Veaux. 2004.\nStats: Modelling the World. Boston MA: Pearson\nAddison-Wesley.\n\n\nDupont, William D. 2002. Statistical Modeling for Biomedical\nResearchers. New York: Cambridge University Press.\n\n\nFaraway, Julian J. 2015. Linear Models with r. Second. Boca\nRaton, FL: CRC Press.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using\nRegression and Multilevel-Hierarchical Models. New York: Cambridge\nUniversity Press. http://www.stat.columbia.edu/~gelman/arm/.\n\n\nGelman, Andrew, and Deborah Nolan. 2017. Teaching Statistics: A Bag\nof Tricks. Second Edition. Oxford, UK: Oxford University Press.\n\n\nGood, Phillip I. 2005. Introduction to Statistics Through Resampling\nMethods and r/s-PLUS. Hoboken, NJ: Wiley.\n\n\nHadley Wickham, Mine Çetinyaka-Rundel, and Garrett Grolemund. 2023.\nR for Data Science. Second. O’Reilly. https://r4ds.hadley.nz/.\n\n\nHarrell, Frank E. 2001. Regression Modeling Strategies. New\nYork: Springer.\n\n\nIsmay, Chester, and Albert Y. Kim. 2022. ModernDive: Statistical\nInference via Data Science. http://moderndive.com/.\n\n\nMcDonald, Gary C., and Richard C. Schwing. 1973. “Instabilities of\nRegression Estimates Relating Air Pollution to Mortality.”\nTechnometrics 15 (3): 463–81.\n\n\nMorton, D., A. Saah, S. Silberg, W. Owens, M. Roberts, and M. Saah.\n1982. “Lead Absorption in Children of Employees in a Lead Related\nIndustry.” American Journal of Epidemiology 115: 549–55.\n\n\nNorman, Geoffrey R., and David L. Streiner. 2014. Biostatistics: The\nBare Essentials. Fourth. People’s Medical Publishing House.\n\n\nPagano, Marcello, and Kimberlee Gauvreau. 2000. Principles of\nBiostatistics. Second. Duxbury Press.\n\n\nPruzek, Robert M., and James E. Helmreich. 2009. “Enhancing\nDependent Sample Analyses with Graphics.” Journal of\nStatistics Education 17(1). http://ww2.amstat.org/publications/jse/v17n1/helmreich.html.\n\n\nRamsey, Fred L., and Daniel W. Schafer. 2002. The Statistical\nSleuth: A Course in Methods of Data Analysis. Second. Pacific\nGrove, CA: Duxbury.\n\n\nVittinghoff, Eric, David V. Glidden, Stephen C. Shiboski, and Charles E.\nMcCulloch. 2012. Regression Methods in Biostatistics: Linear,\nLogistic, Survival, and Repeated Measures Models. Second.\nSpringer-Verlag, Inc. http://www.biostat.ucsf.edu/vgsm/.\n\n\nWainer, Howard. 1997. Visual Revelations: Graphical Tales of Fate\nand Deception from Napoleon Bonaparte to Ross Perot. New York:\nSpringer-Verlag.\n\n\n———. 2005. Graphic Discovery: A Trout in the Milk and Other Visual\nAdventures. Princeton, NJ: Princeton University Press.\n\n\n———. 2013. Medical Illuminations: Using Evidence, Visualization and\nStatistical Thinking to Improve Healthcare. New York: Oxford\nUniversity Press.\n\n\nYamada, SB, and EG Boulding. 1998. “Claw Morphology, Prey Size\nSelection and Foraging Efficiency in Generalist and Specialist\nShell-Breaking Crabs.” Journal of Experimental Marine Biology\nand Ecology 220: 191–211. http://www.science.oregonstate.edu/~yamadas/SylviaCV/BehrensYamada_Boulding1998.pdf.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>References</span>"
    ]
  }
]